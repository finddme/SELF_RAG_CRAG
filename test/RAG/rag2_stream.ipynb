{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b18c46b-2ae7-4fbd-8ef7-a8b7627dfc73",
   "metadata": {},
   "source": [
    "# [Paid operational RAG] Langchain | LangGraph | Claude3.5 | Wevieate | Cohere | Tavily(advanced) | Fastapi | Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c89f35-2852-40d8-9a92-090cf2454586",
   "metadata": {},
   "source": [
    "## Query Router | Hallucination Grader | Reranker | Web Search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82a11953-4742-47fe-96a3-b88b8bc27c2b",
   "metadata": {},
   "source": [
    "# ref.\n",
    "(RAG)\n",
    "- https://blog.gopenai.com/advanced-rag-with-self-correction-langgraph-no-hallucination-agents-groq-42cb6e5c0086\n",
    "- https://github.com/Eduardovasquezn/advanced-rag-app/blob/main/self-rag.ipynb\n",
    "(weaviate)\n",
    "- https://levelup.gitconnected.com/why-you-should-consider-weaviate-as-your-ultimate-vector-database-bc7cd41b2cae\n",
    "- https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker\n",
    "- https://blog.gopenai.com/augmented-knowledge-discovery-query-your-documents-with-dockerized-weaviate-706377966003\n",
    "- https://weaviate.io/developers/weaviate/search/hybrid\n",
    "\n",
    "# summary\n",
    "RAG Framework : Langchain\n",
    "Workflow control : LangGraph\n",
    "LLM : Claude sonnet 3.5\n",
    "text embedding : Openai\n",
    "vector DB : Wevieate\n",
    "chunk method : RecursiveCharacterTextSplitter\n",
    "Reranker : Cohere\n",
    "web search : Tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bb8b3-a89d-45e3-b2e6-4b89871cdea3",
   "metadata": {},
   "source": [
    "<img src=\"attachment:c25766b8-9d4f-4b36-81ba-debcb2b5a816.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34236346-77e2-4c98-8eef-edfb12ff3235",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e1e1bc-5b5e-4bd1-b0d2-d95bcfe09d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "import json\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_vertexai import VertexAIEmbeddings \n",
    "import re\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import anthropic\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e590f48-f809-4d5f-8f21-2a7d06a148da",
   "metadata": {},
   "source": [
    "# API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d70773d-f219-429c-beb0-5985abf8eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = \"\"\n",
    "Openai_API_KEY = \"\"\n",
    "GROQ_API_KEY = \"\"\n",
    "coher_API_KEY = \"\"\n",
    "claude_api_key=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34759687-2aff-4fda-9091-dca57404b566",
   "metadata": {},
   "source": [
    "# LLM setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add0181c-6b36-4d62-9baa-e5f4a85e5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d50e77-d7d4-4b38-93cd-2bce5d4055fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = anthropic.Anthropic(api_key=claude_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d644daf0-1311-442b-83e0-a399e0769fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\",api_key=claude_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0322a-c03a-4fa8-90ba-28c634919c6f",
   "metadata": {},
   "source": [
    "# prepare embedding model (openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5711c7-a533-493a-927d-d8a2cf761cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] =  Openai_API_KEY\n",
    "openai.api_key =os.getenv(\"OPENAI_API_KEY\")\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "embeddings = OpenAIEmbeddings(model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdb9d8f-221e-41b2-8643-c3067523072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, engine=\"text-embedding-ada-002\") : \n",
    "    res = openai.Embedding.create(input=text,engine=engine)['data'][0]['embedding']\n",
    "    # from openai import OpenAI\n",
    "    # embedding_client = OpenAI(api_key=Openai_API_KEY)\n",
    "    # res= embedding_client.embeddings.create(input = text, model=engine).data[0].embedding\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027d9aab-e3fb-4c7f-9324-3001b84f7057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.054653700441122055,\n",
       " -0.019279181957244873,\n",
       " -0.008386376313865185,\n",
       " -0.02194819040596485,\n",
       " 0.0063812327571213245,\n",
       " 0.015973404049873352,\n",
       " -0.020430784672498703,\n",
       " 0.007336385548114777,\n",
       " -0.020146271213889122,\n",
       " 0.009002822451293468,\n",
       " 0.02202947996556759,\n",
       " 0.018019191920757294,\n",
       " -0.008664115332067013,\n",
       " -0.010655710473656654,\n",
       " 0.012227309867739677,\n",
       " -0.00718735484406352,\n",
       " 0.021175937727093697,\n",
       " -0.007627673447132111,\n",
       " -0.0021270776633173227,\n",
       " -0.03495452553033829,\n",
       " -0.0019526437390595675,\n",
       " 0.013019883073866367,\n",
       " -0.021324969828128815,\n",
       " -0.017572099342942238,\n",
       " -0.01474051270633936,\n",
       " 0.0017883710097521544,\n",
       " 0.0005249952664598823,\n",
       " -0.03292228281497955,\n",
       " -0.010825064033269882,\n",
       " -0.020349495112895966,\n",
       " 0.011671830900013447,\n",
       " -0.01326375175267458,\n",
       " 0.01159054134041071,\n",
       " -0.005500595550984144,\n",
       " -0.008847016841173172,\n",
       " 0.01961788721382618,\n",
       " 0.008047669194638729,\n",
       " -0.018235964700579643,\n",
       " 0.004240606911480427,\n",
       " 0.00279771676287055,\n",
       " -0.00515511492267251,\n",
       " 0.005490434356033802,\n",
       " 0.023668820038437843,\n",
       " -0.014903091825544834,\n",
       " 0.010289907455444336,\n",
       " 0.022043026983737946,\n",
       " -0.02278818190097809,\n",
       " -0.017951451241970062,\n",
       " -0.010350874625146389,\n",
       " 0.021853351965546608,\n",
       " 0.0028129585552960634,\n",
       " 0.007451545912772417,\n",
       " -0.03530677780508995,\n",
       " -0.034602269530296326,\n",
       " -0.014347612857818604,\n",
       " -0.0016122435918077826,\n",
       " -0.0022185284178704023,\n",
       " 0.003344727912917733,\n",
       " -0.0037731919437646866,\n",
       " -0.02950812131166458,\n",
       " 0.016975976526737213,\n",
       " -0.005358338821679354,\n",
       " -0.00389173929579556,\n",
       " -0.010933450423181057,\n",
       " 0.03934416174888611,\n",
       " -0.009639590978622437,\n",
       " -0.03365389257669449,\n",
       " 0.025253966450691223,\n",
       " 0.002548767486587167,\n",
       " 0.0011753119761124253,\n",
       " 0.03159455582499504,\n",
       " 0.014862447045743465,\n",
       " -0.005073825363069773,\n",
       " -0.03666160628199577,\n",
       " 0.039967384189367294,\n",
       " -0.016691463068127632,\n",
       " 0.010764096863567829,\n",
       " -0.019875304773449898,\n",
       " 0.018195319920778275,\n",
       " 0.005886721424758434,\n",
       " 0.027719751000404358,\n",
       " 0.0010957159101963043,\n",
       " -0.010242488235235214,\n",
       " 0.040400926023721695,\n",
       " 0.02796361967921257,\n",
       " -0.006428651977330446,\n",
       " 0.007925734855234623,\n",
       " 0.011543122120201588,\n",
       " -0.01187505479902029,\n",
       " 0.010567646473646164,\n",
       " 0.025294611230492592,\n",
       " -0.005070438142865896,\n",
       " 0.004928181413561106,\n",
       " -0.008183152414858341,\n",
       " 0.004233832936733961,\n",
       " 0.03966931998729706,\n",
       " -0.01666436716914177,\n",
       " 0.03473775088787079,\n",
       " -0.010587968863546848,\n",
       " -0.011366994120180607,\n",
       " -0.018439188599586487,\n",
       " 0.0072754183784127235,\n",
       " -0.029968762770295143,\n",
       " -0.003044972661882639,\n",
       " -0.0493156872689724,\n",
       " -0.019672080874443054,\n",
       " 0.0037969013210386038,\n",
       " 0.021866900846362114,\n",
       " 0.0005050962790846825,\n",
       " -0.0008874113555066288,\n",
       " 0.0021253840532153845,\n",
       " 0.03332873433828354,\n",
       " 0.004149156156927347,\n",
       " -0.047662798315286636,\n",
       " -0.01290472224354744,\n",
       " 0.00948378536850214,\n",
       " 0.002203286625444889,\n",
       " -0.025524931028485298,\n",
       " -0.003236341755837202,\n",
       " -0.012762465514242649,\n",
       " 0.03148616850376129,\n",
       " 0.018520478159189224,\n",
       " 0.020525621250271797,\n",
       " -0.012606660835444927,\n",
       " 0.003966254647821188,\n",
       " -0.004057705402374268,\n",
       " 0.009653138928115368,\n",
       " -0.025931378826498985,\n",
       " 0.0020373202860355377,\n",
       " -0.013914068229496479,\n",
       " 0.051700178533792496,\n",
       " 0.017219845205545425,\n",
       " 0.0021829642355442047,\n",
       " 0.02687975764274597,\n",
       " -0.001131280092522502,\n",
       " 0.002577557461336255,\n",
       " -0.014632126316428185,\n",
       " -0.014157936908304691,\n",
       " -0.034710656851530075,\n",
       " -0.00368512817658484,\n",
       " 0.006269459612667561,\n",
       " 0.023804301396012306,\n",
       " -0.016677914187312126,\n",
       " -0.009653138928115368,\n",
       " -0.009544752538204193,\n",
       " 0.005317694041877985,\n",
       " 0.024590101093053818,\n",
       " 0.023221727460622787,\n",
       " 0.00279771676287055,\n",
       " -0.016095338389277458,\n",
       " -0.0069570341147482395,\n",
       " -0.03354550525546074,\n",
       " -0.024143008515238762,\n",
       " -0.012193438597023487,\n",
       " 0.02994166687130928,\n",
       " 0.01479470543563366,\n",
       " 0.0010110392468050122,\n",
       " -0.01777532324194908,\n",
       " -0.023384306579828262,\n",
       " -0.009090886451303959,\n",
       " -0.0016190176829695702,\n",
       " 0.011597314849495888,\n",
       " 0.0012701498344540596,\n",
       " -0.0029264253098517656,\n",
       " 0.025524931028485298,\n",
       " 0.030998431146144867,\n",
       " -0.01914369873702526,\n",
       " -0.009680235758423805,\n",
       " -0.0032871479634195566,\n",
       " 0.010195069946348667,\n",
       " -0.025714607909321785,\n",
       " 0.006808003410696983,\n",
       " -0.02137916348874569,\n",
       " -0.028857804834842682,\n",
       " -0.03387066349387169,\n",
       " 0.036932572722435,\n",
       " 0.023153984919190407,\n",
       " -0.01950950175523758,\n",
       " -0.029697798192501068,\n",
       " -0.035686131566762924,\n",
       " 0.00748541671782732,\n",
       " -0.007167032454162836,\n",
       " 0.035252586007118225,\n",
       " 0.009023144841194153,\n",
       " -0.011895377188920975,\n",
       " -0.007437997497618198,\n",
       " 0.019279181957244873,\n",
       " -0.011258608661592007,\n",
       " -0.002796023152768612,\n",
       " -0.015146960504353046,\n",
       " 0.023777205497026443,\n",
       " 0.02555202879011631,\n",
       " -0.005277049262076616,\n",
       " -0.01964498497545719,\n",
       " -0.6104306578636169,\n",
       " -0.01694888062775135,\n",
       " -0.0025182839017361403,\n",
       " 0.003915448673069477,\n",
       " 0.013453427702188492,\n",
       " -0.00930088385939598,\n",
       " 0.023113340139389038,\n",
       " 0.00997152365744114,\n",
       " -0.026690082624554634,\n",
       " 0.029860377311706543,\n",
       " -0.011407638899981976,\n",
       " 0.0038849650882184505,\n",
       " -0.02724556066095829,\n",
       " -0.0007362635224126279,\n",
       " -0.007133161649107933,\n",
       " -0.02404817007482052,\n",
       " -0.0060018813237547874,\n",
       " -0.008941855281591415,\n",
       " 0.0330306701362133,\n",
       " -0.0018815153744071722,\n",
       " -0.017545003443956375,\n",
       " 0.017761776223778725,\n",
       " -0.020688200369477272,\n",
       " 0.026730727404356003,\n",
       " -0.0016723639564588666,\n",
       " -0.0030703756492584944,\n",
       " 0.011936021968722343,\n",
       " -0.016528883948922157,\n",
       " -0.011495702899992466,\n",
       " 0.011705701239407063,\n",
       " -0.026107506826519966,\n",
       " 0.0011803925735875964,\n",
       " 0.015336636453866959,\n",
       " 0.00032346483203582466,\n",
       " 0.0503995455801487,\n",
       " -0.011820861138403416,\n",
       " -0.02454945631325245,\n",
       " 0.005104308947920799,\n",
       " -0.02192109264433384,\n",
       " 0.05365113168954849,\n",
       " -0.0068181646056473255,\n",
       " -0.021677223965525627,\n",
       " 0.02224625088274479,\n",
       " -0.004735118709504604,\n",
       " 0.017680486664175987,\n",
       " 0.025579124689102173,\n",
       " -0.0007794486591592431,\n",
       " -0.01160408928990364,\n",
       " 0.018940474838018417,\n",
       " -0.01566179469227791,\n",
       " -0.04229768365621567,\n",
       " -0.013439878821372986,\n",
       " 0.012403436936438084,\n",
       " 0.006553973536938429,\n",
       " 0.00962604209780693,\n",
       " 0.024197202175855637,\n",
       " 0.021297873929142952,\n",
       " 0.007993476465344429,\n",
       " 0.02663588896393776,\n",
       " -0.011482154950499535,\n",
       " 0.008535407483577728,\n",
       " 0.008149281144142151,\n",
       " -0.022395282983779907,\n",
       " -0.017978547140955925,\n",
       " -0.019346922636032104,\n",
       " 0.0065607475116848946,\n",
       " 0.016352755948901176,\n",
       " 0.015214702114462852,\n",
       " 0.0011185785988345742,\n",
       " -0.05262146145105362,\n",
       " 0.016583077609539032,\n",
       " 0.014848899096250534,\n",
       " 0.01072345208376646,\n",
       " 0.013663425110280514,\n",
       " -0.014130841009318829,\n",
       " 0.018913378939032555,\n",
       " 0.04340864345431328,\n",
       " -0.026148151606321335,\n",
       " 0.0014022454852238297,\n",
       " 0.024210749194025993,\n",
       " 0.013981809839606285,\n",
       " -0.020864328369498253,\n",
       " -0.009646364487707615,\n",
       " 0.027340399101376534,\n",
       " -0.010432164184749126,\n",
       " 0.010154425166547298,\n",
       " -0.015431473962962627,\n",
       " -0.030293921008706093,\n",
       " 0.010140876285731792,\n",
       " -0.01353471726179123,\n",
       " 0.007349933963268995,\n",
       " 0.016041146591305733,\n",
       " -0.008616697043180466,\n",
       " -0.031621649861335754,\n",
       " 0.010161198675632477,\n",
       " 0.02551138401031494,\n",
       " -0.003973028622567654,\n",
       " -0.0027858619578182697,\n",
       " 0.012870851904153824,\n",
       " 0.0017079281387850642,\n",
       " -0.006374458782374859,\n",
       " 0.014957284554839134,\n",
       " 0.005913817789405584,\n",
       " -0.02257140912115574,\n",
       " 0.0351983942091465,\n",
       " 0.008596374653279781,\n",
       " -0.004189800936728716,\n",
       " -0.0020186915062367916,\n",
       " 0.013954713009297848,\n",
       " -0.007519287057220936,\n",
       " -0.025077838450670242,\n",
       " -0.0083118611946702,\n",
       " 0.022991405799984932,\n",
       " 0.01734177954494953,\n",
       " 0.0005072131752967834,\n",
       " -0.027597816661000252,\n",
       " 0.013013108633458614,\n",
       " 0.028017813339829445,\n",
       " 0.02278818190097809,\n",
       " -0.004484475590288639,\n",
       " 0.025240417569875717,\n",
       " 0.0005880793905816972,\n",
       " 0.028668129816651344,\n",
       " -0.004928181413561106,\n",
       " -0.007255095988512039,\n",
       " 0.0018815153744071722,\n",
       " -0.007268644403666258,\n",
       " -0.022002382203936577,\n",
       " -0.001655428670346737,\n",
       " 0.00542608043178916,\n",
       " -0.027489429339766502,\n",
       " -0.02587718702852726,\n",
       " 0.02561976946890354,\n",
       " -0.013013108633458614,\n",
       " -0.0020186915062367916,\n",
       " -0.003698676358908415,\n",
       " 0.015824373811483383,\n",
       " 0.0008277143351733685,\n",
       " 0.0049180202186107635,\n",
       " 0.007634447421878576,\n",
       " -0.003424324095249176,\n",
       " 0.004782537464052439,\n",
       " 0.008142507635056973,\n",
       " -0.0216230321675539,\n",
       " -0.0480150505900383,\n",
       " -0.031188106164336205,\n",
       " -0.0345480777323246,\n",
       " 0.008765727281570435,\n",
       " 0.006371071562170982,\n",
       " 0.0034785170573741198,\n",
       " 0.006293169222772121,\n",
       " 0.019211439415812492,\n",
       " -0.009002822451293468,\n",
       " -0.02692040242254734,\n",
       " 0.003597064409404993,\n",
       " -9.653138840803877e-05,\n",
       " 0.015851469710469246,\n",
       " -0.01025603711605072,\n",
       " -0.02994166687130928,\n",
       " -0.013507620431482792,\n",
       " -0.0012616822496056557,\n",
       " 0.021026907488703728,\n",
       " -0.01165828201919794,\n",
       " -0.015255346894264221,\n",
       " -0.006337201222777367,\n",
       " -0.03308486193418503,\n",
       " -0.023167533800005913,\n",
       " -0.0007917267503216863,\n",
       " -0.01159054134041071,\n",
       " -0.032515835016965866,\n",
       " -0.028803611174225807,\n",
       " -0.0503995455801487,\n",
       " -0.005700432695448399,\n",
       " -0.01131957583129406,\n",
       " -0.007715736981481314,\n",
       " 0.0185746718198061,\n",
       " -0.003803675528615713,\n",
       " 0.0028231197502464056,\n",
       " -0.019495952874422073,\n",
       " -0.003647870384156704,\n",
       " 0.012281502597033978,\n",
       " -0.0011753119761124253,\n",
       " -0.015268894843757153,\n",
       " 0.017395973205566406,\n",
       " 0.03977770730853081,\n",
       " 0.012897948734462261,\n",
       " 0.0246578436344862,\n",
       " -0.011421187780797482,\n",
       " -0.030862947925925255,\n",
       " 0.008589600212872028,\n",
       " -0.00462334556505084,\n",
       " 0.0012100294698029757,\n",
       " -0.02137916348874569,\n",
       " 0.015417926013469696,\n",
       " 0.025457190349698067,\n",
       " -0.0024386877194046974,\n",
       " 0.008975725620985031,\n",
       " -0.004931568633764982,\n",
       " 0.01308085024356842,\n",
       " 0.021568838506937027,\n",
       " 0.025430094450712204,\n",
       " 0.01261343527585268,\n",
       " 0.014185033738613129,\n",
       " -0.015919212251901627,\n",
       " 0.00877927616238594,\n",
       " -0.029128769412636757,\n",
       " -0.005338016431778669,\n",
       " -0.02379075437784195,\n",
       " 0.019712725654244423,\n",
       " -0.013670199550688267,\n",
       " 0.0032769867684692144,\n",
       " -0.03674289584159851,\n",
       " -0.024143008515238762,\n",
       " 0.014564384706318378,\n",
       " -0.01590566337108612,\n",
       " 0.02950812131166458,\n",
       " -0.023275919258594513,\n",
       " 0.006364297587424517,\n",
       " -0.0007299127755686641,\n",
       " 0.02519977279007435,\n",
       " 0.0032719061709940434,\n",
       " -0.01353471726179123,\n",
       " 0.005676723085343838,\n",
       " -0.017043717205524445,\n",
       " -0.011915699578821659,\n",
       " -0.01192924752831459,\n",
       " 0.011637959629297256,\n",
       " 0.02192109264433384,\n",
       " 0.010289907455444336,\n",
       " -0.015052122995257378,\n",
       " 0.00021698394266422838,\n",
       " -0.006479457952082157,\n",
       " -0.005907043814659119,\n",
       " 0.04023834690451622,\n",
       " 0.015106315724551678,\n",
       " -0.027909426018595695,\n",
       " 0.030890045687556267,\n",
       " -0.004484475590288639,\n",
       " 0.03357260301709175,\n",
       " 0.01637985184788704,\n",
       " -0.016434045508503914,\n",
       " 0.01709791086614132,\n",
       " -0.02177206240594387,\n",
       " -0.007736059371381998,\n",
       " 0.02818039245903492,\n",
       " 0.008786049671471119,\n",
       " 0.04977632686495781,\n",
       " 0.009944426827132702,\n",
       " -0.02213786542415619,\n",
       " -0.01331794448196888,\n",
       " -0.04178284853696823,\n",
       " -0.0003928997030016035,\n",
       " 0.02883070893585682,\n",
       " -0.010709904134273529,\n",
       " 0.004853665828704834,\n",
       " -0.030321016907691956,\n",
       " 0.02731330320239067,\n",
       " -0.0023455433547496796,\n",
       " 0.026297183707356453,\n",
       " 0.02937263995409012,\n",
       " 0.00017051762552000582,\n",
       " 0.018764346837997437,\n",
       " -0.0057241423055529594,\n",
       " -0.0241159126162529,\n",
       " 0.047202154994010925,\n",
       " -0.0055784983560442924,\n",
       " -0.015892114490270615,\n",
       " -0.020579814910888672,\n",
       " -0.014618578366935253,\n",
       " 0.007587028667330742,\n",
       " -0.02950812131166458,\n",
       " -0.007688640616834164,\n",
       " 0.012870851904153824,\n",
       " -0.01950950175523758,\n",
       " 0.01964498497545719,\n",
       " -0.005707206670194864,\n",
       " -0.018371447920799255,\n",
       " -0.0060018813237547874,\n",
       " -0.004193188156932592,\n",
       " 0.008975725620985031,\n",
       " -0.02194819040596485,\n",
       " -0.025606220588088036,\n",
       " -0.0032244871836155653,\n",
       " 0.013033431023359299,\n",
       " 0.00498576182872057,\n",
       " -0.031946808099746704,\n",
       " -0.0068452609702944756,\n",
       " -0.011360220611095428,\n",
       " -0.023763656616210938,\n",
       " 0.003339647315442562,\n",
       " -0.0039967382326722145,\n",
       " 0.012999560683965683,\n",
       " -0.005256726872175932,\n",
       " -0.0005643698968924582,\n",
       " -0.030754562467336655,\n",
       " -0.013609232380986214,\n",
       " 0.015187605284154415,\n",
       " 0.008372828364372253,\n",
       " 0.018114030361175537,\n",
       " -0.02551138401031494,\n",
       " -0.009138304740190506,\n",
       " 0.007329611573368311,\n",
       " 0.0015224863309413195,\n",
       " -0.015607601962983608,\n",
       " 0.006056074518710375,\n",
       " 0.022151414304971695,\n",
       " -0.014212130568921566,\n",
       " -0.0019577243365347385,\n",
       " -0.01579727791249752,\n",
       " -0.009659913368523121,\n",
       " 0.011272156611084938,\n",
       " 0.0001902049407362938,\n",
       " -0.032597124576568604,\n",
       " 0.013202784582972527,\n",
       " 0.033924855291843414,\n",
       " 0.0246578436344862,\n",
       " 0.0047114090994000435,\n",
       " -0.0033413409255445004,\n",
       " 0.030619079247117043,\n",
       " 0.008074766024947166,\n",
       " 0.00044412908027879894,\n",
       " -0.006614940706640482,\n",
       " -0.011936021968722343,\n",
       " 0.01723339408636093,\n",
       " 0.00032346483203582466,\n",
       " -0.00663187587633729,\n",
       " -0.00677413260564208,\n",
       " -0.0001830074324971065,\n",
       " -0.009409270249307156,\n",
       " 0.026473309844732285,\n",
       " -0.012125696986913681,\n",
       " -0.016027597710490227,\n",
       " -0.011068932712078094,\n",
       " -0.023546885699033737,\n",
       " -0.015648245811462402,\n",
       " -0.005070438142865896,\n",
       " 0.0023794141598045826,\n",
       " 0.008738631382584572,\n",
       " 0.026622341945767403,\n",
       " 0.023628175258636475,\n",
       " -0.0032854543533176184,\n",
       " -0.014605030417442322,\n",
       " 0.016095338389277458,\n",
       " 0.008718308992683887,\n",
       " 0.008291538804769516,\n",
       " -0.018696606159210205,\n",
       " 0.004342218860983849,\n",
       " 0.002431913511827588,\n",
       " -0.0008086620946414769,\n",
       " -0.021297873929142952,\n",
       " -0.0019001441542059183,\n",
       " 0.02948102541267872,\n",
       " 0.012369566597044468,\n",
       " -0.027042336761951447,\n",
       " -0.006625101901590824,\n",
       " -0.009490559808909893,\n",
       " -0.02972489409148693,\n",
       " 0.004406573250889778,\n",
       " -0.0060018813237547874,\n",
       " 0.06969227641820908,\n",
       " 0.0038849650882184505,\n",
       " 0.02001078799366951,\n",
       " 0.010459261015057564,\n",
       " 0.005371887236833572,\n",
       " -0.0029856988694518805,\n",
       " 0.04148478806018829,\n",
       " -0.003214325988665223,\n",
       " 0.007160258013755083,\n",
       " -0.009409270249307156,\n",
       " -0.008799598552286625,\n",
       " 0.0024945742916315794,\n",
       " 0.008623470552265644,\n",
       " -0.004362541250884533,\n",
       " -0.011265382170677185,\n",
       " 0.015201153233647346,\n",
       " -0.010466034524142742,\n",
       " -0.05961236730217934,\n",
       " -0.010174747556447983,\n",
       " 0.023587530478835106,\n",
       " 0.005043341778218746,\n",
       " 0.021799158304929733,\n",
       " -0.009368625469505787,\n",
       " -0.010195069946348667,\n",
       " 0.006936711724847555,\n",
       " -0.019116602838039398,\n",
       " -0.011163770221173763,\n",
       " -0.01774822734296322,\n",
       " -0.007641221396625042,\n",
       " 0.001323496107943356,\n",
       " 0.005270275287330151,\n",
       " -0.001988207921385765,\n",
       " -0.01666436716914177,\n",
       " -0.006581069901585579,\n",
       " -0.02627008594572544,\n",
       " 0.015390829183161259,\n",
       " 0.01907595805823803,\n",
       " -0.018520478159189224,\n",
       " -0.0007146709831431508,\n",
       " 0.02570105902850628,\n",
       " 0.027191368862986565,\n",
       " 0.017843065783381462,\n",
       " -0.014008906669914722,\n",
       " -0.018276609480381012,\n",
       " 0.023303017020225525,\n",
       " 0.014835350215435028,\n",
       " -0.0065268767066299915,\n",
       " -0.006238976027816534,\n",
       " -0.024088814854621887,\n",
       " -0.012538919225335121,\n",
       " 0.01881854049861431,\n",
       " 0.0013201091205701232,\n",
       " -0.02666298672556877,\n",
       " -0.034927427768707275,\n",
       " 0.007309289183467627,\n",
       " -0.010066361166536808,\n",
       " 0.007986702024936676,\n",
       " -0.005395596846938133,\n",
       " -0.002469171304255724,\n",
       " -0.021297873929142952,\n",
       " 0.022490119561553,\n",
       " 0.022761086001992226,\n",
       " -0.0034785170573741198,\n",
       " 0.005365113262087107,\n",
       " 0.004934955853968859,\n",
       " 0.0014064792776480317,\n",
       " -0.008088313974440098,\n",
       " -0.006459135562181473,\n",
       " -0.007458319887518883,\n",
       " 0.007763156201690435,\n",
       " 0.01633920706808567,\n",
       " 0.014008906669914722,\n",
       " 0.023221727460622787,\n",
       " 0.02767910622060299,\n",
       " 0.015445022843778133,\n",
       " 0.001669823657721281,\n",
       " -0.004667377565056086,\n",
       " 0.000792996899690479,\n",
       " -0.01940111629664898,\n",
       " 0.028911998495459557,\n",
       " -0.008643792942166328,\n",
       " -0.005094147752970457,\n",
       " -0.018371447920799255,\n",
       " -0.029697798192501068,\n",
       " -0.012626983225345612,\n",
       " 0.00019782583694905043,\n",
       " -0.017626293003559113,\n",
       " 0.03167584538459778,\n",
       " 0.002282882807776332,\n",
       " 0.0063541363924741745,\n",
       " 0.002206673612818122,\n",
       " -0.00915862713009119,\n",
       " -0.021907545626163483,\n",
       " 0.0054599507711827755,\n",
       " 0.02561976946890354,\n",
       " -0.025931378826498985,\n",
       " 0.04037383198738098,\n",
       " 0.02400752529501915,\n",
       " -0.014808254316449165,\n",
       " -0.008968951180577278,\n",
       " -0.01446954719722271,\n",
       " 0.005642852280288935,\n",
       " 0.00556833716109395,\n",
       " -0.013927617110311985,\n",
       " -0.014618578366935253,\n",
       " 0.0052093081176280975,\n",
       " -0.012247632257640362,\n",
       " -0.0026080410461872816,\n",
       " -0.009924104437232018,\n",
       " 0.005693658255040646,\n",
       " -0.016203725710511208,\n",
       " -0.014320516027510166,\n",
       " 0.011726023629307747,\n",
       " 0.023492692038416862,\n",
       " 0.013941165059804916,\n",
       " -0.009829265996813774,\n",
       " -0.005954462569206953,\n",
       " -0.02587718702852726,\n",
       " 0.009443140588700771,\n",
       " -0.00200175610370934,\n",
       " -0.03191971406340599,\n",
       " -0.025904282927513123,\n",
       " -0.002652072813361883,\n",
       " 0.039100293070077896,\n",
       " 0.014225678518414497,\n",
       " 0.022937214002013206,\n",
       " -0.003854481503367424,\n",
       " 0.01896757073700428,\n",
       " 0.03555064648389816,\n",
       " 0.0075057391077280045,\n",
       " 0.00204748148098588,\n",
       " -0.002096594078466296,\n",
       " 0.025064291432499886,\n",
       " -0.026974596083164215,\n",
       " 0.003268518950790167,\n",
       " -0.015824373811483383,\n",
       " -0.006262685637921095,\n",
       " 0.015052122995257378,\n",
       " -0.007011227309703827,\n",
       " 0.029020383954048157,\n",
       " 0.004603023175150156,\n",
       " 0.0026165086310356855,\n",
       " 0.006245750468224287,\n",
       " -0.008907984010875225,\n",
       " -0.016285015270113945,\n",
       " 0.003227874170988798,\n",
       " -0.01529599167406559,\n",
       " -0.005761399865150452,\n",
       " -0.010018941946327686,\n",
       " -0.02874941937625408,\n",
       " -0.020024336874485016,\n",
       " -0.0075057391077280045,\n",
       " 0.01373116672039032,\n",
       " 0.016095338389277458,\n",
       " 0.01340600848197937,\n",
       " 0.04164736717939377,\n",
       " -0.013981809839606285,\n",
       " -0.009619268588721752,\n",
       " 0.022937214002013206,\n",
       " 0.00795283168554306,\n",
       " 0.004437056835740805,\n",
       " -0.010621840134263039,\n",
       " -0.018886281177401543,\n",
       " -0.015390829183161259,\n",
       " 0.028099102899432182,\n",
       " 0.012477952055633068,\n",
       " -0.006198331248015165,\n",
       " -0.0012227309634909034,\n",
       " -0.007227999623864889,\n",
       " 0.00709929084405303,\n",
       " -0.0031313428189605474,\n",
       " -0.015390829183161259,\n",
       " -0.014550836756825447,\n",
       " -0.010499905794858932,\n",
       " -0.024590101093053818,\n",
       " -0.019130149856209755,\n",
       " -0.012389888986945152,\n",
       " -0.005764787085354328,\n",
       " -0.009829265996813774,\n",
       " 0.02490171231329441,\n",
       " -0.006723326630890369,\n",
       " -0.013927617110311985,\n",
       " 0.0063541363924741745,\n",
       " -0.001983127323910594,\n",
       " -0.03517129644751549,\n",
       " 0.013297622092068195,\n",
       " 0.041078340262174606,\n",
       " 0.026012670248746872,\n",
       " -0.008799598552286625,\n",
       " 0.007234773598611355,\n",
       " 0.030293921008706093,\n",
       " -0.004579313565045595,\n",
       " -0.005876560229808092,\n",
       " 0.005934140179306269,\n",
       " -0.008318634703755379,\n",
       " -0.012552468106150627,\n",
       " 0.00023730633256491274,\n",
       " 0.029995858669281006,\n",
       " -0.013453427702188492,\n",
       " -0.003634322201833129,\n",
       " 0.025430094450712204,\n",
       " 0.004240606911480427,\n",
       " -0.028559742495417595,\n",
       " -0.046009909361600876,\n",
       " 0.023452047258615494,\n",
       " 0.003102552844211459,\n",
       " 0.02742168866097927,\n",
       " -0.00568688428029418,\n",
       " -0.04405895993113518,\n",
       " -0.026798468083143234,\n",
       " 0.004223671741783619,\n",
       " -0.006872357334941626,\n",
       " 0.012071504257619381,\n",
       " -0.008725082501769066,\n",
       " -0.026947500184178352,\n",
       " -0.012843755073845387,\n",
       " 0.02242237888276577,\n",
       " -0.005520917940884829,\n",
       " 0.018561122938990593,\n",
       " 0.011265382170677185,\n",
       " 0.01464567519724369,\n",
       " -0.0034446464851498604,\n",
       " -0.011217963881790638,\n",
       " -0.009504107758402824,\n",
       " -0.01280988473445177,\n",
       " -0.006496393121778965,\n",
       " 0.015282442793250084,\n",
       " -0.010757322423160076,\n",
       " 0.012051181867718697,\n",
       " 0.013046979904174805,\n",
       " -0.027543623000383377,\n",
       " 0.015187605284154415,\n",
       " 0.02098626270890236,\n",
       " -0.010391519404947758,\n",
       " 0.030862947925925255,\n",
       " -0.02872232161462307,\n",
       " 0.008020573295652866,\n",
       " -0.024454619735479355,\n",
       " -0.009463462978601456,\n",
       " 0.01990240253508091,\n",
       " 0.01123151183128357,\n",
       " 0.014320516027510166,\n",
       " -0.02483396977186203,\n",
       " -0.00512801855802536,\n",
       " -0.009924104437232018,\n",
       " 0.026256538927555084,\n",
       " 0.011800538748502731,\n",
       " -0.02259850688278675,\n",
       " -0.024427521973848343,\n",
       " -0.009416044689714909,\n",
       " 0.01211214903742075,\n",
       " -0.015702439472079277,\n",
       " -0.010940223932266235,\n",
       " 0.04430282860994339,\n",
       " 0.02007852867245674,\n",
       " -0.01716565154492855,\n",
       " -0.002262560185045004,\n",
       " -0.023126889020204544,\n",
       " 0.04660603404045105,\n",
       " -0.0064659095369279385,\n",
       " -0.0024200589396059513,\n",
       " 0.017585648223757744,\n",
       " 0.0242242980748415,\n",
       " -0.026446213945746422,\n",
       " 0.00030843474087305367,\n",
       " 0.005371887236833572,\n",
       " -0.013473750092089176,\n",
       " 0.025606220588088036,\n",
       " -0.0020051433239132166,\n",
       " 0.008379601873457432,\n",
       " -0.04110543802380562,\n",
       " 0.027232013642787933,\n",
       " -0.0033413409255445004,\n",
       " -0.0315132662653923,\n",
       " 0.027001691982150078,\n",
       " -0.0090163704007864,\n",
       " -0.006581069901585579,\n",
       " 0.009002822451293468,\n",
       " -0.011163770221173763,\n",
       " -0.0019153859466314316,\n",
       " 0.005172050092369318,\n",
       " -0.002370946342125535,\n",
       " -0.007756381761282682,\n",
       " 0.00250642909668386,\n",
       " -0.002834974555298686,\n",
       " -0.018547575920820236,\n",
       " -0.01279633678495884,\n",
       " 0.025795897468924522,\n",
       " -0.011807313188910484,\n",
       " -0.01690823584794998,\n",
       " -0.02948102541267872,\n",
       " -0.0021744966506958008,\n",
       " 0.0007256789831444621,\n",
       " -0.016854042187333107,\n",
       " -0.025294611230492592,\n",
       " 0.0008958790567703545,\n",
       " 0.002938280114904046,\n",
       " 0.007451545912772417,\n",
       " -0.00846089143306017,\n",
       " 0.014767609536647797,\n",
       " -0.015594053082168102,\n",
       " -0.01562114991247654,\n",
       " -0.0010330552468076348,\n",
       " 0.013981809839606285,\n",
       " -0.011915699578821659,\n",
       " 0.02616170048713684,\n",
       " 0.030131341889500618,\n",
       " 0.02511848323047161,\n",
       " 0.0035767420195043087,\n",
       " -0.03574032336473465,\n",
       " -0.046443454921245575,\n",
       " 0.0008200934389606118,\n",
       " -0.02072884514927864,\n",
       " -0.003901900490745902,\n",
       " -0.02505074255168438,\n",
       " -0.022083671763539314,\n",
       " 0.03582161292433739,\n",
       " 0.0332474447786808,\n",
       " -0.018845636397600174,\n",
       " -0.00951765663921833,\n",
       " -0.01535018440335989,\n",
       " -0.02594492770731449,\n",
       " 0.012423759326338768,\n",
       " -0.018628865480422974,\n",
       " 0.006459135562181473,\n",
       " 0.015445022843778133,\n",
       " 0.005165276117622852,\n",
       " -0.018005644902586937,\n",
       " 0.020498525351285934,\n",
       " 0.0015690585132688284,\n",
       " -0.006804616190493107,\n",
       " 0.009944426827132702,\n",
       " 0.02083723247051239,\n",
       " 0.01879144459962845,\n",
       " -0.010540550574660301,\n",
       " -0.01612243615090847,\n",
       " 0.00934152863919735,\n",
       " -0.011306026950478554,\n",
       " 0.011123125441372395,\n",
       " -0.008738631382584572,\n",
       " 0.0021203034557402134,\n",
       " 0.013514394871890545,\n",
       " 0.04373380169272423,\n",
       " -0.015268894843757153,\n",
       " -0.030619079247117043,\n",
       " -0.01788371056318283,\n",
       " 0.036119673401117325,\n",
       " 0.0005038261297158897,\n",
       " 0.01154989656060934,\n",
       " -0.010005393996834755,\n",
       " -0.020146271213889122,\n",
       " 0.0017511133337393403,\n",
       " 0.022503668442368507,\n",
       " 0.002938280114904046,\n",
       " -0.0005846923450008035,\n",
       " 0.006791067775338888,\n",
       " 0.021501097828149796,\n",
       " -0.013270526193082333,\n",
       " -0.004054318182170391,\n",
       " -0.0021643354557454586,\n",
       " 0.012349244207143784,\n",
       " 0.0034107756800949574,\n",
       " -0.027326850220561028,\n",
       " 0.026012670248746872,\n",
       " 0.019224988296628,\n",
       " -0.011888602748513222,\n",
       " 0.02274753712117672,\n",
       " 0.010283133015036583,\n",
       " -0.020810134708881378,\n",
       " 0.0019949821289628744,\n",
       " 0.015851469710469246,\n",
       " -0.024847518652677536,\n",
       " -0.01601404882967472,\n",
       " -0.0024437683168798685,\n",
       " 0.003339647315442562,\n",
       " 0.02483396977186203,\n",
       " 0.012356017716228962,\n",
       " 0.0008251740364357829,\n",
       " -0.025524931028485298,\n",
       " -0.007871542125940323,\n",
       " 0.014672771096229553,\n",
       " -0.014808254316449165,\n",
       " -0.04793376103043556,\n",
       " 0.014212130568921566,\n",
       " -0.001519946032203734,\n",
       " -0.05928720906376839,\n",
       " 0.019658531993627548,\n",
       " -0.004115285351872444,\n",
       " -0.014293420128524303,\n",
       " -0.00674703624099493,\n",
       " -0.020146271213889122,\n",
       " 0.023587530478835106,\n",
       " -0.014821802265942097,\n",
       " 0.0023557045497000217,\n",
       " 0.027773944661021233,\n",
       " 0.017070814967155457,\n",
       " -0.019414663314819336,\n",
       " -0.0018307093996554613,\n",
       " -0.010615065693855286,\n",
       " 0.0019103053491562605,\n",
       " 0.001580913201905787,\n",
       " 0.22826117277145386,\n",
       " -0.042162202298641205,\n",
       " -0.021189486607909203,\n",
       " 0.008250893093645573,\n",
       " 0.005585272330790758,\n",
       " 0.005033180583268404,\n",
       " 0.021284325048327446,\n",
       " -0.0005609828513115644,\n",
       " -0.005968010984361172,\n",
       " 0.01446954719722271,\n",
       " -0.015512763522565365,\n",
       " -0.027326850220561028,\n",
       " -0.005188985727727413,\n",
       " -0.008725082501769066,\n",
       " 0.002081352286040783,\n",
       " 0.0012599886395037174,\n",
       " -0.038233205676078796,\n",
       " -0.005449789576232433,\n",
       " -0.0216230321675539,\n",
       " 0.002750297775492072,\n",
       " 0.0018899829592555761,\n",
       " 0.012518596835434437,\n",
       " -0.005554788745939732,\n",
       " -0.01507921889424324,\n",
       " 0.00577494828030467,\n",
       " -0.038775134831666946,\n",
       " 0.04262284189462662,\n",
       " 0.009781847707927227,\n",
       " -0.0029738442972302437,\n",
       " 0.022679796442389488,\n",
       " -0.01961788721382618,\n",
       " 0.013216332532465458,\n",
       " -0.004789311904460192,\n",
       " 0.010154425166547298,\n",
       " -0.017409520223736763,\n",
       " -0.010154425166547298,\n",
       " 0.014008906669914722,\n",
       " -0.0024031235370785,\n",
       " 0.038097720593214035,\n",
       " 0.016786301508545876,\n",
       " 0.00204748148098588,\n",
       " 0.0037731919437646866,\n",
       " 0.004250768106430769,\n",
       " 0.0021812706254422665,\n",
       " -0.009077337570488453,\n",
       " 0.013765037059783936,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(\"현재 시점 이후의 token들에 대해 masking 처리를 하여 Masked Multi-Head Self-Attention을 사용한다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfaaef-9689-49fb-b778-8aa71f13f01c",
   "metadata": {},
   "source": [
    "# TypedDict (to apply to workflow functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0192bf16-501d-4337-bb39-e5b91f1dde99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the stat.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]\n",
    "    source: List[dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58418861-8dcd-44b0-8cd9-39ae004ce0f3",
   "metadata": {},
   "source": [
    "# Query Router(web or db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c21546-cc96-43dc-836f-f4a891dbf864",
   "metadata": {},
   "source": [
    "## Query Router Test / prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1fec63-04b0-42e1-8027-4dde7895d67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='websearch'\n",
      "datasource='vectorstore'\n",
      "datasource='casualconv'\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"websearch\", \"casualconv\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to casual conversation or web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "\"\"\"\n",
    "ref.\n",
    "https://python.langchain.com/v0.2/docs/integrations/chat/groq/\n",
    "Supported Models: https://console.groq.com/docs/models\n",
    "\"\"\"\n",
    "\n",
    "# client = Groq(\n",
    "#     api_key=GROQ_API_KEY\n",
    "# )\n",
    "\n",
    "# llm = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Explain the importance of fast language models\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"llama3-8b-8192\",\n",
    "# )\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "# system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "# The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "# Use the vectorstore for questions on these topics. For all else, use web-search.\"\"\"\n",
    "# route_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# question_router = route_prompt | structured_llm_router\n",
    "# print(question_router.invoke({\"question\": \"Who will the Bears draft first in the NFL draft?\"}))\n",
    "# print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search or casual conversation.\n",
    "The vectorstore contains documents related to language processing, and artificial intelligence.\n",
    "Use the vectorstore for questions on these topics. For all else, use web-search.\n",
    "If the question is a casual conversation use original knowledge\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(question_router.invoke({\"question\": \"2023년 두산 베어스 1번 타자는 누구야?\"}))\n",
    "print(question_router.invoke({\"question\": \"LLaMa3의 구조에 대해 설명해줘\"}))\n",
    "print(question_router.invoke({\"question\": \"안녕\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ef87a-2c4a-4830-9b1e-2a0e92cf3851",
   "metadata": {},
   "source": [
    "## Query Router Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc21f8a-7055-4a85-8a18-c33fd8bc39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})   \n",
    "    if source.datasource == 'websearch':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source.datasource == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    elif source.datasource == 'casualconv':\n",
    "        print(\"---ROUTE QUESTION TO CASUAL CONV.---\")\n",
    "        return \"casualconv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262ed75-94cc-4ac3-a923-0836b6496314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fdbc3-1153-4c52-a3e5-f943191e3072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4ef9c0-5ed0-480b-bb92-3ecb303d6649",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Build VectorDB (Weaviate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f0767-6565-4cba-96bb-e02122dd6e17",
   "metadata": {},
   "source": [
    "## Test VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba17938-5d50-4e8c-8390-13643d53802c",
   "metadata": {},
   "source": [
    "### split / chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e6d2eb8-b6e0-470d-b7cf-587eb58fc8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "request1 = requests.get(\"https://finddme.github.io/\")\n",
    "html1 = request1.text\n",
    "soup1 = BeautifulSoup(html1, 'html.parser')\n",
    "links1 = soup1.select('h4 > a')\n",
    "urls=[]\n",
    "tags=[\"llm\",\"dev\",\"natural\"]\n",
    "for link in links1:\n",
    "    res={\"title\":\"\",\"url\":\"\"}\n",
    "    if link.has_attr('href'):\n",
    "        href=link.get('href')\n",
    "        for t in tags:\n",
    "            if t in href.split(\"/\")[1]:\n",
    "                # urls.append(\"https://finddme.github.io\"+href)\n",
    "                res[\"title\"]+=link.find(\"li\").get_text().strip(\"\\n        \")\n",
    "                res[\"url\"]+=\"https://finddme.github.io\"+href\n",
    "                urls.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db27729c-441f-4d6e-b937-c2b828bb3bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'title': 'VLM : Models Summary(unfinished post)\\n      30 May 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/30/vlm_models/'},\n",
       " {'title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/'},\n",
       " {'title': 'VLM : Applications\\n      28 May 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/28/vlm_task/'},\n",
       " {'title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'title': 'GPU, Paramter : Model Parameters and Memory Usage\\n      03 May 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/'},\n",
       " {'title': 'LLaMa 3\\n      01 May 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/'},\n",
       " {'title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/'},\n",
       " {'title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'title': 'LLM architecture\\n      01 Oct 2023',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/01/llm_architecture/'},\n",
       " {'title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'title': 'ㅡ\\n      27 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/llm%20/%20multimodal/2019/10/27/Rearrangecategories8/'},\n",
       " {'title': 'Fast Inference 실험 (작성 중)\\n      02 Apr 2024',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2024/04/02/inference/'},\n",
       " {'title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'title': 'RAG\\n      02 Jan 2024',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2024/01/02/rag/'},\n",
       " {'title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'title': 'NER(Named Entity Recognition)\\n      25 Sep 2022',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2022/09/25/NER/'},\n",
       " {'title': 'Sentiment Analysis\\n      24 Sep 2022',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2022/09/24/SentimentAnalysis/'},\n",
       " {'title': 'ㅡ\\n      29 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/dev%20log/2019/10/29/Rearrangecategories7/'},\n",
       " {'title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/'},\n",
       " {'title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'title': 'Word embedding(Distributed Representation)\\n      03 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/Word-embedding(Distributed-Representation)/'},\n",
       " {'title': 'LSA(Latent Sematic Analysis)\\n      03 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/'},\n",
       " {'title': 'How to represent words?( Word Vectors)\\n      01 Nov 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/11/01/Word-Vectors/'},\n",
       " {'title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'title': 'ㅡ\\n      31 Oct 2019',\n",
       "  'url': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2019/10/31/Rearrangecategories/'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9da8ccb9-edac-4e01-8306-c3e0909ecfa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# urls = [\n",
    "#     \"https://finddme.github.io/llm%20/%20multimodal/2023/10/01/llm_architecture/\",\n",
    "#     \"https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/\",\n",
    "#     \"https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/\",\n",
    "# ]\n",
    "\"\"\"\n",
    "normal\n",
    "\"\"\"\n",
    "# docs = [WebBaseLoader(url[\"url\"]).load() for url in urls]\n",
    "\"\"\"\n",
    "metadata title 수정\n",
    "\"\"\"\n",
    "docs =[]\n",
    "for url in urls:\n",
    "    w_context=WebBaseLoader(url[\"url\"]).load()[0]\n",
    "    w_context.metadata[\"title\"] = url[\"title\"]\n",
    "    docs.append([w_context])\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1603904-6d12-4b67-9eef-ace2bae4b637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19a445c1-208d-4387-8c4e-76204b97fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/',\n",
       " 'title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       " 'language': 'en-us'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79adcd0-7819-4ac9-8f32-48a4d5451799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# space_check=re.compile(\"\\s{1,}\")\n",
    "# re.sub(space_check,\" \",'Decoder\\n    \\nencoding 된 representation을을 받아 모델의 출력을 생성하는 부분.\\ninput\\n        \\n학습 시에는 정답 sequence(shifted right)가 입력되고, 추론 시에는 이전에 예측된 token이 입력된다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "66b1ba90-8b60-4240-8911-ad4ff5c36bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence '\\s'\n",
      "<>:2: DeprecationWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_1714/2286961391.py:2: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  space_check=re.compile(\"\\s{1,}\")\n"
     ]
    }
   ],
   "source": [
    "# make chunks\n",
    "space_check=re.compile(\"\\s{1,}\")\n",
    "chunks=[]\n",
    "pass_first=[\"finddme \",\"⊹  Portfolio\",\"© 2024 viein_serenity_singularity_simplicity_savage. This work is liscensed under CC BY-NC 4.0.\"]\n",
    "for i in doc_splits:\n",
    "    # c={'text':i.page_content, 'title':i.metadata[\"source\"].split(\"/\")[-2]}\n",
    "    c={'text':i.page_content, 'source_title':i.metadata[\"title\"], \"source\":i.metadata[\"source\"]}\n",
    "    if c[\"text\"].split(\"\\n\")[0] in pass_first:pass\n",
    "    else:\n",
    "        # save_c={'text':re.sub(space_check,\" \",c['text']),'title':c['title']}\n",
    "        save_c={'text':re.sub(space_check,\" \",c['text']),'source_title':c['source_title'],'source':c['source']}\n",
    "        chunks.append(save_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74e80b05-413d-495a-a0b3-1737968c957e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b8183fd2-7026-44db-a80b-caa796bf5d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'llama_index -> FlagEmbeddingReranker(?) huggingface rerank model langchain_community -> JinaRerank langchain_cohere -> CohereRerank Application Interface FastAPI (graphic interface o) Flask Gradio (graphic interface o) Streamlit (graphic interface o) cherrypy Django Chainlit ... RAG 개발 예시 (인터넷 접속 가능 환경) 기능별 구성 요약 Illustrated by the author RAG Framework : Langchain Workflow control : LangGraph LLM : Mixtral-8x7b Inference accelerate : GROQ text embedding : Openai vector DB : Wevieate web search : Tavily Application Interface : Chainlit 단계별 설명 및 코드 Vector DB 준비 : Weaviate Illustrated by the author Weaviate setting weaviate docker setting weaviate docker-compose file: (https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker)[https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker] docker-compose -f wevieate.yml up -d -> weaviate client url 생성 create weaviate class client = weaviate.Client(\"weaviate client url\") class_obj = { \"class\": \"Test\", \"vectorizer\": \"none\", } client.schema.create_class(class_obj) prepare Documents for Retrieval crawling import requests from bs4 import BeautifulSoup soup1 = BeautifulSoup(html1, \\'html.parser\\') request1 = requests.get(\"https://finddme.github.io/\") html1 = request1.text links1 = soup1.select(\\'h4 > a\\') urls=[] tags=[\"llm\",\"dev\",\"natural\"] for link in links1: if link.has_attr(\\'href\\'): href=link.get(\\'href\\') for t in tags: if t in href.split(\"/\")[1]: urls.append(\"https://finddme.github.io\"+href) split / chunking docs = [WebBaseLoader(url).load() for url in urls] docs_list = [item for sublist in docs for item in sublist] text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=0 ) doc_splits = text_splitter.split_documents(docs_list) space_check=re.compile(\"\\\\s{1,}\") chunks=[] pass_first=[\"finddme \",\"⊹ Portfolio\",\"© 2024 viein_serenity_singularity_simplicity_savage. This work is liscensed under CC BY-NC 4.0.\"] for i in doc_splits: c={\\'text\\':i.page_content, \\'title\\':i.metadata[\"source\"].split(\"/\")[-2]} if c[\"text\"].split(\"\\\\n\")[0] in pass_first:pass else: save_c={\\'text\\':re.sub(space_check,\" \",c[\\'text\\']),\\'title\\':c[\\'title\\']} chunks.append(save_c) chunks save',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': 'client = weaviate.Client(\"weaviate client url\") client.batch.configure(batch_size=100) with client.batch as batch: for i, chunk in enumerate(chunks): vector = get_embedding(chunk[\"text\"]) batch.add_data_object(data_object=chunk, class_name=\"Test\", vector=vector) Query Router : Groq {\\'input\\': query, \\'output\\':\"websearch/vectorstore\"} from langchain_core.prompts import ChatPromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field \"\"\" pydantic_v1 - https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/ - 특정 schema에 맞게 output을 구성하도록 LLM에 quary할 수 있게하는 parser. \"\"\" class RouteQuery(BaseModel): \"\"\"Route a user query to the most relevant datasource.\"\"\" datasource: Literal[\"vectorstore\", \"websearch\"] = Field( ..., description=\"Given a user question choose to route it to web search or a vectorstore.\", ) \"\"\" ref. https://python.langchain.com/v0.2/docs/integrations/chat/groq/ Supported Models: https://console.groq.com/docs/models \"\"\" llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY) structured_llm_router = llm.with_structured_output(RouteQuery) system = \"\"\"You are an expert at routing a user question to a vectorstore or web search. The vectorstore contains documents related to language processing, and artificial intelligence. Use the vectorstore for questions on these topics. For all else, use web-search.\"\"\" route_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system), (\"human\", \"{question}\"), ] ) question_router = route_prompt | structured_llm_router def route_question(state): print(\"---ROUTE QUESTION---\") question = state[\"question\"] source = question_router.invoke({\"question\": question}) # output: datasource=\\'websearch/vectorstore\\' if source.datasource == \\'websearch\\': print(\"---ROUTE QUESTION TO WEB SEARCH---\") return \"websearch\" elif source.datasource == \\'vectorstore\\': print(\"---ROUTE QUESTION TO RAG---\") return \"vectorstore\" Retrieve : Weaviate Illustrated by the author {\\'input\\': query, \\'output\\':{\"documents\": documents, \"question\": query}} def retrieve(state): print(\"---RETRIEVE from Vector Store DB---\") question = state[\"question\"] # Retrieval query_vector = get_embedding(question) documents = client.query.get(\"Test\", [\"text\",\"title\"]).with_hybrid(question, vector=query_vector).with_limit(6).do() return {\"documents\": documents, \"question\": question} Relevant Grader : langchain Illustrated by the author {\\'input\\': {\"documents\": documents, \"question\": query}, \\'output\\':\"yes/no\"}',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': 'from langchain_core.prompts import ChatPromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field \"\"\" pydantic_v1 - https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/ - 특정 schema에 맞게 output을 구성하도록 LLM에 quary할 수 있게하는 parser. \"\"\" class GradeDocuments(BaseModel): \"\"\"Binary score for relevance check on retrieved documents.\"\"\" binary_score: str = Field(description=\"Documents are relevant to the question, \\'yes\\' or \\'no\\'\") # LLM with function call structured_llm_grader_docs = llm.with_structured_output(GradeDocuments) # Prompt system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\\\n Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\" grade_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system), (\"human\", \"Retrieved document: \\\\n\\\\n {document} \\\\n\\\\n User question: {question}\"), ] ) retrieval_grader_relevance = grade_prompt | structured_llm_grader_docs def grade_documents(state): print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\") question = state[\"question\"] documents = state[\"documents\"] # Score each doc filtered_docs = [] web_search = \"No\" for d in documents[\"data\"][\"Get\"][\"Test\"]: score = retrieval_grader_relevance.invoke({\"question\": question, \"document\": d[\"text\"]}) # output: GradeDocuments(binary_score=\\'yes/no\\') grade = score.binary_score # Document relevant if grade.lower() == \"yes\": print(\"---GRADE: DOCUMENT RELEVANT---\") filtered_docs.append(d) # Document not relevant else: print(\"---GRADE: DOCUMENT NOT RELEVANT---\") # We do not include the document in filtered_docs # We set a flag to indicate that we want to run web search web_search = \"Yes\" continue return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search} Web search : Tavily Illustrated by the author from tavily import TavilyClient tavily = TavilyClient(api_key=TAVILY_API_KEY) from langchain.schema import Document def web_search(state): print(\"---WEB SEARCH. Append to vector store db---\") question = state[\"question\"] documents = state[\"documents\"]',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': '# Web search tavily_response = tavily.search(query=question) web_results = \"\\\\n\".join([d[\"content\"] for d in tavily_response[\"results\"]]) web_results = Document(page_content=web_results) if documents is not None: documents.append(web_results) else: documents = [web_results] return {\"documents\": documents, \"question\": question} def decide_to_generate(state): print(\"---ASSESS GRADED DOCUMENTS---\") question = state[\"question\"] web_search = state[\"web_search\"] filtered_documents = state[\"documents\"] if web_search == \"Yes\": # All documents have been filtered check_relevance # We will re-generate a new query print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\") return \"websearch\" else: # We have relevant documents, so generate answer print(\"---DECISION: GENERATE---\") return \"generate\" Generate : Groq Illustrated by the author {\\'input\\': {\"documents\": documents, \"question\": query} + prompt, \\'output\\':\"llm result\"} from langchain_core.output_parsers import StrOutputParser # 출력물을 기본 str 형태로 받는 라이브러리 prompt = ChatPromptTemplate.from_template( \"\"\"You are a Korean-speaking assistant specializing in question-answering tasks. Use the provided context informations and relevant documents to answer the following question as accurately as possible. If the answer is not clear from the context or if you do not know the answer, explicitly state \"모르겠습니다.\" (I don\\'t know). Use three sentences maximum and keep the answer concise. All responses must be given in Korean. Based on the given information, return a very detailed response. Question: {question} Context: {context} Answer:\"\"\" ) # Chain rag_chain = prompt | llm | StrOutputParser() def generate(state): \"\"\" Generate answer using RAG on retrieved documents Args: state (dict): The current graph state Returns: state (dict): New key added to state, generation, that contains LLM generation \"\"\" print(\"---GENERATE Answer---\") question = state[\"question\"] documents = state[\"documents\"] # RAG generation generation = rag_chain.invoke({\"context\": documents, \"question\": question}) return {\"documents\": documents, \"question\": question, \"generation\": generation} Hallucination Grader : Groq Illustrated by the author {\\'input\\': {\"documents\": documents, \"question\": query}, \\'output\\':\"yes/no\"} class GradeHallucinations(BaseModel): \"\"\"Binary score for hallucination present in generation answer.\"\"\"',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': 'binary_score: str = Field(description=\"Don\\'t consider calling external APIs for additional information. Answer is supported by the facts, \\'yes\\' or \\'no\\'.\") # LLM with function call structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations) # Prompt system = \"\"\"You are a grader assessing whether an LLM generation is supported by a set of retrieved facts. \\\\n Restrict yourself to give a binary score, either \\'yes\\' or \\'no\\'. If the answer is supported or partially supported by the set of facts, consider it a yes. \\\\n Don\\'t consider calling external APIs for additional information as consistent with the facts.\"\"\" hallucination_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system), (\"human\", \"Set of facts: \\\\n\\\\n {documents} \\\\n\\\\n LLM generation: {generation}\"), ] ) hallucination_grader = hallucination_prompt | structured_llm_grader_hallucination Answer Grader : Groq Illustrated by the author {\\'input\\': {\"documents\": documents, \"question\": query}, \\'output\\':\"yes/no\"} class GradeAnswer(BaseModel): \"\"\"Binary score to assess answer addresses question.\"\"\" binary_score: str = Field(description=\"Answer addresses the question, \\'yes\\' or \\'no\\'\") # LLM with function call structured_llm_grader_answer = llm.with_structured_output(GradeAnswer) # Prompt system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\\\n Give a binary score \\'yes\\' or \\'no\\'. Yes\\' means that the answer resolves the question.\"\"\" answer_prompt = ChatPromptTemplate.from_messages( [ (\"system\", system), (\"human\", \"User question: \\\\n\\\\n {question} \\\\n\\\\n LLM generation: {generation}\"), ] ) answer_grader = answer_prompt | structured_llm_grader_answer Hallucination + Answer grader pipeline def grade_generation_v_documents_and_question(state): \"\"\" Determines whether the generation is grounded in the document and answers question Args: state (dict): The current graph state Returns: str: Decision for next node to call \"\"\" print(\"---CHECK HALLUCINATIONS---\") question = state[\"question\"] documents = state[\"documents\"] generation = state[\"generation\"] score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation}) # output : GradeHallucinations(binary_score=\\'yes\\') grade = score.binary_score',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': '# Check hallucination if grade == \"yes\": print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\") # Check question-answering print(\"---GRADE GENERATION vs QUESTION---\") score = answer_grader.invoke({\"question\": question,\"generation\": generation}) # output : GradeAnswer(binary_score=\\'yes\\') grade = score.binary_score if grade == \"yes\": print(\"---DECISION: GENERATION ADDRESSES QUESTION---\") return \"useful\" else: print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\") return \"not useful\" else: pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\") return \"not supported\" Set RAG Graph : langGraph from langgraph.graph import END, StateGraph workflow = StateGraph(GraphState) # Define the nodes workflow.add_node(\"websearch\", web_search) # web search # key: action to do workflow.add_node(\"retrieve\", retrieve) # retrieve workflow.add_node(\"grade_documents\", grade_documents) # grade documents workflow.add_node(\"generate\", generate) # generatae workflow.add_edge(\"websearch\", \"generate\") #start -> end of node workflow.add_edge(\"retrieve\", \"grade_documents\") # Build graph workflow.set_conditional_entry_point( route_question, { \"websearch\": \"websearch\", \"vectorstore\": \"retrieve\", }, ) workflow.add_conditional_edges( \"grade_documents\", # start: node decide_to_generate, # defined function { \"websearch\": \"websearch\", #returns of the function \"generate\": \"generate\", #returns of the function }, ) workflow.add_conditional_edges( \"generate\", # start: node grade_generation_v_documents_and_question, # defined function { \"not supported\": \"generate\", #returns of the function \"useful\": END, #returns of the function \"not useful\": \"websearch\", #returns of the function }, ) # Compile app = workflow.compile() Run Graph from pprint import pprint inputs = {\"question\": \"LLaMa3 구조에 대해 알려줘\"} for output in app.stream(inputs): for key, value in output.items(): pprint(f\"Finished running: {key}:\") pprint(value[\"generation\"])',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': 'output: LLaMa 3은 Meta에서 개발한 Open Source LLM(Large Language Model) 모델로, LLaMa 2를 이어 발표된 모델입니다. 이 모델은 Instruct Model과 Pre-trained Model로 8B, 70B 두 사이즈가 공개되었으며, 이는 2024년 4월 18일 기준 해당 파라미터 스케일 모델 중 가장 좋은 성능을 보이고 있습니다. LLaMa 3는 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있습니다. 이 모델은 standard decoder-only transformer architecture를 기반으로 하고, 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding합니다. LLaMa 3는 15T 개의 token으로 학습되었으며, 이는 LLaMa 2보다 약 7배 더 큰 학습 데이터를 사용하였습니다. Graph visualization ref. https://langchain-ai.github.io/langgraph/how-tos/visualization/#using-mermaid-pyppeteer from IPython.display import Image, display from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeColors print(app.get_graph().draw_mermaid()) display( Image( app.get_graph().draw_mermaid_png( draw_method=MermaidDrawMethod.API, ) ) ) × Search',\n",
       "  'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/'},\n",
       " {'text': 'MoE(Mixtures of Experts) MoE는 모든 네트워크가 활성화되는 dense model과는 달리 “experts”라고 불리는 여러 specialized subnetwork를 사용하여 입력에 따라 관련된 expert만 활성화시켜 학습과 추론을 떠 빠르고 효율적으로 만든다. MoE는 Mixtral-8x7B이 공개되며 크게 주목 받았다. Mixtral-8x7B은 MoE를 사용하여 당시 가장 높은 성능을 보인 모델이다. GPT-4도 MoE가 적용되었을 가능성이 있다는 소문이 있는데, 이는 dense model에 비해 OpenAI가 GPT-4를 저렴하게 운영할 수 있게 하기 때문이다. (여러 특수 모델들을 Merge 시키면 되니까 큰 모델을 한번에 학습하지 않아도 됨.) Core Component MoE model의 핵심 요소는 아래 두 가지이다:',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'Sparse MoE Layers (Experts) : transformer architecture의 dense feed-forward network layer를 대체하는 layer이다. (dense feed-forward network layer 여러개라고 볼 수 있음.)각 MoE layer는 expert라고 불린다. expert는 특정 유형의 정보를 처리하는데 특화된 독립적인 신경망(모델)이다. 입력에 대해 일부 expert만 활성화된다. Gate Network / Router : Router는 token의 특성과 Router가 학습한 parameter에 따라 어떤 expert에게 token을 보낼지 결정한다. 즉 어떤 token이 어떤 expert에 의해 처리될지, 어떤 expert를 활성화시킬지 결정한다. 입력의 각 부분이 가장 적합한 expert에 의해 처리되도록 한다. 이는 expert를 잘 선택할 수 있도록 학습된 모델이다. 아래 이미지에서 왼쪽은 일반적인 GPT 계열 모델(dense model)의 구조이고, 오른쪽은 GPT 계열 모델 구조에서 feedforward network가 Router + Exterts로 대체된 구조이다. 오른쪽 이미지를 보면 현재 time step에서의 token은 Router를 통해 Expert2와 Expert4이 활성화되어 이를 통과하고 있다.',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'GPT 계열의 모델들은 모두 Transformers의 Decoder 구조를 기반으로 한다. 아래 그림에서는 layer normalization 수행 시점이 Decoder 구조와는 다르게 각 block 앞에 위치해 있다. 이는 GPT3부터 변경된 구조로, 최근 나오는 많은 모델들이 이 구조를 따르고 있다. GPT3 : Language Models are Few-Shot Learners Illustrated by the author 하지만 위와 같이 복잡해진 구조로 인해 두 가지 문제점이 발생한다. MoE를 통해 모델을 병합한 후의 fine-tuning이 어렵다. tuning하는 동안 expert usage의 균형을 맞추어 적절히 학습시켜야 하는데 이것이 쉬운 일이 아니다. 추론 시 많은 Memory가 요구된다. 각 expert들이 포함된 모델을 memory에 올려야 하기 때문에 VRAM 용량이 많이 필요하다. Essential Parameter Number of experts: huggingface/transformers의 mixtral config 기준 num_local_experts 해당 parameter는 expert 수를 결정하는 parameter이다. 예를 들어 Mixtral은 8개의 expert를 가지고 있다. expert가 많을수록 모델 크기가 커져 memory 요구량이 많아진다. Number of experts/token',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'huggingface/transformers의 mixtral config 기준 num_experts_per_tok 이는 하나의 token에 대해 활성화되는 expert 수이다. 예를 들어 Mixtral은 2개이다. 많은 expert를 선택하면 정확도는 올라가겠지만 연산 속도가 느려질 수 있다. 따라서 빠른 추론과 좋은 성능을 충족시키는 적절한 expert 수 선택이 중요하다. Routing techniques Top-k Routing 입력 token에 대해 가장 높은 affinity(친화성) score를 가진 상위 $k$개의 expert를 선택하는 방법이다. token-choice routing이다. affinity(친화성) score는 일반적으로 Softmax를 통해 산출한다. 이때 선택된 $k$개의 expert 이외의 나머지 expert들은 음의 무한대로 치환하여 무시한다. Expert Choice Routing token이 expert를 선택하는 방법이 아닌 expert가 스스로 가장 잘 처리할 수 있는 token을 선택하는 방법이다.',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'Sparse Top-k Routing 각 token에 대해 일부 expert만 활성화되어 network에 sparsity를 만드는 방법이다. Sparse routing은 모든 expert가 각 token에 대해 활성화 되는 dense routing에 비해 계산량이 적다. token-choice routing 방식으로 작동된다. Mixtral-8x7B는 Sparse Routing 방식을 사용하는데, 이 방법론은 base model에서 layer normalization layer와 self-attention layer의 weight을 복사한 후 각 expert의 FFNN layer weight를 복사하여 사용한다. 이는 FFNN을 제외한 모든 parameter가 공유된다는 뜻이다. 그래서 Mixtral-8x7B가 $8*7B = 56B$ parameter가 아닌 46B parameter를 가지는 이유이다. 그리고 Mixtral-8x7B의 num_experts_per_tok는 2개인데 한 token당 추론에 소요되는 속도(FLOPs)가 dense model 기준 $2$x$7B = 14B$ 정도가 아닌 dense model 12B 정도의 속도가 나는 이유이기도 하다. 아래는 토큰별로 어떤 expert를 통과했는지 시각적으로 나타내는 이미지이다. Mixtral of Experts Router initialization(Mergkit) original MoE 방법은 expert와 router를 함께 학습하지만 최근에는 병합되는 기존 모델들을 upcycling하여 나중에 router를 초기화하는 방식으로 병합이 이루어진다.',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'Random weight를 랜덤하게 초기화. 이 방법은 동일한 expert가 계속 선택될 가능성이 있어 사용 시 이 점을 주의해야 한다. Cheap embed 입력 token의 raw embedding을 direct하게 사용하고 모든 layer에 동일한 transformation을 적용한다. 이 방법은 계산량이 적어 낮은 하드웨어 스펙에서도 잘 작동한다는 장점이 있다. Hidden positive와 negative prompt list의 hidden representation을 생성하여 LLM의 마지막 layer에서 추출한 후 이를 평균화-정규화를 하여 gate를 초기화한다. 이 방법은 token을 관련성 높은 expert에 올바르게 routing하기에 가장 적합하고 효율적이다. Mixtral Moe 아래는 Mixtral-8x7B 구조에서 MoE layer와 관련된 class들이다. class FeedForward(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.w1 = nn.Linear(args.dim, args.hidden_dim, bias=False) self.w2 = nn.Linear(args.hidden_dim, args.dim, bias=False) self.w3 = nn.Linear(args.dim, args.hidden_dim, bias=False) def forward(self, x) -> torch.Tensor: return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x)) # 시그모이드 class MoeLayer(nn.Module): def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: MoeArgs): super().__init__() assert len(experts) > 0 self.experts = nn.ModuleList(experts) self.gate = gate self.args = moe_args',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'def forward(self, inputs: torch.Tensor): inputs_squashed = inputs.view(-1, inputs.shape[-1]) # 입력된 tensor 크기를 input tensor의 마지막 차원으로 통일 gate_logits = self.gate(inputs_squashed) # gate linear layer 통과 weights, selected_experts = torch.topk( gate_logits, self.args.num_experts_per_tok ) # gate_logits에서 top-K개의 expert 뽑기 weights = nn.functional.softmax( weights, dim=1, dtype=torch.float, ).type_as(inputs) # top-K개의 expert에 대한 weight를 softmax에 넣어 score 구하기 results = torch.zeros_like(inputs_squashed) # 나중에 결과 담을 inputs_squashed tensor와 같은 크기의 초기화된 tensor 생성 for i, expert in enumerate(self.experts): batch_idx, nth_expert = torch.where(selected_experts == i) # selected_experts에서 값이 i인 요소들의 index를 반환. results[batch_idx] += weights[batch_idx, nth_expert, None] * expert( inputs_squashed[batch_idx] ) # 결과 반영 return results.view_as(inputs) class TransformerBlock(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.n_heads = args.n_heads self.dim = args.dim self.attention = Attention(args) self.feed_forward = MoeLayer( experts=[FeedForward(args=args) for _ in range(args.moe.num_experts)], gate=nn.Linear(args.dim, args.moe.num_experts, bias=False), moe_args=args.moe, ) self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps) self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps) self.args = args',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'def forward( self, x: torch.Tensor, freqs_cis: torch.Tensor, positions: torch.Tensor, mask: Optional[torch.Tensor], ) -> torch.Tensor: r = self.attention.forward(self.attention_norm(x), freqs_cis, positions, mask) h = x + r r = self.feed_forward.forward(self.ffn_norm(h)) out = h + r return out 위 코드에서 FeedForward는 아래 Mixtral strcture의 block_sparse_moe를 보면 이해가 쉽다. MixtralForCausalLM( (model): MixtralModel( (embed_tokens): Embedding(32000, 4096) (layers): ModuleList( (0-31): 32 x MixtralDecoderLayer( (self_attn): MixtralAttention( (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False) (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False) (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False) (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False) (rotary_emb): MixtralRotaryEmbedding() ) (block_sparse_moe): MixtralSparseMoeBlock( (gate): Linear4bit(in_features=4096, out_features=8, bias=False) (experts): ModuleList( (0-7): 8 x MixtralBLockSparseTop2MLP( (w1): Linear4bit(in_features=4096, out_features=14336, bias=False) (w2): Linear4bit(in_features=14336, out_features=4096, bias=False) (w3): Linear4bit(in_features=4096, out_features=14336, bias=False) (act_fn): SiLU() ) ) ) (input_layernorm): MixtralRMSNorm() (post_attention_layernorm): MixtralRMSNorm() ) ) (norm): MixtralRMSNorm() ) (lm_head): Linear(in_features=4096, out_features=32000, bias=False) )',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'Dense-MoE Hybrid Transformer 최근 open_llm_leaderboard를 보면 snowflake model들이 종종 보이는데 이는 Dense-MoE Hybrid 구조를 가진다. https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/ MoA(Mixture-of-Agents) “집단 지성”으로부터 착안된 방법론으로 볼 수 있다. 단일 LLM만 사용하는 것이 아니라 다수의 LLM이 팀을 이루어 각 LLM의 집단적 전문성을 활용하여 여러 LLM이 힘을 모아 응답하는 방법으로, 2024년 06월 Mixture-of-Agents Enhances Large Language Model Capabilities을 통해 제안되었다. Mixture-of-Agents Enhances Large Language Model Capabilities',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': '위 이미지는 MoA 작동 구조이다. MoA는 여러 layer로 구성되어 있다. 각 MoA layer에는 여러 LLM들이 있는데 위 그림에서 ‘A’들이 다 LLM이다. 각 LLM은 MoA layer에 입력된 prompt를 입력 받아 response를 생성한다. MoA layer 1의 LLM들의 응답은 layer 2에 전달된다. layer 2에도 LLM들이 있는데 layer 1의 LLM과 동일할 수도 아닐 수도 있다. 이런식으로 MoA layer를 다 거친 후 final layer에 들어가는데 final layer에는 하나의 LLM이 있다. final layer의 LLM은 input prompt + 이전 layer들에서 수집된 response를 입력 받아 최종 response를 만들어낸다. final layer에서 사용되는 prompt를 해당 논문에서는 “Aggregate-and-Synthesize prompt”라고 부르는데, 각 layer들의 응답들을 삽입하고 모델에게 이를 평가하여 최종 응답을 도출하도록 유도하는 prompt이다. Mixture-of-Agents Enhances Large Language Model Capabilities MoE에서는 각 모델들의 일부를 병합하지만 MoA에서는 각 모델 통으로 사용한다. Reference MoE Mixtral of Experts Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity A REVIEW OF SPARSE EXPERT MODELS IN DEEP LEARNING https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562 MoA Mixture-of-Agents Enhances Large Language Model Capabilities https://github.com/togethercomputer/MoA × Search',\n",
       "  'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/'},\n",
       " {'text': 'Model merging 기법은 여러 모델을 하나의 모델로 결합하는 기술이다. 최근 open llm leaderboard에도 많은 merge model들이 올라오고 있다. Mergekit 혹은 LazyMergekit을 통해 간편히 Model merge를 할 수 있다. 1. Task Vector Arithmetic “task vectors”라는 것을 통해 신경망의 동작을 변형/수정하는 방법이다. task vectors는 pre-trained model의 weight space에서 특정 task의 성능 향상을 가리키는 방향이다. 이 기법에서는 부정(negation) 혹은 덧셈과 같은 연산을 통해 vector의 수정/변형이 이루어진다. (model의 targeted behavior에 맞게) modifying “task vectors” Forgeting via negation : task vector에 대한 부정을 통해 target task의 성능은 줄이되 control task에 대해서는 성능을 유지할 수 있다. Learning via addition : task vector에 대한 덧셈을 통해 여려 task들에 대한 성능을 향상시킬 수 있다. Task analogiy : 유사한 task의 vector의 결합을 통해 추가 task에 대한 데이터 학습 없이도 해당 task 성능을 향상시킬 수 있다. 장점 Efficient Model Editing : 간단하고 효율적으로 효과적인 결합을 할 수 있다. Versatility Across Models and Tasks : 다양한 model과 task에 대해 잘 작동하는 것이 많은 사례를 통해 입증되었다.',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': '2. SLERP Spherical Linear Interpolation (SLERP)는 두 vector에 대한 부드러운 interpolate(보간법/내삽)을 위해 사용되는 방법이다. 이는 고차원 공간에서 두 모델의 고유한 특성과 곡률을 보존하면서 모델을 혼합시킨다. interpolate 데이터 지점의 고립점 내에서 새로운 데이터 지점을 구성하는 방식 구현 단계 Normalization : vector의 크기보다는 방향에 집중하여 정규화 하기 위해 입력 벡터를 단위 길이로 정규화한다. 가중치 방향의 변화는 변화의 크기보다 더 의미 있는 정보(feature learning, representation 등)를 나타내는 경우가 많다. Angle Calculation : vector의 dot product(내적)을 통해 두 vector 간의 각도를 계산한다. interpolation factor(보간 계수)와 vector 간의 각도를 기반으로 scale factor(scale 계수)를 계산한다. Vector Weighing and Summation : 계산된 계수로 기존 vector를 가중하고, 이를 합산하여 보간된 vector를 구한다.',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': '장점 Smooth Transitions : 부드러운 parameter 전환. 고차원 vector interpolate 시 부드럽게 parameter를 전환시킨다. Preservation of Characteristics : SLERP은 모델의 특성을 잘 보존하며 결합이 진행된다. 즉, 결합하는 모델들의 고유한 특성과 곡률을 유지한다. Nuanced Blending : 세밀한 혼합이 가능하다. vector space의 여러 속성들을 고려하여 두 모델의 특성을 세밀하게 반영하여 혼합한다. 단점 한번에 두 모델만 결합할 수 있다. EmbeddedLLM/Mistral-7B-Merge-14-v0.1에서 시도된 것처럼 계층적으로 둘 이상의 모델을 결합할 수도 있다. 해당 모델의 merge.yml은 https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1/blob/main/merge.yml에서 확인 가능하다. .yml 파일에서 $t$는 보간 계수이다. 3. TIES Task Vector Arithmetic과 SLERP는 비교적 오래된 모델 병합 기법인 반면 TIES는 LLM이 유행하며 새롭게 등장한 기법이다. 기존의 방법들은 각 model parameter간의 간섭을 처리하는데에 문제가 발생하곤 하는데 이 간섭은 모델 병합 이후 모델 성능 저하의 요인이 된다. 간섭의 종류에는 아래와 같이 두 가지가 있다.',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': 'Redundancy in model parameters : 모델 parameter의 중복 방지. fine-tuning 중 변경된 parameter들 중 가장 중요한 상위 k%의 변경 사항을 식별하고 나머지는 버림으로써 모델 내의 중복 parameter를 식별하고 제거한다. Disagreement between parameter signs : 병합하려는 모델들이 동일한 parameter에 대해 상반된 결과를 지니고 있을 대 충돌이 발생하는데 TIES 기법에서는 병합하고자 하는 모든 모델들에서 가장 우세한 변화 방향(다수결 느낌)으로 vector을 생성하여 충돌 문제를 해결한다. TIES는 아래와 같이 세 단계를 통해 위와 같은 간섭을 해결한다. Trim : parameter를 초기화한다. fine-tuning을 통해 변경된 parameter를 초기화하여 task별 model parameter의 중복을 줄인다. 이는 중요한 parameter인 density parameter만 유지하고 나머지는 0으로 초기화함으로써 수행된다. Resolve conflicts : model 간 parameter 값의 부호(sign)가 상이한 경우 발생하는 충돌을 해결한다. 앞서 언급한 바와 같이 모델들의 우세한 방향으로 부호를 통일시킨다. Merge : 최종 합의된 부호와 일치하는 parameter만 병합한다.',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': 'TIES는 기존의 병합법들보다 뛰어난 성능을 보였고, 특히 부호 간섭 문제를 효과적으로 해결하여 모델의 전반적인 성능을 향상시킨다고 알려져 있다. models: - model: mistralai/Mistral-7B-v0.1 # no parameters necessary for base model - model: OpenPipe/mistral-ft-optimized-1218 parameters: density: 0.5 weight: 0.5 - model: mlabonne/NeuralHermes-2.5-Mistral-7B parameters: density: 0.5 weight: 0.3 merge_method: ties base_model: mistralai/Mistral-7B-v0.1 parameters: normalize: true dtype: float16 위 구성은 mistralai/Mistral-7B-v0.1와 OpenPipe/mistral-ft-optimized-1218를 TIES기법으로 병합하는 예시이다. mistral-ft-optimized-1218(50%) + NeuralHermes-2.5-Mistral-7B(30%) weight의 합이 100이 되지 않지만 나머지 20은 normalize: true 를 통해 parameter를 내부적으로 자동 정규화시킨다. density는 두 모델 모두 parameter 중 50%만 유지. 나머지 50%는 base_model에서 가져옴. 4. DARE DARE는 주로 유사한 모델들 간의 병합에 많이 사용된다. DARE는 TIES와 접근 방식은 유사하지만 두 가지 큰 차이점이 있다. Pruning(가지치기) : fine-tuning된 weight를 무작위로 base model 값으로 재설정한다. Rescaling : model의 output의 기대값을 대체로 동일하게 유지하기 위해 weight를 rescaling한다. 둘 이상의 병합 모델의 rescaling된 weight를 base model에 scale factor와 함께 추가한다. DARE는 아래와 같이 구현된다:',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': 'Pruning Merging : 여러 모델의 parameter를 평균화하여 단일 모델을 만든다. Rescaling : 병합된 모델의 가중치를 조정하여 예상 성능을 유진한다. models: - model: mistralai/Mistral-7B-v0.1 # No parameters necessary for base model - model: samir-fama/SamirGPT-v1 parameters: density: 0.53 weight: 0.4 - model: abacusai/Slerp-CM-mist-dpo parameters: density: 0.53 weight: 0.3 - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.2 parameters: density: 0.53 weight: 0.3 merge_method: dare_ties base_model: mistralai/Mistral-7B-v0.1 parameters: int8_mask: true dtype: bfloat16 위 구성은 dare_ties기법을 통해 Mistral-7B를 기반으로 세 가지 모델을 병합하는 예시이다. (Mergekit에는 TIES의 부호 선택 단계를 포함한 DARE(dare_ties)과 포함하지 않는 DARE(dare_linear)이 있다.) 위 구성에서 병합 세 모델의 weight 합계는 1이된다. (합계는 0.9에서 1.1 사이여야 합니다) DARE 논문에서는 density parameter를 <0.5로 권장한다. 이 값을 지킬 때 더 일관된 결과를 얻는다고 한다.(위 구성에서는 0.5를 넘는다.)',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': '5. Passthrough 기존의 모든 방법들과는 다른 방법이다. 7B model 두 개를 병합하여 9B 모델을 생성하는… 특이한 기법이다. SOLAR-10.7B-v1.0에서 사용된 depth-up 스케일링과 유사한 아이디어에서 나온 기법이다. slices: - sources: - model: OpenPipe/mistral-ft-optimized-1218 layer_range: [0, 32] - sources: - model: mlabonne/NeuralHermes-2.5-Mistral-7B layer_range: [24, 32] merge_method: passthrough dtype: bfloat16 OpenPipe/mistral-ft-optimized-1218의 32개 layer와 mlabonne/NeuralHermes-2.5-Mistral-7B의 8개 layer를 병합한다. 병합 결과, 총 40개의 layer와 8.99B 모델이 생성된다. 6. MoE 관련 포스트 : MoE, MoA : MoE(Mixtures of Experts) 7. MoA 관련 포스트 : MoE, MoA : MoA(Mixture-of-Agents) Reference Geometric Algebra - Linear and Spherical Interpolation (LERP, SLERP, NLERP) https://slgero.medium.com/merge-large-language-models-29897aeb1d1a https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54 × Search',\n",
       "  'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/'},\n",
       " {'text': '학습에 사용되는 데이터는 OCR을 통해 추출한 text token과 그에 대한 bounding box가 포함된 데이터이다. 해당 모델은 기존의 Multimodal Model들이 시각 정보를 처리할 때 image encoder를 사용하는 것과는 달리 문서 내 시각 정보(layout 구조)를 이해할 때 OCR을 통해 얻어진 bounding box 정보를 사용한다. (image encoder를 사용하지 않기 때문에 모델 크기가 타 Multimodal Model에 비하여 작고, 이에 따라 추론 시간도 단축된다.) 해당 모델은 text semantic과 spatial layout 간의 관계를 포착할 때 확장된 attention mecanism을 도입한다. 본 모델은 문서 내 text 와 공간적(spatial) 정보를 각각 따로 처리하기 위해 두 modality 간의 cross-alignment를 포착할 때 classical transformer의 self-attention mechanism을 분해하여 사용한다. 구체적으로, 각 modality에 대한 attention score 뿐만 아니라 두 modality간의 관계를 포착하는 attention score도 계산한다. 다양한 layout과 문서 내 시각적 정보를 잘 파악하기 위해 사전 학습 과정에서 infill text segment task를 학습한다. infill text segment task는 다양한 layout을 지닌 문서에서 이전 text 정보가 현재 text와 관련 없을 가능성이 있기 때문에 이러한 경우를 파악하기에 적합한 과제이다.',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': '사전 학습 후에는 네 가지 핵심 문서에 대해 intelligence task에 대한 fine-tuning(instruction-tuning)을 수행한다. 본 연구의 contribution은 아래와 같다: visual document를 이해하기 위해 설계된 light-weight LLM 개발 text and layout modality 간의 cross-alignment를 위한 분리된 attention mechanism 제안 불규칙적인 layout을 효과적으로 파악하기 위한 infilling pre-training objective 도입 visual document intelligence task 수행을 위해 특수 생성된 instruction-tuning dataset 구축 2. Related Work 2.1 LLM Architectures Autoregressive Infilling autoregressive infilling approach에는 두 가지 방법이 있다. FIM(fill-in-the-middle): single span 예측 GLM(General language model pretraining with autoregressive blank infilling): multiple span 예측',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'OpenAI의 FIM approach는 문서를 (prefix, middle, suffix) 이렇게 세 부분으로 나눈다. 나누어진 문서는 (prefix, suffix, middle)로 재구성되어 모델이 middle segment를 예측할 수 있게 한다. 이 과정은 [PRE], [SUF] 그리고 [MID] token을 기준으로 수행된다. 즉, 실제 수행 시 [MID] token을 예측 시작 기준으로 삼는다. 이와 같은 방법은 autoregressive model이 middle part가 누락된 텍스트를 채우는 방법을 학습하게 한다. GLM은 multiple span을 채우는 과제이다. 빈칸을 채우기 위해 해당 과제에서는 [blank_mask]와 [start_to_fill] 쌍을 사용한다. Disentangled attention Disentangled attention은 DeBERTa에서 처음 도입되었다. 해당 mechanism은 token embedding과 relative positional encoding을 합치지 않고 별도로 유지하며, attention weight 를 계산할 때 분리된 matrix를 사용하여 각각 독립적으로 처리한다. 이는 content와 position이 각각 분리된 attention alignment를 학습할 수 있게 하는데, 이것이 NLU benchmark에서 DeBERTa가 RoBERTA-large와 T5를 능가하게 한 지점이라고 알려져 있다. 3. DocLLM Framework DocLLM: A layout-aware generative language model for multimodal document understanding',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': '3.1 Model Architecture DocLLM은 auto-regressive transformer language model의 decoder 구조를 기반으로 개발되었다. transformer decoder 모델을 구성하는 각 transformer block은 multi-head self-attention layer 과 fully connected feed forward network를 포함하며, transformer block은 여러개로 구성될 수 있다. Standard language model은 일반적으로 unimodal로, text만 입력으로 받는다. 그리고 Visual Language Model은 text와 image data를 입력받아 각각의 encoder(model)를 통해 얻어진 modlity 정보를 통합시킨다. 반면 DocLLM은 OCR을 사용하여 text token에 대한 공간적(spatial)위치와 크기를 얻고 이를 활용하여 시각 정보를 통합한다. 3.2 Disentangled Spatial Attention classical transformers attention mechanism input toeken sequence $x = (x_1, …, x_i, …, x_T )$가 있을 때, 일반적인 attention에서는 학습된 embedding matrix와 token position에 대한 학습된 parameter set을 사용하여 input token을 우선 hidden vector $H\\\\in \\\\mathbb{R}^{T\\\\cdot d}$ self-attention head은 toekn $i$와 $j$간의 attention score를 계산한다. $W^{q}\\\\in \\\\mathbb{R}^{d\\\\cdot d}$과 $W^{k}\\\\in \\\\mathbb{R}^{d\\\\cdot d}$는 projection matrix이고 윗 첨자 $t$는 text modality를 나타낸다. attention score $A \\\\in \\\\mathbb{R}^{T\\\\cdot T}$와 또다른 projection matrix $W^{v}$는 hidden vector $H^{‘}$를 계산하는데 사용되며, 이 hidden vector는 다음 layer의 입력으로 사용된다.',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'DocLLM attention mechanism DocLLM은 text token과 그에 대한 bounding box 정보가 쌍을 이룬 data를 input으로 받는다. 본 모델은 bounding box를 separate hidden vector로 encoding하고 attention mechanism을 네 가지 score로 분해한다: text-to-text, text-to-spatial, spatial-to-text, spatial-to-spatial. 각각의 점수의 균형을 맞추기 위해 projection matrix와 hyperparameter를 사용한다. spatial information에 대한 hidden vector는 layer 전반에 걸쳐 재사용된다. DocLLM의 input은 $\\\\mathrm{{(x_i,b_i))}}_{i=1}^{T}$으로 표현되는데 여기에서 $b_i$은 $x_i$에 해당하는 bounding box (left, top, right, bottom)이다. LLM이 spatial information을 포착할 수 있도록 하기 위해 bounding box를 hidden vector $S\\\\in \\\\mathbb{R}^{T\\\\cdot d}$로 encoding한 후 attention matrix를 네 가지 다른 score로 분해한다: text-to-text, text-to-spatial, spatial-to-text, spatial-to-spatial.',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': '여기에서 $W^{s,q}\\\\in \\\\mathbb{R}^{d\\\\cdot d}$와 $W^{s,k}\\\\in \\\\mathbb{R}^{d\\\\cdot d}$는 classical attention과 달리 spatial modality에 해당하는 새로 도입된 projection matrix이다. 그리고 $\\\\lambda$들은 각 점수들(4 가지)의 상대적 중요성을 제어하는 hyperparameter이다. input hidden vector $H^{‘}$는 classical attention과 동일하게 계산된다. hidden vector $S$는 모든 layer에 걸쳐 재사용되지만, 각 layer는 다른 projection matricx를 사용할 수 있다. bounding box정보를 encodding하는데 필요한 추가 parameter 수는 VLM과 같이 image model을 따로 두는 방식보다 훨씬 적다. 3.3 Pretraining DocLLM은 unlabeled document에 대해 self-supervised 방식으로 사전학습 한다. 학습 과제는 text infilling objective로, 이를 수행하기 위해 사전 학습이 진행하는 동안 OCR 정보로부터 식별된 text block음 무작위로 masking되며, masking된 block이 shuffle된(GLM 방식과 동일). block은 본 블로그 DocLLM Framework 아래 있는 그림에서 “Name”, “John Doe”, “Doctor”과 같은 것들이다. (이 block들의 span 범위는 더 커질 수 있다.) block infilling은 시작과 끝을 나타내는 special token과 함께 autoregressively하게 수행된다. 해당 task는 모델이 문맥적으로 빈칸을 적절히 완성할 수 있게한다. 이와 같은 방법은 사전학습 단계에서만 사용되고, fine-tuning 단계에서는 사용되지 않는다. 3.4 Instruction Tuning',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'DocLLM은 16개 dataset과 그에 해당하는 OCR을 사용하여 4가지 DocAI task를 처리할 수 있도록 instruction-tuning된다. 4가지 DocAI task는 아래 표에 기재된 바와 같이 Visual Question Answering (VQA), Natural Language Inference (NLI), Key Information Extraction (KIE), Document Classification (CLS)이다.',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'DocLLM: A layout-aware generative language model for multimodal document understanding 본 연구에서는 DocAI dataset의 탬플릿으로부터 파생된 다양한 instruction으로 instruction tuning을 수행했다. 또한 supervised fine tuning (SFT) 수행 시 입력되는 instruction의 다양성이 높을수록 해당 모델의 zero-shot generalization 능력이 향상되기 때문에 최대한 다양한 template을 사용하려고 했다(“Finetuned language models are zero-shot learners”, “Scaling instruction-finetuned language models”, “ Training language models to follow instructions with human feedback”). 각 template은 다른 질문을 제시하고, 몇몇 경우 다른 유형의 답변을 반환하도록 했다. dataset의 탬플릿은 “Modularized multimodal large language model for document understanding”과 “Universal ocr-free visually-situated language understanding with multimodal large language model” 연구에서도입된 탬플릿을 가능한 재사용하였다. Visual Question Answering(VQA) DocVQA, WikiTableQuestions(WTQ), VisualMRC, DUDE, BizDocs2를 수집하여 VQA instruction-tuning data mix를 구축한다. VQA 학습 시 input은 위 표에 나타난 바와 같이 하나의 instruction template이다. {document} {question} e.g. {document} What is the deadline for scientific abstract submission for ACOG - 51st annual clinical meeting? Natural Language Inference(NLI) 본 연구에서 구축한 instruction-tuning data mix를 DocAI NLI dataset으로 사용하기에는 수량 부족한 관계로, 여기에 TabFact dataset을 추가하여 사용한다. {document}\\\\\"{statement}\\\\\", Yes or Ne? e.g. {document} \\\\\"The UN commission on Korea include 2 Australians.\\\\\", Yes or No?',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'Key Information Extraction(KIE) Kleister Charity(KLC), CORD, FUNSD, DeepForm, PWC, SROIE, VRDU ad-buy (with random train-test splitting), and BizDocs를 수집하여 KIE instruction-tuning data를 구축하였다. instruction template 유형은 위 표에 기재된 바와 같이 총 3가지를 사용하였다: extraction, internal classification, MCQ. [Extraction] -> {document} What is the value for the \\\\\"{key}\\\\\"? e.g. {document} What is the value for the \\\\\"charity number\\\\\"? [Internal classification] -> {document} What is \\\\\"{value}\\\\\" in the document? [MCQ] -> {document} What is \\\\\"{value}\\\\\" in the document? Possible choices: {choices}. Document Classification(CLS) RVL-CDIP, BizDocs를 수집하여 CLS instruction-tuning data를 구축하였다. 이 작업에는 두 가지 유형의 template이 사용되었다. [MCQ] -> {document} What type of document is this? Possible choices: {choices}. e.g. {document} What type of document is this? Possible answers: [budget, form, file folder, questionnaire]. [Internal classification] -> {document} What type of document is this? 4. Experiments 4.1 Model Setup and Training Details 실험에 사용된 모델은 Falcon-1B를 기반으로 한 DocLLM-1B, Llama2-7B를 기반으로 한 DocLLM-7B로 두 가지 버전이 있다. 실험 시 사용된 hardware specification은 24GB A10g GPU 하나고 fully sharded data parallelism으로 학습이 진행되었다. 실험에 사용된 모델의 각종 config와 hyperparameter는 아래 표와 같다: DocLLM: A layout-aware generative language model for multimodal document understanding DocLLM-1B의 경우에는 1 epoch의 pre-training, 10 epoch의 instruct-tuning을 수행했고, DocLLM-7B의 경우에는 1 epoch의 pre-training, 3 epoch의 instruct-tuning을 수행했다. 4.2 Downstream Evaluation 본 연구에서는 두 가지 설정 하에 실험을 진행한다.',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'Same Datasets, Different Splits (SDDS) 일반적인 방법이다. 16개의 데이터셋에 대해 trainset을 학습하고, 각 trainset을 학습한 모델을 상응하는 testset으로 평가하는 방식이다. 이와 같은 방식 아래에서 zero-shot setting으로 실험한 결과, GPT-4와 Llama2를 포함한 다른 모델들보다 16개 데이터셋 중 12개에서 더 좋은 성능을 보였다. 특히 KIE와 CLS와 같이 레이아웃 집약적인 task에서 특히 좋은 성능을 보였다. 아래는 SDDS 실험 결과이다. DocLLM: A layout-aware generative language model for multimodal document understanding',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'Same Tasks, Different Datasets (STDD) 동일한 task를 수행하는 모델이지만 testdata와 traindata의 출처를 상이하게 설정한 실험이다. 예를 들어 16개의 dataset에서 11개의 dataset을 train에 사용하고 나머지 3개의 dataset을 평가에 활용한다. 이때 (1 )각 task에 대해 최소 하나의 dataset을 SFT 수행에서 제외시키고, (2) data split 시 각 task마다 충분한 trainset을 확보하고, (3) data leakage를 방지한다. 예를 들어 동일한 출처로부터 data가 중복되지 않도록 한다. 마지막으로 (4) 가능한 유명한 benchmark를 사용한다. 실험 결과, 5개의 데이터셋 중 4개에서 Llama2를 능가했으며, 특히 KIE 작업에서 두 개의 데이터셋에서 최고 점수를 기록했다. 하지만 classification accuracy가 낮은데, 이는 하나의 분류 dataset을 학습했기 때문에 새로운 데이터셋에 대한 generalization이 떨어진 상황으로 파악된다. 아래는 STDD 실험 결과이다. DocLLM: A layout-aware generative language model for multimodal document understanding 5. Ablation Studies 5.1 Disentangled Spatial Attention spatial-to-spatial interaction에 집중할 때 문서 이해력이 가장 높은 것으로 확인되었다. 이 결과는 문서 분석 시 공간적 feature가 중요하다는 것을 보여준다. DocLLM: A layout-aware generative language model for multimodal document understanding',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': '5.2 Autoregressive Block Infilling Pretraining task로 block infilling + spatial modality를 학습한 모델이 가장 높은 성능을 보이는 것으로 확인되는데 이 또한 공간적 정보가 중요하다는 것을 나타낸다. DocLLM: A layout-aware generative language model for multimodal document understanding 5.3 Prefix Decoder and Causal Decoder 해당 실험은 prefix decoder와 causal decoder를 비교하는 실험이다. 실험 결과, 여러 조합에서 두 decoder 간의 차이는 미미했지만, causal decoder가 약간 더 좋은 성능을 보였따. DocLLM: A layout-aware generative language model for multimodal document understanding DocLLM: A layout-aware generative language model for multimodal document understanding Reference DocLLM: A layout-aware generative language model for multimodal document understanding https://llava-vl.github.io/ × Search',\n",
       "  'source_title': 'DocLLM: A layout-aware generative language model for multimodal document understanding\\n      04 Jun 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/04/docllm/'},\n",
       " {'text': 'OpenVLM Leaderboard https://huggingface.co/spaces/opencompass/open_vlm_leaderboard LLAVA 관련 포스트 : LLaVA : Visual Instruction Tuning Idefics2 https://huggingface.co/blog/idefics2 Fine-tune-IDEFICS-Vision-Language-Model 8B모델로, Idefics1보다 OCR 성능이 향상되었고, VAQ benchmark에서 공개 당시 최고 성능을 기록했다. 학습데이터는 webdocuments (Wikipedia,OBELICS), image-caption pairs (Public Multimodal Dataset, LAION-COCO), OCR data (PDFA (en), IDL and Rendered-text, image-to-code data (WebSight)) 등을 혼합하여 사용했다. Florence-2 Gemini Gemini는 Google에서 공개한 multi-modal large language model로, 모델 사이즈에 따라 Ultra, Pro, Nano 버전이 있다. Ultra 버전은 세 버전 중 모델 사이즈가 가장 큰 버전으로, 복잡한 multimodal task를 위해 개발되었다. Pro는 중간 사이즈의 모델로, performance와 scalability의 균형을 잘 맞춘 모델이다. 마지막으로 Nano 버전은 세 버전 중 가장 작은 모델이다. 버전에 상관 없이 해당 모델은 vision과 language 뿐만 아니라 audio와 video도 처리할 수 있다. BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models(BLIP) BLIP에서는 lightweight Querying Transformer(Q-Former)로 vision과 language modality를 연결한다. Q-Former는 아래와 같이 두 단계의 사전 학습 과정을 거치는데, 이를 통해 vision-language alignment를 모델에 효과적으로 학습시켜 high-quality text output 생성을 가능하도록 한다. frozen image encoder로부터 visual feature를 추출하여 vision-language representation 학습 frozen language model과 연결하여 vision-to-language 생성 학습',\n",
       "  'source_title': 'VLM : Models Summary(unfinished post)\\n      30 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/30/vlm_models/'},\n",
       " {'text': 'BLIP-2는 기존의 모델들보다 적은 trainable parameter로 다양한 vision-language task에 대해 좋은 성능을 보인다는 점에서 주목 받았다. CLIP Contrastive Language-Image Pre-training(CLIP) 웹 페이지에서 크롤링한 약 4억 개의 image-text pair dataset을 학습한 모델. 두 modality encoder 간의 연결이 dot product로만 되어 있어 복잡한 task 수행에는 최근 나오는 모델들에 비해 추론 품질이 좋지는 않다고 한다. × Search',\n",
       "  'source_title': 'VLM : Models Summary(unfinished post)\\n      30 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/30/vlm_models/'},\n",
       " {'text': 'Vision Language Model(VLM)은 visual data와 text data를 동시에 학습하여 두 modality 정보를 모두 활용할 수 있는 모델이다. VLM의 종류는 다양하지만 image와 text를 입력 받아 text 생성하는 모델이 가장 일반적이다. 이와 같은 모델은 vision model과 language model을 융합(fusion)시켜 만들어진다. 이때 융합(fusion)은 모델이 visual 정보와 그에 해당하는 text 정보를 입력 받아 두 modality의 정보를 연관/연동 시키는 방법을 학습함으로써 이루어질 수 있다. Edited by author Vision Language Model에는 one-size-fits-all architecture가 딱히 없다. vision language model의 구조는 task나 output에 따라 달라질 수 있다. task별 input과 output은 본 블로그 Vision Language Model : Applications 게시물에서 확인 가능하다. 가장 일반적인 task는 아래 그림과 같이 input으로 image와 그와 관련된 text를 입력 받아 두 입력이 요구하는 정답을 text로 반환하는 것이다. Illustrated by the author',\n",
       "  'source_title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/'},\n",
       " {'text': '위 그림과 같이 두 유형의 data를 입력 받은 후 각 modality에 적합한 encoder를 거친 후 각각의 encoder에서 나온 embedding 결과를 융합시키기 위한 fusion layer를 거치고 마지막으로 fused representation 결과를 출력시킬 output block을 통해 최종 결과가 반환된다. 따라서 Vision Language Model은 크게 Vision Encoder, Language Encoder, Fusion Block, Output Block으로 구성된다. 1. Vision Encoder Vision Encoder는 visual input을 처리한다. 해당 부분에서는 visual data로부터 meaningful representation을 추출하여 이를 vector representation로 변환한다. 2024년 5월 기준 CLIP, ViT 등의 모델이 VLM model의 vision encoder로 많이 사용되고 있다. huggingface의 OpenVLM Leaderboard에서 동향을 확인할 수 있다. 2. Language Encoder Language Encoder는 textual input을 처리한다. 해당 부분에서는 text data의 semantic information이 잘 포함된 embedding을 생성한다. Language Encoder가 text data의 seqence를 잘 파악할 수록 VLM의 성능이 올라간다. 3. Fusion Block Fusion Block은 Vision Encoder와 Language Encoder를 통해 visual과 textual data를 각각의 vector representation으로 encoding한 이후 이를 잘 융합시켜 시각적 정보와 텍스트 정보를 모두 포괄하는 최종 embedding 결과를 도출하는 단계이다. Fusion 방식에는 매우 다양한 방법론이 존재한다. 대표적인 방법으로는 아래 다섯 가지가 있다. Contrastive Learning PrefixLM Multi-modal Fusing with Cross Attention MLM/ITM No Training',\n",
       "  'source_title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/'},\n",
       " {'text': '3.1 Contrastive Learning Contrastive Learning(대조 학습)은 입력된 image와 text를 동일한 vector space에 놓고 쌍을 이룬 image-text의 각 embedding 거리를 최소화 하면서 맞춰가는 방법이다.(맞지 않는 쌍 간의 거리는 최대화 시키다.) 이때 거리 최소화 작업에 사용되는 vector간 거리 산출 공식은 보통 cosine distance formula이다. OpenAI도 CLIP(Contrastive Language-Image Pre-training) 모델에서 cosine distance formula를 사용하였다. https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40 3.2 PrefixLM PrefixLM은 Transformer의 Encoder와 Decoder가 결합된 모델로, prefix token set이 주어졌을 때 다음 token을 예측하는 모델이다. PrefixLM은 visual embedding과 text embedding을 병렬적으로 처리한다. 아래 그림과 같이 Transformer의 Encoder가 concat된 image와 text emebedding(-> prefix emebedding)을 입력 받고 Decoder 그 뒤에 이어질 token을 예측한다. 예측 token은 text이다. SIMVLM: SIMPLE VISUAL LANGUAGE MODEL PRETRAINING WITH WEAK SUPERVISION 3.3 Multi-modal Fusing with Cross Attention Multi-modal Fusing with Cross Attention은 cross-attention mechanism을 통해 시각적 정보를 language model에 통합시키는 방법이다. 아래 이미지와 같이 visual encoder를 통해 image를 embedding한 후 이 embedding 결과를 language model의 cross-attention layer에 입력한다. 이 방법을 사용한 대표적인 모델은 VisualGPT이다. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning',\n",
       "  'source_title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/'},\n",
       " {'text': '3.4 Masked-Language Modeling Masked-Language Modeling은 self-attention을 통해 input image와 input text를 align시킬 때 사용되는 task로, 이를 학습한 모델은 주로 주어진 caption이 어떠한 이미지에 대한 것인지 예측하는 과제로 많이 사용된다. MASKED VISION AND LANGUAGE MODELING FOR MULTI-MODAL REPRESENTATION LEARNING 4. Output Block Output Block은 fused representation(embedding)을 통해 task에 따라 적절한 output을 생성하는 부분이다. Output Block은 해결하고자 하는 task에 따라 다르게 설계된다. Reference VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning MASKED VISION AND LANGUAGE MODELING FOR MULTI-MODAL REPRESENTATION LEARNING SIMVLM: SIMPLE VISUAL LANGUAGE MODEL PRETRAINING WITH WEAK SUPERVISION https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40 × Search',\n",
       "  'source_title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/'},\n",
       " {'text': 'image (혹은 video)와 해당 image에 대한 질문을 text로 입력받아 text로 질문에 대한 답을 반환하는 task. 예를 들어 아래 그림에 대해 “저 꽃은 무슨 색인가요?”라는 질문을 하고 “주황색”이라는 답변을 받는 task이다. Openai Image generator 1.2 Visual Captioning and Description Illustrated by the author image에 대한 설명을 생성하는 task. image를 입력 받아 text로 설명을 반환하거나, image와 prompt를 함께 입력 받아 설명을 반환할 수도 있다. 1.3 Identifying Objects in Images with Textual Cues Illustrated by the author image와 그에 대한 설명을 입력 받아 설명에 등장한 객체를 이미지 내에서 찾아내는 task. 예를 들어 아래 그림과 함께 “사람이 물을 마시고 있다” 라는 caption을 입력받으면 입력된 텍스트 정보 내에 등장한 객체(“사람” 과 “물”)을 이미지 내에서 찾는 task이다. Openai Image generator 1.4 Visual Commonsense Reasoning image를 입력 받아 image 내의 객체를 식별한 후 객체 간의 관계를 파악하여 이미지에 나타난 객체들의 상관관계를 상세히 설명하는 task. https://visualcommonsense.com/',\n",
       "  'source_title': 'VLM : Applications\\n      28 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/28/vlm_task/'},\n",
       " {'text': '1.5 Visual Summarization image를 입력 받아 이미지에 대한 설명을 간략하게 요약하는 task. 이 과제를 수행하기 위해서 model은 image 내 객체와 객체간의 관계를 잘 파악하고 그에 대한 설명을 잘 요약하는 능력이 있어야 한다. 1.6 Visual Generation https://visualcommonsense.com/ 생성하고자 하는 image에 대한 설명이나 prompt를 텍스트로 입력받아 image를 생성하는 task. 2. Retrieval Tasks 2.1 Vision-Language Navigation (VLN) Vision-Language model에 text 혹은 image가 포함된 질문을 받아 답변을 도출하는 task. 이와 같은 taks는 어떠한 물품에 대한 설명 혹은 유사한 물품에 대한 이미지 정보가 입력되면 마트 같은 곳에서 해당 물건과 그 물건의 위치를 반환하는 시스템에 활용될 수 있다. × Search',\n",
       "  'source_title': 'VLM : Applications\\n      28 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/28/vlm_task/'},\n",
       " {'text': '1. Introduction machine-generated instruction-following data를 사용하여 LLM을 instruction tuning하는 것이 새로운 task에 대한 zero-shot 성능을 향상시킨 다는 점은 이미 여러 연구를 통해 입증되었지만 이를 multimodal task에 적용하는 연구는 2023년 12월 11일 기준 아직 활발히 연구되고 있지 않다. 본 논문은 language-only GPT-4를 통해 multimodal language-image instruction-following data를 생성하고 이를 통해 instruction tuning을 진행하는 방법론을 제안한다. 이와 같이 생성된 모델을 LLaVA(Large Language and Vision Assistant)칭하며, 해당 모델은 vision encoder와 LLM을 end-to-end로 연결한 모델이다. Visual instruction following 연구를 위해 본 논문에서는 두 가지 evaluation benchmark를 만들었다. 해당 benchmark를 통해 GPT-4와 비교 평가한 결과, GPT-4에 준하는 성능을 보이는 것을 확인하였다. 본 논문의 contribution은 아래와 같이 요약할 수 있다: Visual instruction-tuning 방법론 제안 Visual instruction-tuning 방법론을 제안함으로써 language-image multimodal 분야에 instruction-tuning을 적용을 통해 general-purpose visual assistant 구축',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'Multimodal instruction-following data 구축 vision-language instruction-following data의 수량이 대규모 모델을 학습하기에 부족한 것이 본 연구의 큰 문제였다. 본 논문에서는 이 문제를 ChatGPT/GPT-4를 사용하여 image-text pair data를 instruction-following format으로 변환하는 pipeline을 구축함으로써 해결하였다. 해당 데이터셋을 통해 모델이 이미지에 대해 더 정교하게 이해함과 동시에 user instruction을 더 잘 수행할 수 있게 되었다. Large multimodal model 개발 본 논문에서는 visual encoder로 CLIP을, language decoder로 Vicuna(LLaMa를 instruction tuning한 모델) 선정한 후 이를 연결하여 Large multimodal model을 개발하였고, 이 모델에 instructional vision-language data을 학습시켰다. Multimodal instruction-following benchmark 생성(LLaVA-Bench) 본 논문에서는 두 가지 Multimodal instruction-following benchmark를 생성하여 공개하였다. Open-source multimodal instruction data, codebase, model checkpoints, visual chat demo를 모두 공개한다. Instruction Tuning',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'Google이 발표한 FLAN을 통해 처음 제안된 방법론이다. 다양한 task가 instruction 형태로 들어가 있는 data(질문(prompt)-답변(completion) 쌍으로 구성)를 통해 Pre-trained model을 fine-tuning하는 방법론. Instruction Tuning을 통해 사람이 원하는 대답을 생성할 수 있도록 모델의 weight를 업데이트 하는 방법이다. Instruction Tuning은 자연어처리 분야에서 이루어진 많은 연구들을 통해 새로운 task에 대한 unseen task에 대한 성능을 향상시키는 것이 입증되었다. 즉, Instruction Tuning이 수행된 모델 prompt로 context나 예시를 입력하지 않아도 적절한 답변을 만들어내는 것이 확인되었다. 하지만 많은 task에 대한 data를 수집하는 데에 많은 비용이 들고, 모델이 학습한 instrction data와 인간의 preference 간의 align이 잘 되지 않아 생기는 문제들도 있다. 그리고 hallucination(환각현상 : 거짓 정보 반환) 문제도 있다. hallucination 문제 완화를 위해 RLHF나 RAG 등이 사용되고 있다.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'Lora 등의 기법을 사용하지 않으면 fine-tuning과 마찬가지로 모델 전체 weight가 update됨. 모델 자체가 엄청 크기 때문에 매우 많은 계산 자원과 시간이 필요하기 때문에 Lora와 같은 기법을 사용하거나 양자화를 수행하는 추세. LoRA는 모델의 특정 파라미터를 저랭크(low-rank)로 분해하여 학습하는 방법으로, 효율적으로 파라미터 수를 줄여 메모리 사용량과 계산 비용을 감소시킴.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '2. GPT-assisted Visual Instruction Data Generation 우선 instruction tuning을 위해 Text-Only GPT-4나 ChatGPT를 활용하여 visual instrction data를 생성한다. 생성 과정을 간략히 요약하자면 다음과 같다: Visual Content를 인지할 수 없는 GPT-4모델을 위해 Image Caption과 Bounding Box를 이용하여 image에 대한 설명을 제공하여 onversation, Detailed Description, Complex Reasoning 문답 형태로 총 158,000개의 데이터셋을 생성한다. CC, Laion과 같은 기존의 image-text pair dataset은 image와 caption text로 구성된 data이다. LLaVA 학습에는 multimodal instruction following data가 필요하다. 이 데이터를 human crowd-scouring으로 구축 하려니 시간도 많이 소요되고 less well-defined data(데이터 정의가 잘 되지 않은 데이터)가 구축될 가능성이 있어 연구진들은 ChatGPT/GPT-4를 활용한 데이터 구축 방법을 고안하였다. 연구진들은 기존에 공개된 Image-Text Pair data를 활용하여 image $X_v$와 그에 해당하는 caption $X_c$가 있을 때, ChatGPT/GPT-4에게 이미지 내용을 설명하도록 하는 질물 세트 $X_q$를 생성하라고 지시하여 instruction following data를 구축하였다. 이와 같은 방법은 저비용으로 dataset 구축을 가능하게 하지만 instruction과 response에 대해 다양성과 in-depth reasoning이 부족하다는 단점이 있다. 생성된 데이터는 아래와 같은 형태가 된다. Visual Instruction Tuning',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '위 방법으로 Instruction-Following Dataset을 구축할 때 두 가지 문제가 있었다. 아래는 문제와 문제 완화를 위해 적용한 방법이다: 1. language-only GPT-4/ChatGPT를 사용했기 때문에 모델이 Visual Content를 인지할 수 없는 문제 이 문제 해결을 위한 대전제는 image를 LLM이 인식할 수 있는 시퀀스로 encoding 가능하게 하는 것이다. (i) Caption Caption을 GPT-4/ChatGPT에 함께 입력함으로써 language-only 모델이 visual scene을 알 수 있도록 한다. (ii) Bounding box visual scene에서 특정 객체의 위치 정보를 좌표로 prompt에 넣어주어 language-only 모델이 객체를 인식할 수 있도록 한다. 2. Instruction-following data 유형 정의 및 유형별 포함 내용 정의 문제 Conversation, Detailed description, Complex reasoning 유형을 구축하여 세부 사항을 정의하여 총 158,000개의 data를 구축하였다. (i) Conversation (58,000개 생성) - 주어진 이미지에 대해 Assistant와 Human이 대화하는 형태로 구성. - Assistant가 image를 보고 Human의 질문에 답하는 것과 같은 형식. - image 내의 직관적인 시각적 정보에 대한 QAset으로 생성. (객체 유형, 객체 수, 객체 동작, 객체 위치, 객체 간의 상대적 위치 등) (ii) Detailed description (23,000개 생성)',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '- image의 caption과 box 정보를 제공한 후 이에 대한 상세 설명을 생성하라는 prompt를 입력하여 GPT-4가 image에 대한 상세 설명을 생성하도록 함. - prompt는 다양하게 준비해 놓고 랜덤하게 입력 (iii) Complex reasoning (77,000개 생성) - 위 두 유형을 통해 시각적 내용 자체에 중점을 둔 data를 생성하한 후 이를 바탕으로 심층적 추론 질의응답셋을 GPT-4를 통해 생성한다. - 답변에 단계별로 엄격한 논리를 포함하도록 prompt를 생성하고 이 prompt와 caption 그리고 box 정보를 모델에 함께 임력한다. 위 내용을 요약 정리하면 아래와 같다.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '이런 데이터에서 caption과 box 정보를 바로 아래 이미지에 나온 pormpt의 [“context”] 에 입력하여 language only GPT-4에 넣는. 이미지는 입력 x Visual Instruction Tuning Visual Instruction Tuning 각 유형별로 아래와 같이 데이터가 생성된다. Visual Instruction Tuning 3. Visual Instruction Tuning 3.1 Architecture LLaVA의 visual encoder와 LLM($f_ϕ$)을 연결한 구조를 가진다. 현재는visual encoder와 LLM에 대해 매우 여러 조합의 버전으로 LLaVA가 나왔지만 해당 논문에 작성된 LLaVA는 visual encoder로 CLIP Vision Encoder인 VIT-Large 14와 LLM으로는 LLama를 instruction tuning한 Vicuna를 용하였다. visual encoder와 LLM은 단순한 linear layer로 연결한다. 즉, LLM의 Word Embedding Sapce와 Visual Encoder의 Fueature Space를 Linear 결합한다. Visual Instruction Tuning 3.2 Training 각 image $X_v$에 대해 turn 수가 $T$개인 multi-turn conversation data $(X_1^q, X_1^a, \\\\cdots, X_T^q, X_T^a)$를 생성한다. 이때 답변은 dataset 상에서 Assistant에 해당하는 것으로 하고, $t$번째 turn의 instrction $X_t^{instruct}$은 아래와 같이 정의한다. 첫 번째 turn에만 image $X_v$를 입력하고 나머지 turn들은 text이기 때문에 첫 turn에 입력되는 $X_v$와 $X_q$의 순서는 크게 상관 없다. Visual Instruction Tuning multimodal instruction-following sequence는 아래 이미지와 같은 형식으로 표현된다. Visual Instruction Tuning',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'LLM에 위 데이터를 넣어 original auto-regressive training task를 통해 token을 예측하며 학습을 수행한다. 위 예시는 두 turn만 나타내지만 실제 turn 수는 더 다양하다. 실제 구현은 사용된 모델에 따라 달라지겠지만 LLM으로 Vicuna를 사용할 때는 을 ### 구분자로 설정했다. 모델은 assistant의 답변과 답변 중단 지점을 예측하도록 학습되기 때문에 위 이미지에서 초록색으로 표시된 sequence/tokens만 loss 계산에 사용된다. sequence의 길이가 $L$일 때 정답 $X_a$에 대한 확률은 아래와 같이 계산된다. 이때 $X_{instruct<i}$는 현재 예측 token인 초록색 글씨 $x_i$ 이전의 모든 turn에 대한 instruction token을 의미하고, $X_{a<i}$는 현재 예측 토큰인 $x_i$ 이전의 모든 turn에 대한 answer token을 의미한다. 아래 수식은 image와 query에 대해 answer가 나올 확률을 계산할 때 image, instruction, answer에 대한 이전의 모든 token을 집어넣고 이를 모두 곱하여 확률값을 계산한다. Visual Instruction Tuning LLaVA는 두 단계의 instruction-tuning 절를 거친다.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'Stage 1. Pre-training for Feature Alignment: CC3M의 subset을 기반으로 projection layer matrix만 업데이트한다. Stage 2. Fine-tuning End-to-End: 투영 행렬과 LLM 모두 두 가지 다른 사용 시나리오에 맞게 업데이트됩니다: Multimodal Chatbot(Visual Chat): LLaVA는 일상적인 사용자 지향 application(downstream) 수행을 위해 multimodal instruction-following data로 fine-tuning을 진행한다. Science QA: LLaVA는 science domain을 위해 multimodal reasonsing dataset으로 로 fine-tuning을 진행한다.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '3.2.1 Stage 1: Pre-training for Feature Alignment Pre-Training 과정에서는 image Caption을 활용하여 Visual Tokenizer를 LLM에 호환 가능하도록 학습시키는 것이 주요 목적이다. 1) CC3M dataset을 595,000개의 image-text 쌍으로 filtering. 이때 instrction과 image, question을 입력으로 받고 원 데이터셋에서 caption에 해당하는 것을 예측하며 학습한다. 2) 학습 시에는 visual encoder와 LLM의 weight는 frozen시키고 trainable parameter인 $\\\\theta = W$(projection matrix)에 대해서만 likelihood를 maximize한다. 이와 같은 과정을 통해 학습으 하면 image feature $H_v$가 pre-trained LLM word embedding에 align될 수 있다. 해당 단계는 frozen LLM에 대해 visual tokenizer를 학습시키는 것으로 볼 수 있다. 3.2.2 Stage 2: Fine-tuning End-to-End Fine-tuing 과정에서는 LLM과 projection layer를 미세 조정하는 것이 주요 목적이다. Fine-tuning 시, visual encoder weight는 frozen 시키고 projection layer와 LLM의 weight만 업데이트한다. 이 때 두 가지 scenario를 위한 fine-tuning을 수행한다. 1) Multimodal Chatbot(Visual Chat) 앞서 구축한 158K(15만 8천개) language-image instruction-following data로 fine-tuning 2) Science QA 과학 관련 질의응답 데이터로, 이미지 혹은 자연어로 질문이 들어가면 자연어로 답변을 반환하는 데이터이다. 해당 데이터의 question & context을 instruction으로 LLaVA에 입력하고 reasoning & answer를 output으로 설정하여 학습한다. 4. Experiments Multimodal Chatbot과 ScienceQA dataset를 통해 추론 능력을 평가한다.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'hardware specification: A100 GPU * 8 hyperparameter 설정: Vicuna와 동일',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '[Pretraining] dataset: filtered CC-595K subset epoch: 1 learning rate: 2e-3 [Fine-tuning] dataset: LLaVA-Instruct-158K dataset epoch: 3 learning rate: 2e-5 batch size: 32 4.1 Multimodal Chatbot 4.1.2 Qualitative Evaluation (정성평가) Multimodal Chatbot 추론 능력 평가를 위해 chatbot demo를 만들고 아래 표와 같이 in-depth image understanding을 요구하는 예제들을(GPT-4 논문에 나온 예제들) LLaVA, GPT-4, BLIP-2, OpenFlamingo에 입력하여 비교했다. BLIP-2와 OpenFlamingo는 ser’s instructions에 따르기보다는 장면에 대해 간단하게 설명하는 반면 LLaVA는 user’s instructions에 따라 정확하게 답변하는 것으로 확인되었다. 그리고 GPT-4와 비교했을 때, 유사한 추론 결과를 보이면서 보다 포괄적인 답변을 반환하는 것으로 확인된다. LLaVA는 비교적 소규모인 80K unique image를 학습한 모델임에도 학습 이미지와 다른 도메인에 속하는 이미에 대해서도 잘 이해하고 사용자의 요구에 맞게 잘 답변하는 것으로 확인되었다. Visual Instruction Tuning',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '4.1.2 Quantitative Evaluation (정량평가) 1) GPT-4를 통해 생성된 결과를 참고 답안으로 삼고 2) LLaVA에도 동일한 질문에 대한 결과를 만든 후 3) 두 모델을 통해 생성된 결과를 GPT-4에 다시 넣어서 결과에 대한 helpfulness, relevance, accuracy 그리고 level of detail을 1에서 10 사이의 점수로 평가하게 한다. 그리고 평가 결과에 대한 설명도 제공하도록 한다. 본 논문에서는 위와 같은 과정에 사용될 두 개의 benchmark를 만들었다. LLaVA-Bench (COCO) 평가 데이터 생성 방법: COCO-Val-2014에서 30개의 이미지를 랜덤하게 선택 각 이미지에 대해 “2. GPT-assisted Visual Instruction Data Generation”에서 제안된 방식으로 세 가지 유형의 질문 (conversation, detailed description, complex reasoning)을 생성해서 총 90개의 질문을 만든다. 위 데이터로 Ablation study를 진행한 결과는 아래와 같다: Visual Instruction Tuning instruction tuning을 하지 않은 모델보다 tuning을 한 모델이 50% 이상 성능이 좋다. Conv + 5% Detail + 10% Complex를 학습한 모델이 conversation만 학습한 모델보다 7% 정도 성능이 좋다. Full data(세 유형의 데이터 전체)를 학습한 모델이 가장 높은 성능을 보인다. LLaVA-Bench (In-the-Wild)',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': '실내, 실외, 밈, 그림, 스케치 등을 포함한 24개의 이미지와 60개의 질문을 수집한다 각 이미지에는 highly-detailed, manually-curated description이 있다. Visual Instruction Tuning LLaVA, BLIP 그리고 OpenFlamingo에 대해 비교 실험을 진행한 결과, BLIP-2보다는 29%이상, OpenFlamingo보다는 48%이상 좋은 성능을 보였다. Visual Instruction Tuning 4.2 ScienceQA ScienceQA는 3 subjects, 26 topics, 127 categories, and 379 skills에 걸쳐 다양한 도메인을 포함한 21k(21,000개)의 multimodal multiple choice question을 포함한다. 해당 데이터셋은 trainset 12726개, validation set 4241개, 그리고 teset set 4241개로 나뉜다. GPT-3.5 model (text-davinci-002) with and without chainof-thought (CoT), LLaMA-Adapter, multimodal chain-of-thought (MM-CoT)로 실험한 결과는 아래 표와 같다. Visual Instruction Tuning LLaVa Model만 이용한 결과, SOTA(당시 91.68%)에 근접한 성능(90.92%)을 내는 것으로 확인되었다. LLaVa와 GPT-4를 함께 사용한 실험도 했다. (i) GPT-4 complement: GPT-4의 답이 틀렸을 때 LLaVA 담면 사용. 이때 90.97%의 성능 기록 (ii) GPT-4 as the judge: LLaVA와 GPT-4의 답변이 다른 경우, GPT-4에게 두 답변을 넣고 최종 결론을 요구하였고, 그 결과 SOTA(당시 92.53%)를 달성하였다. 4.2.1 Ablations ScienceQA에서 일부 선택사항을 제거하는 Ablation study도 진행했다.',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'Vision Encoder에서 Last Layer Feature를 사용하는 경우, Before the laset Layser Feature를 사용하는 경우보다 0.96%낮은 성능을 보였다고 한다. 이에 대한 이유로 추측되는 것은 마지막 Layer는 그 직전 Layer보다 일반적이고 추상적인 이미지 속성을 담고 있기 때문으로 추정된다. Answer를 먼저 예측하는 경우. 12Epoch에서 89.77%에 도달하였지만, Reasoning을 먼저 예측하는 경우에는 단 6 Epoch만으로도 89.77%에 도달하였다. 최종으로는 89.96%. 그래서 저자들은 Reasoning을 먼저 하는 것이 더 좋지 않을까 했지만 CoT 전략이 학습 속도 개선에는 좋지만 최종 성능에는 큰 영향을 미치지 않았다고 한다. Pre-train을 거치지 않은 경우, Pre training을 했을 때보다 5.11% 성능이 낮았다. 이를 통해 Pre training의 중요성이 강조되었다. 기존 13B 모델을 7B fh skwcns rufrhk 1.08% 성능이 감소하였다. 이에 따라 모델 크기에 따른 성능 차이가 존재함이 증명되었다. Visual Instruction Tuning Reference Visual Instruction Tuning https://llava-vl.github.io/ × Search',\n",
       "  'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/'},\n",
       " {'text': 'LLM이 70B라면 이는 모델이 조정 가능한 parameter를 700억개 가지고 있다는 것을 의미한다. parameter가 많을수록 모델 내부 신경망은 더 복잡해지며, 더 많은 데이터를 잘 학습할 가능성이 높아진다. 하지만 parameter가 많아 내부 신경망이 복잡할수록 계산량이 많아져 모델 학습 혹은 추론에 많은 비용이 요구된다. Trainable Parameters Trainable Parameters는 모델이 학습단계에서 값이 계산되고, 없데이트 되는 parameter이다. LoRA의 경우, 아래 코드 model에 LoRA를 적용한 모델을 넣으면 학습 파라미터가 나오겠지만 , 개념적으로 서명하자면 아래와 같다: 일반적으로 LoRA는 self attention layer weights matrix에 적용되는데 LoraConfig의 “target_modules”을 통해 LoRA를 적용하고자 하는 Matrix를 설정할 수 있다. 대상 matrix의 크기가 1024x512이고 LoRA rank를 8로 설정했을 때 Trainable Parameters는 1024×8 + 512×8로 계산될 수 있다. def print_trainable_parameters(model): \"\"\" Prints the number of trainable parameters in the model. \"\"\" trainable_params = 0 all_param = 0 for _, param in model.named_parameters(): all_param += param.numel() if param.requires_grad: trainable_params += param.numel() print( f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\" )',\n",
       "  'source_title': 'GPU, Paramter : Model Parameters and Memory Usage\\n      03 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/'},\n",
       " {'text': 'Estimate Memory Usage Model Parameter 1B를 기준으로 model parameter에 따라 대략 아래 표와 같은 memory가 요구된다. 그리고 학습 시에는 model parameter로 인해 요구되는memory 외에 학습 시 gradients, optimizer states, weight 계산 등 추가적으로 memory가 할당되어야 하는 요소들이 존재한다. Optimizer states를 위해 요구되는 memory 사용량은 optimizer 유형에 따라 다르다. AdamW optimizer: model parameter 1B당 요구되는 memory의 2배 SGD optimizer: model parameter 1B당 요구되는 memory와 동일 Gradients: model parameter 1B당 요구되는 memory와 동일 Inference Model parameter 1B float4GB BF162GB int81GB Estimate Memory Usage for Training (Full Training) Model parameter type: float optimizer state: AdamW, Train batch size=1 Illustrated by the author num_gpu=(model parameters in billions * 18 * 1.25)/GPU size in GB model parameters in billions: 일반적으로 모델 크기를 나타낼 때 사용되는 “B” 단위 (모델 총 parameter 수를 10억으로 나눈 값) 18: 학습에 사용되는 요소들을 고려한 대략적인 계수. optimizer state: 8 (AdamW. model parameter 1B당 요구되는 memory의 2배) gradients: 4 (model parameter 1B당 요구되는 memory와 동일) weights: 6 1.25: 입력된 데이터를 모델이 처리할 때 변하는 dynamic data structure에 25% 메모리를 추가한 값. (입력데이터의 크기가 크면 이 값이 커 질 수도 있을 것 같다.)',\n",
       "  'source_title': 'GPU, Paramter : Model Parameters and Memory Usage\\n      03 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/'},\n",
       " {'text': 'import math def calculate_gpus(model_params_in_B, gpu_size): gpus_required = (model_params_in_B * 18 * 1.25) / gpu_size return math.ceil(gpus_required) 예를 들어 A100 80GB로 LLaMa3 7B를 학습할 때 위 공식에 따르면, (7 * 18 * 1.25)/80 약 2개의 GPU가 필요하다. Model parameter type: BF16 optimizer state: AdamW, Train batch size=1 Illustrated by the author num_gpu=(model parameters in billions * 12 * 1.25)/GPU size in GB model parameters in billions: 일반적으로 모델 크기를 나타낼 때 사용되는 “B” 단위 (모델 총 parameter 수를 10억으로 나눈 값) 18: 학습에 사용되는 요소들을 고려한 대략적인 계수. optimizer state: 4 (AdamW. model parameter 1B당 요구되는 memory의 2배) gradients: 2 (model parameter 1B당 요구되는 memory와 동일) weights: 6 1.25: 입력된 데이터를 모델이 처리할 때 변하는 dynamic data structure에 25% 메모리를 추가한 값. (입력데이터의 크기가 크면 이 값이 커 질 수도 있을 것 같다.) import math def calculate_gpus(model_params_in_B, gpu_size): gpus_required = (model_params_in_B * 12 * 1.25) / gpu_size return math.ceil(gpus_required) 예를 들어 A100 80GB로 LLaMa3 7B를 학습할 때 위 공식에 따르면, (7 * 18 * 1.25)/80 약 2개의 GPU가 필요하다. Estimate Memory Usage for Training (LoRA )',\n",
       "  'source_title': 'GPU, Paramter : Model Parameters and Memory Usage\\n      03 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/'},\n",
       " {'text': 'LoRA나 quantization이 적용된 경우, Trainable Parameter를 계산하여 적용. LoRA는 original model의 parameter가 frozen된 상태로 추론처럼 사용되기 때문에 해당 모델이 추론에 소요되는 GPU수 + LoRA를 적용한 Trainable Parameter에 대한 필요 GPU 수로 계산하면 될 것으로 보임. import math def calculate_gpus_lora(model_params_in_B, lora_trainable_params_in_B, gpu_size): gpus_required_frozen_metrix = (model_params_in_B * 2 * 1.25) / gpu_size gpus_required_lora_metrix = (lora_trainable_params_in_B * 12 * 1.25) / gpu_size gpus_required= gpus_required_frozen_metrix+gpus_required_lora_metrix return math.ceil(gpus_required) Estimate Memory Usage for Inference 일반적으로 모델 parameter를 BF16로 불러오기 때문에 GPU 요구량은 아래와 같이 계산될 수 있다 Illustrated by the author num_gpu=(model parameters in billions * 2 * 1.25)/GPU size in GB import math def calculate_gpus_inference(model_params_in_B, gpu_size): gpus_required = (model_params_in_B * 2 * 1.25) / gpu_size return math.ceil(gpus_required) Reference https://medium.com/@plthiyagu/calculate-gpu-requirements-for-your-llm-training-7122a3700547 https://medium.com/illuminations-mirror/how-many-gpus-do-you-really-need-for-model-training-ce0cacf3ce9b https://medium.com/@plthiyagu/llm-model-parameter-memory-required-for-training-and-inference-634963b36b59 × Search',\n",
       "  'source_title': 'GPU, Paramter : Model Parameters and Memory Usage\\n      03 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/03/num_gpu/'},\n",
       " {'text': 'LLaMa 3 LLaMA 3는 Meta에서 공개한 Open Source LLM model로, LLaMa2에 이어 공개된 모델이다. LLaMA 3는 Instruct Model과 Pre-trained Model에 대해 각각 8B, 70B 두 사이즈가 공개되었다. Pretraining과 Post-training 방법의 개선으로 공개된 8B, 70B의 Pretrained, Instruction-fine-tuned model이 2024 4월18일 기준 해당 parameter scale 모델 중 가장 좋은 성능을 보인다고 한다. Post-training과정에서는 false refusal rate를 줄이고, model의 alignment를 개선하고, model response의 다양성을 증가시켰다. 특히 LLaMa 3는 2보다 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있을 것으로 보인다. LLaMa 3 개발 시 Meta팀은 benchmark 성능 향상 뿐만 아니라 실제 추론 능력(real-world scenario) 최적화에도 집중하였다고 한다. 실제 추론 능력 검증을 위해 새로운 high-quality human evaluation set을 개발했다고 한다. 해당 데이터는 12가지 use case에 대한 총 1,800개 prompt로 구성되어 있다. (12가지 use case: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, summarization). 아래 표는 해당 evaluation set에 대한 Claude Sonnet, Mistral Medium, GPT-3.5의 추론 결과를 비교한 것이다. https://ai.meta.com/blog/meta-llama-3/ LLaMA 3 개발팀은 모델 개발 시 Model architecture, Training data, Scaling up pretraining, Instruction fine-tuning을 중점적으로 개선했다고 한다. Model architecture',\n",
       "  'source_title': 'LLaMa 3\\n      01 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/'},\n",
       " {'text': 'Base architecture: LLaMa 3는 standard decoder-only transformer architecture를 기반으로 개발되었다. Tokenizer size: 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding하여 모델 성능을 향상시킴. -> LLaMa 2보다 4배 큰 tokenizer size Attention method: 8B, 70B 모두 GQA를 사용하여 모델 추론 효율성을 높였다. -> LLaMa 2는 70에만 GQA 적용 context window: 8,192 token -> LLaMa 2는 4,096였다. Tokenizer size가 커지면 encoding을 효과적/효율적으로 할 수 있는 이유: tokenzer size가 4배 커지면 동일한 데이터가 약 4배정도 압축되어 모델에 입력될 수 있다. 예를 들어 입력한 데이터가 LLaMa 2의 tokenizer로 200 token으로 encoding 될 때, LLaMa 3 tokenizer로는 100개가 되지 않는 token 수로 encoding될 수 있기 때문이다. 그리고 tokenizer size가 커진 만큼 모델이 많이 학습하지 않은 한국어 데이터에 대한 tuning을 수행할 때 vocab에 따로 한국어 token을 추가하지 않아도 된다. Training data size LLaMa 3는 15T개의 token으로 학습되었다. (LLaMa 2는 2T로, 2에 비해 3는 약 7배 더 큰 학습데이터를 사용하였다.) 수집된 데이터는 모두 publicly available source에서 수집되었다. code data: LLaMa 2에 사용된 code data 수보다 4배 더 많음 multilingual use case를 위해 전체 training data의 5% 이상을 high-quality non-English(30개 이상) data로 구성',\n",
       "  'source_title': 'LLaMa 3\\n      01 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/'},\n",
       " {'text': 'data filtering data의 품질을 향상시키기 위해 data-filtering pipeline을 개발했다. heuristic filters NSFW filters(음란 데이터 filtering) semantic deduplication approaches(의미 중복 제거) text classifier filtering model은 LLaMa 2를 기반으로 한다. Scaling up pretraining scaling law 새로운 scaling law를 발견했다고 한다. Chinchilla-optimal은 8B model 학습 시 가장 최적의 학습 token양은 ~200B token이라고 했지만 200B보다 많은 token을 학습해도 model의 성능은 계속 증가한다. Llama3 8B, 70B를 15T token data로 학습시켰을 때 수렴점에 근접하지 않으면서 log-linearly하게 모델 성능이 계속 향상되는 것을 확인했다고 한다. parallelization 70B 모델 학습을 위해 data parallelization, model parallelization, pipeline parallelization을 결합했다. Instruction fine-tuning LLaMa 3의 post-training은 supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), direct preference optimization (DPO)를 결합한 방식으로 수행되었다. post-training에서는 SFT에 사용되는 prompt와 PPO, DPO에 사용되는 preference ranking이 중요하다. Reference https://ai.meta.com/blog/meta-llama-3/ × Search',\n",
       "  'source_title': 'LLaMa 3\\n      01 May 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/'},\n",
       " {'text': 'SFT는 학습 데이터와 일치하는 텍스트 생성 가능성 극대화에 집중하여 인간의 선호도에 반하는 답변을 반환할 확률이 비교적 높다는 한계점이 있다. 이와 같은 한계를 완화하고 Fine-tuning을 더욱 효과적으로 수행하기 위해 Reinforcement Learning(강화학습) 아이디어를 SFT에 적용한 방법론들이 있다. 강화학습은 Agent가 주어진 환경에서 어떠한 행동을 취하고 그에 대한 보상을 얻으며 학습이 진행되는 방법론이다. 이를 SFT에 적용하여 LLM은 Agent, 환경은 LLM의 vocabulary에서 가능한 모든 token 조합, action space는 모델의 vocabulary, Agent의 행동은 token-prediction으로 설정한 후 학습을 진행할 수 있다. Reinforcement Learning(RL)',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': '주요 구성요소 1) Agent: 학습을 수행하는 주체. 주어진 환경 내에서 행동을 선택하고 그 결과로 보상을 받는 주체. 2) Environment: Agent가 상호작용하는 세계. Agent의 행동에 대한 보상(feedback)을 제공한다. 3) State: Environment의 상태 4) Action(Action Space): Agent가 취할 수 있는 선택지 5) Reward: Agent가 특정 행동을 취했을 때 환경으로부터 받는 feedback. 이것을 기준으로 학습이 진행된다. 6) Policy: Agent가 특정 State에서 어떤 행동을 취할지 결정하는 전략 7) Value Function: 특정 State에서 시작하여 장기적으로 얻을 것으로 기대되는 보상들의 누적 총합. 8) Q-Function: 특정 State에서 특정 행동을 취했을 때 장기적으로 얻을 것으로 기대되는 보상들의 누적 총합. 학습과정(아래 과정을 반복하며 최적의 Policy를 얻는다.) 1) 초기화: Agent를 초기화 시킨 후 Agent의 초기 Policy를 가지고 Environment와 상호작용한다. 2) Action 선택: 현재 State를 기반으로 Policy에 따라 Action을 선택한다. 3) Environment와의 상호작용: Agent가 선택한 행동을 실행한 후 그 결과로 새로운 State와 Reward를 받는다.',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': '4) Policy update: Agent는 받은 State와 Reward를 기반으로 더 나은 보상을 얻기 위한 Policy를 개선한다.',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': '1. RLHF/PPO GPT-3.5와 같이 LLM 유행 초기에 많이 사용된 방법이다. 이는 크게 두 단계로 작동된다. Step 1. Reward Model 구축 prompt dataset(instruction, context, output pair)으로 모델을 fine-tuning 시킨다. 이때 두 개 이상의 추론 결과를 반환하도록 한다. human labeler에게 추론 결과에 대한 점수/순위를 매기게 한다.(“Anthropic/hh-rlhf”와 같이 honesty와 harmlessness를 검증하는 open-sourced preference ranking dataset도 있다.) 매겨진 점수의 normalization 과정을 거친 후 이것을 single sample-reward pair로 만들어 reward model을 학습시킨다.',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'Step 2. SFT Reward Model을 진짜로 구축하고자 하는 모델 학습에 사용한다. Tuning할 모델에 prompt dataset을을 넣어 모델의 추론 결과를 생성하고 이걸 Reward Model에 입력해서 Reward를 도출시킨다. Proximal Policy Optimization (PPO)라는 policy기반의 RL algorithm을 통해 모델의 가중치를 점진적으로 조정하며 모델의 응답에 할당된 reward를 최대화시킨다. (경사 하강법(gradient descent)을 사용하는 일반적인 학습 방식과 달리 경사 상승(gradient ascent)법을 사용한다. (reward에 대해서는 경사 하강.)) Reinforcement Learning 기반 접근 방식으로 인해 모델의 행동이 과도하게 변형되는 것을 방지하기 위해 prediction shift penalty를 보상에 추가하여 동일한 입력 prompt에 대한 초기 모델의 예측 확률 분포에서 너무 벗어나는 답변에 대해서는 penalty를 부여한다. 2. Direct Policy Optimization (DPO) DPO는 RLHF의 대안으로 제안된 방법론이다. DPO는 RLHF와 동일하게 모델을 인간의 선호에 맞게 조정하는 것을 목적으로 한다. 그러나 DPO는 Reward Model이 필요하지 않기 때문에 RLHF보다 비용 측면에서 효율적이다. RLHF에는 아래와 같은 단점이 있다:',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'Reward Model을 위한 학습에 자원이 필요하고, 이 자원도 Reward Model의 크기에 따라 매우 많은 자원이 요구될 수 있다. initial LM, tuned LM, reward model이 한번에 돌아가야 하기 때문에 최소 세 개의 모델을 구동시킬 대규모 컴퓨터 자원이 필요하다. 위와 같은 단점을 완화하기 위해 나온 것이 DPO이다. DPO의 핵심 아이디어는 Reward Model 훈련을 건너뛰고 LLM을 직접적으로 preference data에 맞추는 것이다. Reward Model의 역할은 손실함수를 조작하여 대체한다. 자세한 수식은 “Direct Preference Optimization: Your Language Model is Secretly a Reward Model”에서 확인 가능하다. Illustrated by the author 품질 좋은 데이터를 선별하여 데이터의 양은 줄이되 학습되었으면 하는 데이터 종류의 분포를 조절하는 것이 좋다고 한다. 2.1 Preference Dataset DPO data는 pormpt(instruction), preferred response 그리고 dispreferred response가 기본적으로 포함되어야 한다. DPO data의 형식은 아래와 같다: {\"instruction\":instruction, \"context\": context, \"response\":response, \"category\": category, \"rejected\", rejected} 2.2 DPO Fine-tuning',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'DPO에서는 따로 강화학습을 진행하지 않는다. DPO를 활용한 fine-tuning 과정에서는 우선 모델의 사본이 생성된다. 생성된 사본의 parameter는 frozen 시킨다. 각 datapoint에 대해 preferred response, rejected response가 두 모델에 모두 입력되어 각각 점수가 매겨진다. preferred response score의 확률이 rejected response score보다 높은 경우 모델은 보상을 받는다. 학습이 이루어지는 모델의 점수가 frozen 모델의 점수와 가까울 수록 모델은 보상을 받는다. 이는 DPO 학습 중 pre-trained model이 지닌 지식에서 벗어나 fine-tuning data에 과적합되는 것을 방지하기 위한 장치이다. Illustrated by the author 2.2.1 DPO loss equation 요약 \\\\begin{matrix} Loss= Winner(W)-Loser(L) \\\\end{matrix} \\\\begin{matrix} \\\\downarrow \\\\text{minimizing} \\\\downarrow \\\\end{matrix} \\\\begin{matrix} Loss = - (Winner(W)-Loser(L)) \\\\end{matrix} $\\\\pi_\\\\theta$ : parameter update가 이루어지는 model, 즉 학습되고 있는 model $\\\\pi_{ref}$ : forzen model $y_w$ : preferred response score $y_l$ : rejected response score',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': '$\\\\pi_\\\\theta$와 $\\\\pi_{ref}$에 대한 preferred response, rejected response에 대한 각 token별 예측 확률(정답 token을 예측할 확률)의 곱을 각각 구하여 $\\\\pi_\\\\theta$와 $\\\\pi_{ref}$ 간의 비율을 구한다. 이때 prompt에 해당하는 token들은 점수에 반영되지 않는다. 산출된 비율은 아래 수식에서 확인할 수 있듯 loss 산출에 사용되고, 이 loss는 gradient descent update과정에서 model weight를 update하는데에 사용된다. 수식에서 $\\\\beta$는 hyperparameter로, 논문 저자는 0.1로 지정하였고, $\\\\sigma$ sigmoid(logistic) 함수이다. Cross-entropy loss function used in DPO pipeline(https://medium.com/@pakhapoomsarapat/forget-rlhf-because-dpo-is-what-you-actually-need-f10ce82c9b95) DPO 논문에서는 classification loss를 활용하여 Reinforcement Learning(RL)를 대체하는 방법을 제안한다. loss는 모델이 어떠한 데이터에 대하여 잘 학습되고 있는지 측정하는 지표로, 모델은 loss를 최소화 하는 방향으로 학습을 진행한다. DPO data에는 preferred response, rejected response가 포함되어 있는데, 이 두 답변에 대한 점수를 구한 후 3. Odds Ration Preference Optimization (ORPO) ORPO: Monolithic Preference Optimization without Reference Model',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'Instruction tuning(SFT)과 preference alignment는 최근 LLM tuning 시 많이 사용되는 방법이다. 이는 일반적으로 아래와 같이 두 단계를 통해 수행된다: 1 단계. Supervised Fine-Tuning (SFT) : 일단 instruction dataset에 대한 tuning을 수행한다. 2 단계. preference alignment : RLHF 혹은 DPO를 통해 rejected response보다 preferred response를 반환할 확률을 높인다. SFT가 학습 데이터와 일치하는 텍스트 생성 가능성 극대화에 집중하여 학습이 진행되기 때문에 특정 domain 혹은 task에 대해서는 효과적으로 tuning되지만 인간이 선호하지 않는 답변 반환률이 비교적 높다는 문제점이 있다. 이에 따라 preference alignment단계에서는 선호되는 답변과 그렇지 않는 답변 사이의 확률 격차를 확실히 넓혀야 하는 것이 핵심 과제가 된다.',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'ORPO는 SFT와 preference alignment algorithm를 single process로 결합하였다. ORPO가 두 단계를 결합할 수 있던 요인에는 목표 함수 수정에 있다. ORPO는 CLM의 목표 함수를 수정하여 negative log-likelihood(NLL) loss와 odds ratio (OR)를 결합하였다. 이것을 OR loss라고 부르는데 이는 rejected response에 대해서는 약한 패널티를 주고, preferred response에 대해서는 강한 보상을 주어 모델이 학습 과정에서 자연스럽게 인간의 선호도도 학습할 수 있도록 한다. 3.1 Preference Pairs ORPO 학습 시, data에는 Preference Pairs가 포함되어야 한다. Preference Pairs는 주어진 입력에 대한 선호 및 비선호 예시가 구체적으로 작성된 요소이다.',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'Preference Pairs example input: 쭈꾸미 볶음 레시피 알려줘. preferred output: 쭈꾸미 볶음은 매콤하고 달콤한 양념이 매력적인 한국 인기 요리입니다. 재료는 쭈꾸미 500g을 기준으로 양파 1개, 대파 1개, ...(중략)..., 양념장 재료로는 고추장 3큰술, 설탕 2큰술, ...(중략). + 준비 및 요리 과정 상세 설명 + 요리 팁 맛있는 쭈꾸미 볶음 즐기시기 바랍니다! dispreferred output: 쭈꾸미를 잘 씻고 고추장 등 적당한 양념들을 배합하여 볶아줍니다.',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': '3.2 Odds Ratio Calculation ORPO는 주어진 입력에 대해 선호 출력과 비선호 출력을 생성할 확률 간의 odds ratio를 계산한다. odds ratio는 모델의 출력에서 선호 / 비선호 답변 반환률을 정량화한다. 아래 식에서 $\\\\mathcal{L}OR$는 preferred response와 dispreferred response 간의 odds ratio이다. odds ratio는 한 사건이 다른 사건의 존재 하에 발생할 확률을 나타낸다. 즉, LLM에 대한 입력 시퀀스 $x$가 주어졌을 때 모델이 $yw$ preferred response 혹은 $yl$ dispreferred response을 선택할 확률을 나타낸다. 아래 식을 보면 $yw$에 대한 확률이 높을수록 odds ratio가 높아진다. 따라서 odds ratio가 높으면 선호 답변 출력 가능성이 높은 모델이고, odds ratio가 낮으면 비선호 답변 출력 가능성이 높은 것이다. ORPO: Monolithic Preference Optimization without Reference Model 3.3 Model Update 모델은 계산된 승산비를 기반으로 weight 업데이트를 진행한다. 모델이 비선호 출력을 많이 반환하면 선호 출력을 생성하도록 조정된다. 이와 같은 방식으로 ORPO는 모델의 의사 결정 과정을 지속적으로 통제 및 조정하여 preference pairs를 통해 입력 받은 인간의 선호도에 맞는 텍스트를 생성하도록 유도된다. 3.4 Benefits of ORPO',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'Efficiency SFT와 preference alignment 개념을 간단하게 결합하여 SFT 수행 이후 별도의 preference alignment 과정을 거칠 필요가 없어 학습 시간과 계산 자원이 줄어들었다. Improved Alignment preference pairs를 통해 fine-tuning을 진행하면서 인간의 선호도를 학습하기 때문에 반환된 답변과 선호도 간의 align이 더 잘 된다. Reduced Bias preference pairs는 데이터 내에 존재하는 bias를 완화하는데 도움을 준다. preference pairs 구축에는 인간이 개입되기 때문에 모델이 편향된 텍스트를 생성하기보다는 더욱 객관적이고 윤리적인 답변을 반환하는 것에 도움을 줄 수 있다. Flexibility ORPO는 preference pairs를 생성할 수 있다는 조건만 충족된다면 매우 다양한 도메인에 간단히 적용될 수 있다. ORPO의 objective function은 SFT loss와 relative ratio loss (LOR)로 구성된다. LOR항은 favored response와 disfavored response간의 likelihood를 최대화함으로써 rejected response를 반환한 모델에 패널티를 적용한다. Reference Training language models to follow instructions with human feedback Direct Preference Optimization: Your Language Model is Secretly a Reward Model ORPO: Monolithic Preference Optimization without Reference Model × Search',\n",
       "  'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/'},\n",
       " {'text': 'Transformer 계열의 모델은 고정된 입력 길이를 input으로 받는다. 모델의 context window가 커도 문서 전체를 입력으로 넣는 것보다 질문과 의미 유사도가 높은 몇몇 chunk만 입력으로 넣는 것이 정확한 답변을 생성하는 데에 도움이 된다. Chunking 단계에서는 각 chunk들이 의미를 잃지 않는 선에서 일정한 크기로 분절되는 것이 중요하다. 예를 들어 하나의 문장이 chunk size에 걸려 두 부분으로 나뉘어서는 안 된다. 문장이나 단락 단위로 chunk를 나누는 것이 좋다. chunk size는 사용중인 embedding 모델에 따라 결정하는 것이 좋다. Sentence-Transformer의 경우 max token size가 512이고, OpenAI ada-002는 8191이다. 분절된 chunk를 vector로 encode(indexing) embedding model 선정도 매우 중요하다. MTEB Leaderboard를 통해 적절한 Embedding Model을 선정할 수 있다. vector DB에 적재 2) Retrieval 입력된 질문과 의미 유사도가 높은 Top k의 chunk를 찾는다. Indexing 단계에서 사용된 embedding model과 동일한 모델을 사용하여 사용자 질문을 encoding해야 한다. 3) Generation',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '입력된 질문과 retrieved chunk를 LLM에 함께 입력하여 답변을 생성한다. 아래와 같이 user_message(사용자 질문)과 context(retrieval 결과)를 함께 LLM에 입력하며, context를 참고하여 user_message에 대한 답변을 생성하라는 prompt를 포함시켜 LLM에 입력한다. prompt=f\"\"\" <|im_start|>system You are a helpful assistant chatbot. Write a response that appropriately completes the request, referring to given Context. When generating responses, it is crucial to adhere to the following conditions. - Do not generate new question. You must respond only to the given question. - You should never repeat the same sentence. - Do not repeat the questions in your response. - You must answer in Korean Language. Do not use any other languages except Korean. Here is context to help: context: {context}<|im_end|> <|im_start|>user {user_message}<|im_end|> <|im_start|>assistant \"\"\" 2. Naive RAG Naive RAG(standard RAG)은 가장 기본적인 RAG이다. Illustrated by the author',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '2.1 Naive RAG의 문제점 Naive RAG의 각 단계마다 문제점들이 존재한다. 1) Query 사용자의 질문이 명확하지 않은 경우, similarity search 과정에서 오류가 발생할 수 있다. 2) Indexing 2.1) Parsing: PDF와 같은 비정형 문서 내 이미지 및 표에 담긴 유용한 정보에 대한 추출이 불완전하다. 2.2) Chunking: 파일 특성을 고려하지 않고 일률적인 크기로 chunking함으로써 의미상 포함되어야 할 정보가 고정된 size 문제로 잘려 각 chunk에 불완전한 정보가 담겨있을 가능성이 있다. 2.3) Indexing: vector db의 indexing 구조가 파일 유형마다 최적화되어 있지 않아 retreival 과정에 부정적인 영향을 미칠 수 있다. 2.4) Embedding Model: 임베딩 모델의 semantic representation 성능이 좋지 않을 경우 retreival 과정에 부정적인 영향을 미칠 수 있다. 3) Retrieval 3.1) 검색된 chunk들과 질문의 관련성이 낮을 가능성이 있다. 3.2) 여러 검색 알고리즘을 종합적으로 사용할 수 없어 검색 기능이 제한적이다. 3.3) 검색된 chunk들에 유사한 정보가 중첩되는 경우, LLM이 생성 시 참고할 정보가 제한적이다. 4) Generation',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '1) 생성모델 특성 상 동일한 질문에 대해 일관적이지 않은 답변이 생성될 수 있다. 2) 생성 모델의 성능에 따라 새로운 답변을 생성하지 않고 검색된 chunk에 과도하게 의존하는 경우도 있다. 이 경우 chunk를 그대로 반환하기도 한다. 3) LLM의 문제점을 완화하기 위해 RAG을 적용했지만 여전히 부정확하거나 관련 없는 답변을 생성할 가능성이 있다. 3. Advanced RAG Advanced RAG은 Naive RAG의 문제점을 완화하기 위해 Indexing 과정 혹은 Retrieval 앞뒤에 검색 정확도를 높이기 위한 과정이 추가된 RAG이다. 아래 그림에서 파란색 부분이 이에 해당한다.',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Illustrated by the author 3.1 Enhancing Parsing Techniques Illustrated by the author RAG은 문서를 검색한 후 검색된 문서를 기반으로 답변을 반환하는 구조로 작동되기 때문에 비정형 데이터로부터 정보를 정확히 추출하는 것이 중요하다. RAG을 위한 비정형 데이터의 종류는 매우 다양하지만 그 중 PDF parsing이 매우 까다롭다. PDF parsing 방법은 크게 세 가지이다. 1) Rule-base parsing pypdf, pdfplumber와 같은 규칙 기반 parser를 사용하는 방법이다. 이와 같은 라이브러리는 정해진 규칙에 따라 pdf에서 텍스트를 추출하기 때문에 다양한 형식의 PDF를 다루기에는 범용성이 떨어지는 문제가 있다. 2) deep learning model base parsing object detection과 OCR 기술을 결합한 deep learning 모델을 기반으로 PDF에서 정보를 추출하는 방법이다. object detection을 통해 문서의 레이아웃을 식별하거나 표 내부 구조를 파악하여 정보를 보다 정확하게 추출할 수 있다. 하지만 모델을 사용하는 만큼 pasing에 소요되는 시간이 rule-base보다 길다는 것이 단점이다. 3) multimodal large model base parsing multimodal LLM을 사용하여 PDF를 parsing하는 방법이다. 3.2 Pre-Retrieval(Query transformations) Illustrated by the author',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Pre-Retrieval 단계에서는 Retrieval 시 질문과 관련있는 chunk를 더 잘 검색하기 위해 Query를 조작한다. 1) Query reformulation 사용자의 의도와 더 가깝게 질문을 재작성하는 방법. 2) Query expansion 동의어나 관련 표현들을 통해 질문을 확장함으로써 관련있는 chunk 검색을 돕는 방법 3) Query Routing Query를 LLM에 효과적으로 입력하기 위해 Query부터 LLM 입력까지의 경로를 지정하는 것이다. 예를 들어 인덱스에 계층이 있는 경우, 각 chunk에 대해 요약 등의 metadata가 있는 경우 등 query가 LLM까지 도달하기까지 다양한 경로가 존재할 때 Query router를 통해 최적의 옵션을 타고 LLM에 도달하게 된다. 5) Query normalization 철자 수정, 용어 차이 완화, 특수문자 및 공백 제거 불용어 제거 등 query 정규화 방법 6) Multi/Sub query transformation 복잡한 질문을 LLM을 통해 단순한 여러 질문으로 수정한 후 Multi Query Retrieval을 수행하는 방법. 예를 들어 “John과 Mike 중 이번 중간 평가 평균 점수를 알려줘” -> sub query 1) John의 중간 평가 평균 점수를 알려줘. sub query 2) Mike의 중간 평가 평균 점수를 알려줘.',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '하지만 이 방법은 두 sub query에 대해 검색된 chunk가 상이하여 서로 다른 문서로부터 답을 찾을 가능성이 있다는 문제가 있다. 7) Rewrite-Retrieve-Read Query Rewriting for Retrieval-Augmented Large Language Models 논문에서 제안된 방법으로, 사용자의 질문이 LLM 검색에 최적화되지 않은 경우가 많아 LLM을 통해 질문을 재 작성하는 방법을 소개한다. 8) Step-Back Prompting prompting을 통해 LLM이 질문의 핵심을 파악하도록하는 것이다. 질문이 너무 구체적인 경우, LLM은 특정 세부 사항에 꽂혀서 제대로된 답을 하지 못한다. 예를 들어 “2023년 4월부터 2024 5월까지 나온 LLM 관련 논문은 몇 개야?”라는 질문은 너무 세세하게 명시된 기간으로 인해 LLM이 적절한 답을 반환하지 못할 가능성이 높다. 이 예시의 경우, “LLM 관련 논문 수를 연도별로 알려줘”라는 query로 묻는 것이 더 효과적이다. Step-Back Prompting은 두 단계를 통해 진행된다. 1. 추상화 : LLM이 query에 대한 high-level concept 혹은 기본 개념에 관한 광범위한 질문을 만들도록 prompting한다. 2. 추론 : 추상화된 질문에 대한 답을 추론한다. 이를 abstract reasoning이라고 부른다.',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '3.3 Enhanced indexing strategy',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Illustrated by the author',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '1) Hierarchical index retrieval 검색 데이터(문서)가 많은 경우, 검색 효율을 높이기 위해 문서 요약 index와 문서 chunk index 단계를 만들어 우선 요약을 통해 관련 문서를 filtering한 후 해당하는 문서 chunk 중 질문과 의미 유사도가 높은 chunk들을 뽑는 방법이다. 2) Hypothetical Questions LLM을 통해 각 chunk들에 대한 질문을 생성하여 이와 같은 가상 질문을 embedding하여 chunk와 함께 vector DB에 저장하는 것이다. 이와 같은 방법은 질문과 가상 질문 간의 의미적 유사도가 질문과 chunk 사이의 유사도보다 높기 때문에 검색 품질을 향상시킬 수 있다. 3) HyDE(Hypothetical Document Embeddings) HyDE는 Hypothetical Questions와 반대의 접근 방식이다. LLM에 사용자의 질문을 입력하여 가상 응답을 생성하도록 한 후 생성된 가상 응답과 chunk 사이의 유사도를 통해 검색을 수행하는 방법이다. (비추다.) 4) Context enrichment Context enrichment는 검색된 chunk에 대해 추가적으로 context를 강화하는 방법이. Context enrichment에는 크게 두 가지 방식이 있다. 4.1) Sentence Window Retrieval',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'chunk를 문장 단위로 만들어 embedding한 후 query와 문장 사이의 유사도를 검색한 후 해당 문장의 전 후 k개 문장으로 context window를 확장하여 LLM에 입력하는 방법. 4.2) Auto-merging Retriever (Parent Document Retriever) 우선 작은 size의 chunk를 검색한 후 검색된 상위 k개의 chunk 중 n개 이상의 조각이 동일한 상위 node(larger chunk)와 연결된 경우 해당 node를 LLM이 참고할 context로 입력한다. 3.4 Optimizing Embedding Model',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Illustrated by the author retrieval 수행 시 가장 관련성 높은 chunk를 추출하는 것이 중요하다. 일반적으로 retrieval은 query와 indexing된 chunk들 간의 벡터 검색을 통해 산출된 의미 유사도를 기반으로 수행된다. 이에 따라 embedding model이 문장 의미 정보를 잘 표현하는 것이 중요하다. Embedding model을 강화하는 방법으로 아래 두 가지가 잘 알려져 있다. 1) Fine-tuning embedding model Embedding model을 domain-specific context에 맞게 fine-tuning하는 방법이다. 2) Dynamic Embedding 각 단어에 하나의 벡터만 사용하는 static embedding(정적 임베딩)과 달리 model이 context에 맞게 vector를 embedding하는 방법으로, OpenAI의 embeddings-ada-02가 대표적인 동적 임베딩 모델이다. 3.5 Post-Retrieval Illustrated by the author',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Post-Retrieval 단계에서는 검색된 문서들을 정제하여 답변 생성 품질을 향상시키기 위한 처리들을 한다. 1) Fusion retrieval or hybrid search Fusion retrieval 혹은 hybrid search는 Query와 chunk에 대한 의미 유사도, 키워드 매칭을 모두 고려하기 위해 tf-idf 혹은 BM25와 같은 keyword-based old school search와 최근에 많이 사용되는 semantic/vectore search 를 결합하는 방법이다. 이 방법론에서 중요한 것은 두 검색 결과를 적절히 결합하는 것이다. 검색 결과 결합에는 일반적으로 Reciprocal Rank Fusion algorithm이 사용된다. 2) Re-Ranking 검색 결과에 대해 유사도 점수 등을 기준으로 chunk를 정렬하여 query와 관련성 높은 문서를 강조하기 위한 처리이다. Re-Ranking model은 query와 context를 input으로 받아 유사도를 측정한다. 3) Prompt compression 관련 없는 context는 제거하고 중요한 context는 강조하여 전체 prompt 길이를 줄인다. 이 과정을 통해 LLM을 중요한 정보만 입력 받아 정확한 답변을 생성할 가능성이 높아지고, LLM의 입력이 줄어들어 추론 속도가 감소할 수 있다. 4) Filtering',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '키워드나 metadata를 기반으로 검색된 chunk를 filtering하는 단계로, 관련 없는 문서가 검색되었을 때 해당 문서를 제거하기 위한 과정이다. 4. Self-RAG 일반적인 RAG은 입력된 모든 query에 대해 vector DB에 저장된 전체 문서와 유사도를 구하여 관련 chunk를 찾아 답을 생성하는 방식으로 작동된다. Self-RAG의 경우, 아래 그림과 같이 익숙한 주제에 대해서는 LLM이 바로 답변하도록 하고, 그렇지 않은 주제에 대해서는 RAG을 수행하는 방식으로 작동된다.',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Pinecone(https://www.pinecone.io/learn/advanced-rag-techniques/) reward 모델을 align하는 것처럼 reflection token을 예측하는 model을 학습시켜 익숙한 주제인지 아닌지 분류한다. 해당 모델의 데이터는 특정 도메인에 대해 fine-tuning된 모델이든 아니면 그냥 사전학습모델이든 frozen LLM 모델이 반환하는 답변과 질문을 openai gpt-4에 입력하여 답변이 질문과 관련있는지, 어느정도 관련이 있는지, 얼마나 supported한지, retreival이 필요해 보이는지 등의 label을 수집한다. 자세한 데이터 수집 방법은 [https://github.com/AkariAsai/self-rag/tree/main/data_creation/critic/gpt4_reward(]https://github.com/AkariAsai/self-rag/tree/main/data_creation/critic/gpt4_reward) 여기에서 확인 가능하다. 위 과정을 통해 수집된 데이터로 reflection token 예측 모델을 학습시켜 RAG에 사용한다. 5. Corrective RAG (CRAG) CRAG은 일반적인 RAG과 동일하게 입력된 모든 query에 대해 vector DB에 저장된 전체 문서와 유사도를 구하여 관련 chunk를 찾는다. chunk를 찾은 이후 답을 생성하기 전, 검색된 정보가 올바른지, 아닌지 혹은 애매한지 분류하고 분류 유형에 따라 답 생성 방식을 달리한다. Correct, Incorrect, Ambiguout 분류 모델은 T5-large를 학습시킨 모델을 사용한다. Pinecone(https://www.pinecone.io/learn/advanced-rag-techniques/)',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '각 분류 결과에 따른 답 생성 방식은 다음과 같다: 1) Correct: 검색된 문서에 query가 요구하는 정보가 포함되어 있기 때문에 해당 문서에 대한 filtering 작업만 거치고 RAG을 수행한다. filtering은 문서를 보다 작은 단위로 자르고 다시 Retrieval evaluate model(T5-large based)로 분류 한 후 0.5보다 낮은 부분은 제거, 나머지는 사용하는 방식으로 수행된다. 2) Incorrect: 검색된 문서에 query가 요구하는 정보가 없기 때문에 Web 검ㅅ개을 통해 관련 문서를 가져온다. 3) Ambiguout: 검색된 문서에 query가 요구하는 정보가 충분하지 않은 경우로, “Incorrect”의 경우와 마찬가지로 추가 정보도 찾고 “Correct”에서 수행한 filtering 작업을 거친 후 RAG을 수행한다. 6. Handling Chat history Chat Engine에서 이전 대화 내용을 반영하여 답변을 반환한는 것은 중요한 과제이다. LLM이 이전 대화를 참조할 수 있도록 chat history와 현재 query를 적절하게 LLM에 입력하여 사용자가 chatbot이 자신과의 대화를 기억하는 것처럼 보이게 해야 한다. chat history를 다루는 방법은 크게 두 가지가 있다. 6.1. Chat Engine Context',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': '가장 간단하게 chat history를 다루는 방법이다. retrieval 결과+query와 함께 chat history를 LLM에 입력하는 방식이다.',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Illustrated by the author 6.2. Chat Engine condense plus context chat history와 query를 LLM에 입력하여 새로운 query를 생성한 후 RAG을 수행하는 방법이다. Illustrated by the author Reference Retrieval-Augmented Generation for Large Language Models: A Survey https://medium.com/@vipra_singh/building-llm-applications-advanced-rag-part-10-ec0fe735aeb1#979b https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6 https://ai.plainenglish.io/advanced-rag-part-01-problems-of-naive-rag-7e5f8ebb68d5 https://www.pinecone.io/learn/advanced-rag-techniques/ SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION https://github.com/AkariAsai/self-rag/tree/main × Search',\n",
       "  'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/'},\n",
       " {'text': 'Grouped-Query Attention (GQA) 빠른 추론을 위해 Grouped-query attention (GQA)를 사용. (LLaMa2에도 사용되었다.) Sliding Window Attention (SWA) 더 낮은 계산 비용으로 더 긴 시퀀스를 처리하기 위해 Sliding Window Attention (SWA)를 사용. Performance in details 아래 그래프는 LLaMa2 family 중 7B와 13B 그리고 LLaMa1 34B와 Mistral 7B의 성능을 각종benchmark를 통해 비교한 자료이다. 성능 평가 결과, Mistral 7B는 모든 metric에서 LLaMa2 13B의 성능을 뛰어 넘었고, LLaMa1 34B와는 유사한 성능을 보이는 것으로 확인되었다. (34B 모델로 LLaMa2가 아닌 LLaMa1 이 채택된 이유는 단순히 LLaMa2 34B가 공개되지 않았기 때문이다. LLaMa 개발팀이 34B 성능이 다른 사이즈들보다 좋지 않아 공개하지 않았다고 LLaMa2 논문에서 밝혔다.) 성능 측정에 사용된 benchmark의 범주는 아래와 같이 정리할 수 있다: Commonsense Reasoning: Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge 그리고 CommonsenseQA의 0-shot 평균 World Knowledge: NaturalQuestions과 TriviaQA의 5-shot 평균 Reading Comprehension: BoolQ와 QuAC의 0-shot 평균 Math: GSM8K with maj@8의 8-shot과 MATH with maj@4의 4-shot 평균 Code: Humaneval과 3-shot MBPP의 0-shot 평균 Popular aggregated results: 5-shot MMLU, 3-shot BBH 그리고 3-5-shot AGI Eval (English multiple-choice questions only)',\n",
       "  'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/'},\n",
       " {'text': '모델간 cost 대비 performance를 비교하기 위해 “equivalent model sizes” 계산 metric을 제안하였다. 즉, 두 모델이 유사한 성능을 내기 위해 model parameter(size)를 각각 어디까지 scaling up해야 하는지 계산하는 metric을 제안하였다. 여기에는 아래 그래프와 같이 MMLU, Commonsense Reasoning, World Knowledge 그리고 Reading comprehension benchmark가 사용되었다. 실험 결과, reasoning, comprehension and STEM reasoning (MMLU)에서 Mistral 7B보다 3배 이상의 파라미터가 있어야 LLaMa2 모델이 동등한 성능을 낸다는 것이 밝혀졌다. 이를 통해 Mistral이 LLaMa2보다 메모리 절약과 데이터 전송 처리에 효과적인 모델임을 알 수 있었다고 한다. equivalent model sizes 실험에 사용된 benchmark 중 World Knowledg를 제외한 모든 범주에서 Mistral 7B가 LLaMa2 13B보다 좋은 성능을 보였다. (이와 같은 결과는 Mistral의 제한적인 parameter 수로 인해 압축할 수 있는 지식의 양이 제한되었기 때문으로 추측된다고 한다.) Mistral과 LLaMa2가 각각의 논문에서 수행한 성능 평가에서 주요하게 다른 점은 아래와 같다: MBPP의 경우, Mistral에서는 hand-verified subset을 사용하였다. Trivia QA의 경우, Mistral에서는 Wikipedia context를 제공하지 않았다.',\n",
       "  'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/'},\n",
       " {'text': 'Sliding Window Attention (SWA) mechanism Mistral 7B는 각 layer가 이전 4096개의 hidden staet를 참조하는 Sliding Window Attention (SWA) mechanism을 사용한다. 이 방법론은 O(sliding_window.seq_len)의 선형 계산 비용을 가지기 때문에 고안되었다. 실제로 해당 방법론 적용으로 FlashAttention과 XFormers는 sequence length 16k, window size 4k일 때 2배 정도의 속도 향상이 있었다고 한다. Sliding Window Attention은 현재 time step에 대한 window size 이상의 과거 context를 참조하기 위해 transformer의 stacked layer들을 사용한다: layer $k$의 token $i$는 layer $k-1$에 있는 $[i-sliding_window, i]$에 집중한다. 그리고 $k-1$에 있는 이 token들은 $[i-2*sliding_window, i]$에 집중한다. higher layer일수록 attention pattern이 수반하는 것보다 더 멀리 있는 정보에 접근할 수 있다. Rolling buffer cache 1. Vanilla attention Attention은 sequence 내 토근 간의 정보를 공유하는 방법이다. Vanilla Transformer의 attention은 causal mask 방식을 채택하여 sequence의 각 token이 자신을 포함한 과거의 모든 token에 attention을 하였다. 이와 같은 방법은 미래의 token 예측에 과거의 정보만 사용할 수 있다. fixed attention span은 rotating buffer를 사용하여 cache 크기를 sliding_window token size로 제한할 수 있는 것을 의미한다. 이를 통해 모델 품질에는 영향을 미치지 않으면서 추론 시 8192 sequence length에 대한 cache memory를 절반 정도 줄일 수 있다.',\n",
       "  'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/'},\n",
       " {'text': '2. Sliding window to speed-up inference and reduce memory pressure attention 연산 횟수는 sequence length에 대해 이차적(quadratic)이고 memory pressure은 sequence length에 대해 선형적이다. 이로 인해 추론 시 cache 가용성이 감소하기 때문에 데이터 처리량은 줄고 시간은 더 지연된다. 이 문제를 완화하기 위해 Mistal은 각 token이 window size만큼의 과거 토큰에만 attention을 하는 Sliding Window Attention을 사용한다. 아래 그림은 window size가 3인 경우이다. 해당 방법론의 특이 사항은 sliding window 범위 밖에 있는 token도 next word prediction에 영향을 미친다는 점이다. 각 attention layer에서 정보는 최대 window size 만큼의 token만 전달될 수 있다. 따라서 2개의 attention layer를 거친 후에는 2*window size만큼의 정보가 전달될 수 있다. 예를 들어 sequence length가 16k이고 sliding window size가 4k일 때 4 layer를 거친 후에는 정보가 전체 sequence length로 전달된다. 이와 같이 해당 방법론이 sliding window 밖의 정보도 전달하지만 sequence 길이가 너무 길어지면 모델이 더 이상 full context를 전부 사용하진 않는 현상이 관찰되었다고 한다 3. Rolling buffer cache Mistral에는 rolling buffer cache가 적용되었다. cache는 W(window size)로 고정되고 (key, value)값을 cache position(i%W) 내의 position i에 저장한다. position i가 W보다 크면 과거 cache의 value들이 덮어씌워진다.',\n",
       "  'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/'},\n",
       " {'text': '4. Pre-fill and chunking sequence generating 시 각 token은 이전 token에 의존하기 때문에 token을 하나씩 예측해야 한다. 하지만 prompt는 이미 앞서 나와 있으니까 이에 대한 (k, v) cache를 미리 채울 수 있다(pre-fill). Prompt가 아주 큰 경우에는 작은 조각(chunk)으로 나누어 각 chunk로 pre-fill을 할 수 있다. 이때 chunk size는 window size로 선택 가능하다. 따라서 attention 연산 시 각 chunk에 대해 연산을 수행하면 된다. Fine-tuning Mistral 7B for chat Mistral 7B의 일반화 능력 검증을 위해 Mistral 팀은 Huggingface에 공개된 instruction dtat로 fine-tuning된 chat model을 만들었다. (data에 다른 trick이나 독점 데이터를 추가하지 않았다고 한다.) 그 결과, MT-Bench에서 모든 7B 모델보다 좋은 성능을 보였고, 13B chat model들과는 유사한 성능을 보였다고 한다. Reference https://mistral.ai/news/announcing-mistral-7b/ https://github.com/mistralai/mistral-src × Search',\n",
       "  'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/'},\n",
       " {'text': '2.1 Foundation Model - pretraining methodology 1) Pretraining Data Meta의 제품 또는 서비스의 데이터를 포함하지 않은 공개적으로 사용 가능한 소스를 학습 corpus 추가 개인 정보가 포함된 특정 사이트들의 데이터 제거 2 trillion tokens 학습 최대한 사실적인 출처를 up-sampling함으로서 hallucinations을 줄임 2) Training Details Standard transformer architecture의 부분을 수정하여 사용 Pre-normalization using RMSNorm SwiGLU activation function Rotary positional embeddings LLAMA1과의 주된 차이점: Context length 증가 (2048 tokens → 4096 tokens) 추론 성능 향상을 위한 GQA (Grouped-Query Attention) 적용 Tokenizer bytepair encoding (BPE) algorithm 사용 total vocabulary size는 32k tokens 2-1) Training Details : Context length 조정 결과 더 긴 context window는 더 많은 정보 처리를 가능하게 한다: longer histories in chat applications various summarization tasks understanding longer documents 아래 두 표는 각각 context length만 다른 모델이 long-context benchmarks와 general tasks를 수행한 결과이다. 2-2) Training Details : GQA (Grouped-Query Attention) 적용 결과 MHA: 일반적인 autoregressive decoding 방식 sequence 내 이전 token의 K와 V쌍을 caching. 이 방식은 attention 연산 속도 증가에 기여한다. context window나 batch size가 증가할 경우 K, V와 관련된 메모리 비용이 크게 증가한다는 단점이 있다. (메모리 문제 해결을 위해 제안된 것이 MQA이다.)',\n",
       "  'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'text': 'MQA: 성능 저하와 학습이 불안정하다는 한계를 지니는데 이를 해결하기 위해 제안된 방법론이 GQA이다. GQA: H개 존재하던 K, V head를 G개의 그룹으로 줄이는 것이다. (즉, G=1일 경우 MQA가 되는 것이기 때문에 MQA와 MHA를 적절히 섞은 방법론이라고 볼 수 있다.) 아래 표는 동일한 모델에서 Attention 방식만 바꾸어 실험한 MHA, MQA, GQA 비교 결과이다. 3. Fine-Tuned Chat Model 대화 use-cases에 최적화된 fine-tuned version self-supervised learning으로 학습된 LLAMA2를 fine-tuning하여 chat version을 만듦 3.1 fine-tuning methodology Foundation Model LLAMA2를 SFT(Supervised Fine-Tuning)하여 LLAMA2-Chat을 생성 SFT를 통해 만들어진 LLAMA2-Chat에 RLHF(Reinforcement Learning from Human Feedback)를 수행하여 반복적으로 튜닝 1) Supervised Fine-Tuning(SFT) LLAMA2-Chat 개발진들은 처음에 publicly available instruction tuning data를 사용하였는데 데이터 품질 문제로 인해 직접 FT data를 수집했다. 27,540개의 high-quality SFT data를 수집하여 학습에 적용한 결과, 비교적 적은 양의 고품질 데이터가 다량의 저품질 데이터보다 모델 성능 향상에 기여함을 확인하였다. 수집한 SFT data는 아래 샘플과 같이 Prompt와 Response로 구성된다. 2) Reinforcement Learning with Human Feedback(RLHF) : Human Preference Data Collection',\n",
       "  'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'text': 'Reward Modeling을 위해 인간이 선호하는 데이터를 수집한다.(100만개의 Meta reward modeling data 수집) Annotation을 위해 hyperparameter중 temperature만 변경하여 만든 두 모델이 사용되었다. annotator는 선호도를 significantly better, better, slightly better, or negligibly better/unsure로 구분하여 labeling한다. annotation 작업 시 helpfulness(유용성)와 safety(안정성)에 초점을 맞추어 각각 따로 labeling을 수행한다. helpfulness: 모델의 응답이 사용자의 요청을 얼마나 잘 만족시키는가 safety: 연구진들이 설정한 safety guideline을 잘 지키는가 Human annotation은 매주 batch 단위로 수집되었다. 수집 데이터가 쌓일수록 Reward Modeling의 성능이 개선되어 점점 더 나은 LLAMA2-Chat이 더 나은 방향으로 학습할 수 있게 된다. 3) Reinforcement Learning with Human Feedback(RLHF) : Reward Modeling 유용성과 안정성은 Bai et al., 2022a연구 결과, 서로 trade-off 관계에 있다는 사실이 알려져 단일 보상 모델을 유용성 보상모델(Helpfulness RM)과 안전성 보상모델(Safety RM)로 분리하여 학습과 최적화를 진행 Reward Model은 Aligned Model의 생성문의 Safety, Helpfulness에 대해 각각 binary classification task를 수행하도록 학습 보상 모델의 정확도는 LLAMA2-Chat의 최종 성능으로 이어지는 중요한 지표이다. 따라서 보상 모델의 개선이 곧 LLAMA2-Chat의 개선이다. 4) Reinforcement Learning with Human Feedback(RLHF) : Iterative Fine-Tuning',\n",
       "  'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'text': '기존의 RLHF 훈련 파이프라인은 선형적 진행 방식을 띄고 있어 SFT와 Reward Model간 상호 보완이 안 되는 문제점이 있는데 이를 보완하기 위해 RM과 SFT 훈련을 반복적으로 수행하는 방법론을 제안함. LLAMA2-Chat 개발 시 매주 수집되는 reward modeling data로 더 나은 Reward Model을 학습하여 RLHF-V1, . . . , RLHF-V5와 같이 여러 버전을 생성 Iterateve FT 도입으로 helpfulness와 safety가 좋아졌다. RLHF fine-tuning에는 두 가지 주요 알고리즘에 사용되었다. Rejection Sampling : Aligned Model이 다수의 response를 생성하고 그 중에서 reward score가 가장 높은 response를 채택하는 과정 PPO(Proximal Policy Optimization): Rejection Sampling과 Reward Model을 이용한 최종 모델 RLHF 훈련 Rejection Sampling: Aligned Model에서 K개의 sample을 생성한 후 Reward Model을 사용하여 최적의 후보를 선정하는 방법. 계속 동일한 promp를 활용하여 response를 생성하고 k개의 response 중 가장 높은 reward score를 가진 sample을 새로운 Gold Response로 채택 LLAMA2-Chat(70B)에만 Rejection Sampling 수행 7B, 13B, 34B은 70B의 Rejection Sampling 데이터를 fine-tuning하여 큰 모델의 능력을 작은 모델로 distillation',\n",
       "  'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'text': 'PPO(Proximal Policy Optimization): RL에서 가장 많이 사용되는 알고리즘인 표준 PPO 사용 Aligned Model 과 Reward Model을 이용하여 PPO 강화학습을 진행 PPO 훈련시 Reward Model score는 Safety와 Helpfulness 중 하나로 학습되도록 설계 4) System Message for Multi-Turn Consistency : Gatt(ghost attention) Method chatbot으로 사용하기 위한 추가적인 SFT 방법. chatbot user의 초기 instruction을 지속적으로 반영하여 답변을 생성하기 위한 방법으로서 해당 방법론으로 multi-turn이 가능한 chatbot을 생성한다. 초기 instructin을 매 입력마다 강제적으로 삽입하는 방법은 입력 문장의 길이가 길어지게 되어 inference 비용이 커지는 문제점이 있다. 이를 해결하기 위해 LLAMA2에서는 Ghost Attention을 제안한다. system message와 마지막 turn message를 제외한 모든 message의 loss를 0으로 설정해 초기 system message가 turn이 오래 지속되도 계속 유지되도록 함 중간 message들이 학습에는 반영되지 않지만 마지막 message를 학습할 때 참고는 되어서 ghost attention이라고 부름 4. Results 4.1 Pretrained Model 모델 성능 비교는 open-source model과 closed-source model에 대해 각각 수행. 성능 평가에는 standard academic benchmarks 사용. 대부분의 task에서 가장 좋은 성능을 보인다. 가장 많은 데이터로 pretrain이 진행되었기 때문으로 추측된다.',\n",
       "  'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'text': 'Llama1보다 성능 좋음 Code benchmark외 모든 benchmar에 대해 MPT보다 성능 좋음 모든 benchmark에 대해 Falcon보다 성능 좋음 LLAMA2–70B는 MMLU과 GSM8K에서 GPT-3.5에 근접한 성능을 보임 LLAMA2–70B는 모든 benchmark에서 PaLM(540B)과 비슷하거나 더 좋은 성능을 보임 LLAMA2–70B는 GPT-4와 PaLM-2-L에 비해 낮은 성능을 보임 4.2 Reward Model safety, helpfulness 모두 좋은 성능을 보인다. Reward Model 평가에는 StreamSHP-XL(FLAN-T5-xl기반), Open Assistant(DeBERTa V3 Large기반 보상모델), GPT-4(OpenAI’s API)가 사용됨 평가 결과, Meta RM이 전반적으로 높은 성능을 보이는 것으로 확인됨. 4.2 Chat Model helpfulness와 safety에 대해 chatgpt보다도 좋은 성능을 보인다. 특히 safety가 높다.(34b의 safet가 안 좋아서 공개하지 않은 거라고 한다) Reference Hugo Touvron et al.”Llama 2: Open Foundation and Fine-Tuned Chat Models,” . 2023. https://ai.meta.com/llama/ × Search',\n",
       "  'source_title': 'LLAMA 2: Open Foundation and Fine-Tuned Chat Models\\n      10 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/10/LLMA2/'},\n",
       " {'text': 'Full Fine-Tuning에는 데이터, 컴퓨터 자원, 시간 등 대규모 자원이 요구되는 문제가 있다. Large Language Model이 많이 나오며 이렇게 자원이 많이 요구되는 Full Fine-Tuning을 대체하며 LLM을 효율적으로 학습시킬 수 있는 방법론들이 활발이 연구되고 있다. 해당 연구는 주로 LLM tuning 이후의 성능을 더 좋게 하거나 모델을 보다 빠르게 훈련시킬 수 있도록 하는 방향으로 진행되고 있다. 이와 같은 연구 중 하나인 Parameter-Efficient Fine-Tuning (PEFT)에는 다양한 변gud들이 존재하는데 대부분 상대적으로 적은 dataset으로 model의 일부 parameter를 업데이트하거나 소수의 새로운 파라미터를 추가하는 방법들이다. PEFT의 장점은 크게 세 가지로 볼 수 있다: Reduced memory footprint PEFT은 model의 파라미터 중 제한된 일부 parameter만 업데이트하고 model의 원래 parameter는 유지시켜 학습 시 요구되는 메모리를 크게 줄일 수 있다. Faster training 훈련 할 parameter, 즉 업데이트되는 parameter의 수가 적기 때문에 자연스럽게 훈련 시간도 단축된다.',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': 'LoRA Low-Rank Adaptation (LoRA)는 저차원 행렬을 훈련시키는 방법으로, LLM을 빠르고 효과적으로 학습시킬 수 있도록 한다. Transformer 기반의 모델들의 경우 대부분의 parameter가 self attention layer에 있기 때문에 self attention layer weights matrix에 LoRA를 적용하는 것이 효과적이다. Low-Rank Adaption (LoRA)',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': '위 이미지처럼 LoRA는 pre-trained LLM layer의 parameter weight matrix $W$를 frozen 시키고 $W$ 외에 “adapter”라 불리는 두 개의 행렬 A와 B를 추가한다. 추가된 두 행렬은 $W$보다 작다. 예를 들어 $W$의 크기가 $d x d$일 때 A와 B의 크기는 $d x r$과 $r x d$이며, $r$($rank$)은 일반적으로 100 이하의 매우 작은 크기이다. $r$의 크기가 클 수록 더 많은 parameter를 학습시키게 된다. 더 많은 parameter를 학습시킨 다는 것은 더 좋은 성능으로 이어질 가능성이 있지만 학습 시간이 길어진다는 단점이 있다.',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': '학습이 진행 될 때 frozen된 $W$와 B*A에 동일한 값을 입력한 후 B*A의 출력을 original matrix $W$의 출력에 추가한다. 즉, 일부 parameter를 학습시키고 그 출력을 original prediction에 더하여 모델에 영향을 주는 방식으로 작동된다. 처음에 행렬 A는 평균이 0인 랜덤 값들로 구성된다(Random values of mean zero). 즉, 랜덤 변수의 값들이 평균적으로 0을 중심으로 분포된다. 그리고 B는 완전히 0으로 초기화된다. 이와 같은 처리를 통해 adpater 행렬이 original matrix $W$의 출력을 완전 초기부터 변경시키지 않도록 한다. A와 B의 parameter가 적절한 방향으로 tuning될 때 A와 B의 출력이 $W$의 출력에 영향을 미치도록 조정하기 위함이다. LoRA는 끝쪽에서만 이루어져야 하는 것은 아니고 neural network 내부의 깊은 layer에도 적용 가능하다. LoRA rank 설정 LoRA를 처음 도입한 논문에서 Microsoft 연구진들은 rank가 16을 넘기 loss 값이 일정해지는 것을 발견했다. Microsoft 연구진들은 rank가 4에서 32 사이일 때 trainable parameter 수를 줄이면서 성능을 유지한다고 제안했다.',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': 'Low-Rank Adaption (LoRA) QLoRA Low-Rank Adaption (LoRA) QLoRA는 4-bit Quantized language model into Low Rank Adapters의 줄임말로, 양자화를 거친 모델에 대해 LoRA를 활용해 tuning하는 기법을 뜻한다. 계산량 및 메모리 감축을 위해 QLoRA의 저자들은 크게 세 가지 기법을 사용했다. 4-bit NormalFloat Pretrained Model의 weight를 4-bit로 양자화시켰을 때 그 데이터 타입을 NormalFloat이라고 한다. 일반적으로 컴퓨터는 실수를 32-bit로 floating(fp32)하고 모델들은 보통 FP16으로 경량화되어 사용된다. Double Quantized 이 기법은 이중 양자화로, 양자화된 것을 또 양자화하는 것이다. 앞서 4-bit NormalFloat으로 양자화 된 weight를 또 양자화하는 것이 아니라 weight를 양자화하면서 발생한 quantization constant(양자화 상수)를 양자화하여 조금 더 가볍게 만드는 것이다. Paged Optimizer 이 기법은 모델 자체에 대한 경량화는 아니지만 제한된 컴퓨터 자원에서 모델을 사용할 때 GPU가 사용하는 VRAM page를 CPU의 RAM에도 일부 저장할 수 있게 해서 메모리를 최대한 사용핟로록 하도록 하는 방법이다. 이 기법의 적용으로 동일한 크기의 모델에 대해 OOM 발생을 피할 수 있는 같다.',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': 'process를 memory에 올릴 때 process를 page 단위로 나누어 처리함. DoRA Low-Rank Adaption (LoRA)',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': 'DoRA(Weight-Decomposed Low-Rank Adaption)는 LoRA의 변형 중 하나로, 행렬이 행렬의 길이/크기(magnitude)와 방향(direction)의 곱으로 분해(decompose)될 수 있다는 개념에서 출발한 방법론으로, weight matrix $W$는 matrix 크기 m과 방향 v로 분해되어 각각 독립적으로 tuning한다. 즉, pretrained matrix $W$를 magnitude vector m (size $1xd$)와 direction matrix V로 분리하여 magnitude와 direction을 독립적으로 학습시킨다. direction matrix V는 standard LoRA approach와 같이 B*A matirx에 의해 강화되고, m은 또 따로 그대로 훈련된다. 이와 같이 direction 강화/업데이트에 집중하여 학습이 진행되기 때문에 Full Fine-Tuning에 준하는 tuning 방법이 될 수 있다고 DoRA 저자는 주장한다. 또한 LoRA가 학습에 사용하는 parameter보다 더 적은 수의 parameter를 사용하면서도 더 좋은 성능을 낸다고 한다. 그 외 다른 PEFT방법론 Adapter Adapter based model은 frozen pre-trained method 위에 layer를 추가하고(추가되는 위치는 조정 가능하) fine-tuning 과정에서 추가된 layer만 학습시키는 방법이다. 이때 추가된 layer를 adapter라고 부른다. 학습 시에는 adapter 외의 다른 모델 부분들은 변경되지 않는다. 추론 시 추가적인 부탐이 발생하여 비효율적이다. Soft Prompting (prompt tuning) 관련 포스트: PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training) P-tuniung: GPT Understands, Too',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks randomly initialised soft token을 input prompt에 추가하고, LLM의 가중치는 frozen시킨 상태에서 해당 embedding을 학습시킨다. prompt를 추가함으로써 모델에는 형태가 입력된다. prompt에는 \"다음 문장에 대한 감성을 긍정, 부정, 중립으로 표시하시오\" 와 같이 모델이 task를 잘 인지하도록 하는 문구가 들어간다. 이 과정을 통해 frozen 된 모델이 해당 task에 대해 가장 답변을 잘하도록 만드는 prompt를 얻게 된다.',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': '× Search',\n",
       "  'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/'},\n",
       " {'text': 'Fine-tuning은 일종의 transfer learning으로, Pre-trained Model를 특정 task에 적합하도록 model에 관련 지식을 주입하는 과정이다. Fine-tuning 시에는 model의 network architecture는 그대로 두고 모델의 weight만 업데이트시킨다. 매우 많은 비용이(방대한 학습 데이터, 컴퓨터 자원, 학습 시간 등) 소요되는 full pre-training과 달리 fine-tuning은 full pre-trained 된 모델을 활용하여 비교적 효율적으로 모델을 원하는 task에 맞게 학습시킬 수 있다. 1. Fine-tuning Data 1) Data collection and selection 데이터의 품질과 적합성은 tuning 결과에 주요한 영향을 미치기 때문에 데이터 선정은 매우 중요한 과정이다. 학습의 목적에 따라 적합한 데이터는 모두 상이하다. 학습의 목적이 Domain adaptation인 경우에는 해당 domain과 관련 있는 un-labelled data가 필요하다. 학습의 목적이 특정 task를 수행하는 모델 개발인 경우에는 task에 적합한 labeled data가 필요하다. labeled data의 내용과 형식은 task에 따라 달라진다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'labeled data는 un-labelled data에 비해 수집에 많은 비용이 들고, 높은 품질의 labeled dataset을 구축하기도 어렵다. 최근에는 LLM을 사용하여 labeled data를 수집하는 방법도 크게 유행하고 있다. 이와 같은 방법은 human-labeling보다 비용이 적게 들고, 시간도 적게 소요된다는 장점이 있다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'LLM pre-training 시에는 web-crawling data가 일반적으로 사용된다. 2) Data pre-processing Quality-related pre-processing(데이터 품질 관련 전처리) 형식 구조화, 중복 제거, 개인정보 제거 Basic pre-processing in NLP (기본적인 자연어 전처리) normalization: 문장 내 과도한 공백 혹은 과도한 emoji 등(노이즈)을 제거 tokenization: text를 token으로 변환 embedding: token을 모델이 처리할 수 있는 embedding 값으로 변환 chunking, splitting …',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': '2. Training Tranformers 계열의 모델은 label이 없는 데이터에 대해서도 supervised learning을 한다. 이는 un-labeled data를 입력 받아 모델 내부에서 각 모델마다 설정된 task에 맞게 데이터를 처리하여 학습에 사용하기 때문이다. 예를 들어 BERT는 입력 문장의 일부를 masking 처리하여 자체적으로 masking한 부분을 예측하며 학습을 수행하고, GPT의 경우에는 현재 시점 이후의 token들을 보지 않고 다음 token들을 예측해 나가며 학습을 수행한다. 이와 같이 알고리즘이 자체적으로 스스로 통제/감독하며 학습하는 방식을 self-supervised learning이라고 한다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': '학습은 기본적으로 loss function을 최적화 하는 model weight를 찾는 것을 목표로 한다. Pre-training 과정을 예를 들어 설명하겠다. GPT계열(decoder based model)은 학습 시 일반적으로 Causal Language Modeling(CLM, =Standard Language Modeling = Original Language Modeling)을 사용하는데, CLM 수행 시 모델은 vocabulary의 모든 token에 대한 예측 확률을 ground truth값과 비교하여 loss를 계산한다.(이때 prediction값은 1.0의 확률을 가진 sparse vector이다.) loss function은 모델의 세부 architecture에 따라 다를 수 있지만 일반적으로 cross-entropy 혹은 perplexity loss를 많이 사용한다. 모델이 loss function을 최적화하는 방식은 loss를 최소화 하는 방향으로 weight를 조정해나가는 것이다. loss를 계속 최소화하기 위해 매 iteration마다 backpropagation 과정에서 경사 하강(gradient descent)을 수행한다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Illustrated by the author Backpropagation, Gradient Descent neural network의 weight를 학습시킬 때 사용되는 알고리즘으로, 역으로(출력에서 입력 방향으로) 오류를 전파하여 가중치를 업데이트하는 방법이다. Backpropagation 과정은 간략하게 아래와 같다:',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Forward Pass (순전파): Backpropagation을 위해서는 예측값이 있어야 하니까 우선 순전파로 모델의 예측값을 구한다. Loss 계산: loss function을 통해 예측값과 실제값 간의 오를 계산한다. (cross-entropy, perplexity loss 등을 사용) (오차함수(error function) = 손실함수(loss function) = 비용함수(cost function)) Backward Pass (역전파): 손실함수를 통해 산출된 전체 에러값($E$)을 출력층->입력층으로 전파하며 각 layer마다의 weight가 전체 에러값 $E$에 얼마나 영향을 미쳤는지 그 기여도를 구하하고 그걸 경사하강법에 대입하여 가중치를 업데이트 한다. 1) 산출된 오차값을 출력층->입력층으로 전파하여 각 layer의 weight가 전체 오차에 얼마나 영향을 주었는지 그 기여도 Chain Rule을 통해 구한다. 이 값은 해당 layer의 weight에 대한 기울기이다. 2) 가중치 업데이트 공식에 산출된 기울기와 Learning Rate를 넣어 가중치를 업데이트한다. 이때 사용되는 가중치 업데이트 공식이 경사하강법이다. Learning Rate는 얼마나 빨리 학습시킬지 정하는 것인데 일반적으로 0.1보다 낮은 값으로 직접 설정한다. 2.1 Training Strategies',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Supervised Fine-tuning : labeled data로 모델 학습 Unsupervised Fine-tuning : un-labled data로 모델 학습 Transfer Learning : pre-trained model을 freeze 시키고 마지막 layer만 pre-trained model이 학습한 task와 동일한 task에 대해 다른 데이터로 학습시키는 것. (Fine-tuning은 pre-trained model을 가져와서 model parameter 전체를 작은 Learning Rate로 학습시킴) 2.2 Hyperparameter Learning Rate : 학습 시 weight update 속도를 결정하는 값으로, 매 iteration마다 weight가 얼마나 크게 조정될지를 결정하는 요소이다. 예를 들어 Learning Rate가 0.01이라면, 매 iteration마다 weight가 변화하는 양은 0.01배로 조정다. Learning Rate 값이 작을 때는 모델이 천천히 학습한다. 수렴하는 데 시간이 오래 걸리지만, 최적의 값을 찾을 가능성이 높다. Learning Rate 값이 클 때는 모델이 빠르게 학습한다. 하지만 너무 크면 최적의 값을 지나쳐 버리거나 학습이 불안정해질 수 있다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Batch Size : 모델이 학습할 때 한 번에 처리하는 데이터 샘플의 개수이다. Batch Size가 작으면 한 번에 적은 양의 데이터를 처리한다. 메모리 사용량은 적지만 느리게 학습될 가능성이 높다. Batch Size가 크면 한 번에 많은 데이터를 처리한다. 학습이 빠를 수 있지만 메모리 사용이 많다는 문제가 있다. Epochs : 모델이 전체 데이터셋을 보는 횟수이다. Warm-up Steps : 훈련 초기에 Learning Rate 점진적으로 증가시켜 안정성을 높이는 요소이다. 2.3 Regularization Techniques 모델 학습 시 overfitting(과적합) 방지를 위해 몇 가지 정규화 기법이 사용된다. Dropout : 학습 시 일 neuron을 랜덤하게 비활성화하는 기법. 이를 통해 특정 뉴런에 모델이 너무 의존하지 않도록 할 수 있고, 다양한 neuron 조합을 학습하게 되어 모델의 일반화 능력을 향상시킬 수 있다. 예를 들어 Dropout 비율이 0.5라면, 학습 중 각 학습 단계마다 neuron의 절반을 무작위로 비활성화한다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Weight Decay : weight에 작은 패널티를 부과하여 weight 값이 너무 커지지 않도록 하는 방법이다. 예를 들어 Weight Decay 값이 0.01이라면, 가중치 업데이트 시 가중치 값의 1%가 패널티로 추가된다. Early Stopping : validation set에서 성능 개선, 즉 loss 가 더이상 떨어지지 않을 때 학습을 중단시킨다. 2.4 Evaluation Metrics Accuracy : 전체 예측 중에서 맞게 예측한 비율을 나타내는 지표 $\\\\text{Accuracy} = \\\\frac{\\\\text{맞게 예측한 샘플 수}}{\\\\text{전체 샘플 수}}$ 분류 문제에서 자주 사용된다. 하지만 class 불균형이 있는 경우 성능을 제대로 반영하지 못한다는 한계점이 있다. F1 Score : precision과 recall의 조화 평균으로, Precision과 Recall의 균형을 잡아주기 때문에 불균형 데이터셋 평가에 유용하다. $\\\\text{F1 Score} = 2 \\\\times \\\\frac{\\\\text{Precision} \\\\times \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}$ BLEU Score : 번역 과제 평가에 많이 사용된다. 번역된 텍스트와 target 텍스트 간의 n-gram 일치를 측정한다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Perplexity : 언어 모델의 예측 성능을 평가하는 지표로, 낮을수록 모델의 예측이 정확함을 의미한다. $PPL = e^{-\\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log p(w_i)}$ 언어 모델의 생성 품질이나 자연스러운 언어생성 평가는 어렵다. 3. Supervised Fine-Tuning (SFT) Pre-training 과정에서는 앞서 언급한 CLM과 같이 단순히 다음 token을 예측하는 task를 수행하며 학습이 진행된다. 하지만 보다 task/domain specific한 모델을 만들기 위해서는 supervised fine-tuning(SFT) 접근 방식 필요하다. SFT는 일반적으로 pre-trained Model에 대해 labeled data를 학습시키는 과정으로 진행된다. 예를 들어 CLM으로 학습된 모델로 conversation 혹은 instruction following 과제를 잘 수행하는 모델을 만들기 위해 이와 같은 target application을 잘 나타내는 dataset을 학습시킨다. 이렇게 학습한 모델은 특수한 요구사항 혹은 과제에 더 적합한 답변을 생성할 수 있다. 3.1 Full Fine-Tuning Pre-trained Model에 대해 Pre-trained Model이 학습한 데이터셋보다 작은 데이터로 모델 전체 layer를 다시 훈련 시키는 것이다. 즉, 모델 전체 parameter를 업데이트시키는 것이다. Full Fine-Tuning에는 몇 가지 문제점이 있다.',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'model의 전체 layer를 학습시키기 위해서는 많은 계산비용이 요구되는데 이는 곧 긴 훈련 시간과 큰 컴퓨터 자원 요구로 이어진다. Catastrophic forgetting문제 발생 가능성이 있다. 이는 모델이 모델이 새로운 task에 대해 학습하는 동안 pre-training 과정에서 배운 general knowledge를 잊어버리는 문제이다. 이는 잘 학습된 사전학습 모델을 무쓸모로 만든다. 모델의 전체 wight가 업데이트되기 때문에 사전학습 모델과 동일한 크기의 모델이 서버에 또 저장되기 때문에 용량 문제가 생길 수 있다. 위와 같은 문제를 완화하기 위해 Parameter-Efficient Fine-Tuning와 같은 대안들이 많이 연구되고 있다. 3.2 Parameter-Efficient Fine-Tuning (PEFT) Parameter-Efficient Finetuning (PEFT) methods 3.3 Representation Fine-tuning (ReFT) Pre-trained Model의 가중치는 frozen시키고, Model의 representation의 일부를 조작하여 downstream task를 해결하도록 하는 방법이다. ReFT의 종류 중 하나인 Low-rank Linear Subspace(LoReFT)는 PEFT보다 10배에서 50배 더 parmeter를 효율적으로 사용한다고 한다. 4. Fine-tuning variations : RLHF/PPO, DPO, ORPO Fine-tuning variations : RLHF/PPO, DPO, ORPO Reference',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'https://medium.com/@aipapers/reft-representation-finetuning-for-language-models-4e804753e886 https://levelup.gitconnected.com/the-5-prompt-engineering-techniques-ai-engineers-need-to-know-a208af13d8e4 https://towardsdatascience.com/different-ways-of-training-llms-c57885f388ed https://medium.com/@animeshchaturvedi007/fine-tuning-large-language-models-a-comprehensive-guide-0446975dc40e https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224 × Search',\n",
       "  'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/'},\n",
       " {'text': 'Encoder input를 representation을 변환하는 부분. 자연어의 의미를 vector space의 representation으로 변환하는 부분이다. 즉, contextualized embedding으로 input을 변환시킨다. input으로 들어온 token은 self-attention layer를 통과하는데 이는 input token encoding 시 token들의 관계를 파악하도록 돕는다. Attention 모델이 입력 sequence의 모든 token들이 서로의 관계를 학습할 수 있도록 Multi-Head Self-Attention을 사용한다. 이는 모델이 각 token의 context를 이해하는데 도움을 준다. Decoder encoding 된 representation을을 받아 모델의 출력을 생성하는 부분. input 학습 시에는 정답 sequence(shifted right)가 입력되고, 추론 시에는 이전에 예측된 token이 입력된다. Attention Masked Multi-Head Self-Attention 현재 시점 이후의 token들에 대해 masking 처리를 하여 Masked Multi-Head Self-Attention을 사용한다. 이와 같은 처리는 현재 시점 이전 정보만을 가지고 현재 시점의 token을 예측하도록 한다. Multi-Head Attention with Encoder Output Encoder의 출력(input에 대한 representation)을 받아서 input sequnece와 decoder의 input으로 입력 받은 target sequnece 간의 관계를 매핑하며 학습하도록 돕는다.',\n",
       "  'source_title': 'LLM architecture\\n      01 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/01/llm_architecture/'},\n",
       " {'text': 'output token별로 softmax 함수를 거쳐 (현재 시점을 기준으로 다음 token으로) 예측된 단어의 확률 분포를 산출하고 가장 높은 확률의 token을 출력한다. Attention Attention은 입력 sequence의 중요한 부분에 대해 모델이 집중할 수 있도록 하여 sequence 내부에서의 token 간의 거리와는 무관하게 token 간의 관련성을 파악하도록 하는 알고리즘이다. Transformers 기반 Model 요약 Encoder-Only Models 대표 모델: BERT Pretraining Approach/Task: 기본적으로 Masked Language Modelling (MLM)을 통해 학습하며, 모델에 따라 이외에 추가적인 task를 함께 수행한다. BERT는 MLM 외에 NSP를 학습한다. Use Case: 일반적으로 classification task에 많이 활용된다. Decoder-Only Models 이전 time step의 출력이 다음 time step의 입력으로 들어가는 점이 자기 회기적이기 때문에 Auto-regressive model이라고도 불린다. 최근 LLM은 대부분 Decoder-Only 구조를 가진다. 대표 모델: GPT Pretraining Approach/Task: Next Token Prediction. Original Language Modeling이라고도 많이 불린다. Use Case: 일반적으로 Generative task에 많이 사용된다. (e.g. generating text, completing sentences, answering questions based on context, etc. ) Encoder-Decoder Models 대표 모델: T5, BART, Gemini Pretraining Approach/Task: Task에 따라 다르다.',\n",
       "  'source_title': 'LLM architecture\\n      01 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/01/llm_architecture/'},\n",
       " {'text': 'Reasons Why Causal Decoders Are Often Used for Generative Tasks In-Context Learning 관련 포스트: Language Models Summary - GPT2 : Language Models are Unsupervised Multitask Learners GPT-2에서 제안된 In-Context Learning은 Decoder 기반 모델의 생성 과제 수행 능력을 높인다. in-context learning은 문맥 내에서 풀고자 하는 task를 학습하는 것을 의미한다. LLM은 이 능력을 통해 문맥이나 task에 대한 이해를 높이는 prompt를 활용하여 다양한 생성 과제들을 잘 수행할 수 있다. Efficiency Optimization Decoder 모델은 이전 token의 key, value matrix를 다음 token 예측에 재사용할 수 있다. 현재 시점에서 token 예측에 이전 token들만 참조는데, 이때 이미 처리된 key, value matrix에 대해서는 추가적으로 계산할 필요가 없어 계산 비용 측면에서 효율적이다. Autoregressive Decoder Model에서는 Masked Multi-Head Self-Attention을 사용한다. 여기에서 집중해야 하는 것은 “Masked”와 “Multi-Head”이다. “Masked”가 의미하는 것은 현재 시점을 기준으로 다음 token들은 참조하지 못하도록 막아 놓는 다는 것이다. 다음 token은 참조하지 않고 이전 token을 현재 시점의 입력으로 받아 예측을 수행한다. 이를 autoregressive하다고 표현한다. 아래 그림과 같이 Casual Decoder Model은 “am”을 예측할 때 이전 token인 “I”만 보고 “a”를 예측할 때는 “I”와 “am”을 본다.',\n",
       "  'source_title': 'LLM architecture\\n      01 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/01/llm_architecture/'},\n",
       " {'text': 'DocLLM: A layout-aware generative language model for multimodal document understanding Autoregressive라는 특징이 생성 과제에 도움을 주는 이유는 attention matrix가 full rank status를 유지하기 때문이다. 즉, 양방향이 아닌 선형적이다. 입력된 representation들이 독립적으로 처리되기 때문에 이전 시점의 정보가 손실 없이 다음 시점으로 전달된다. 이는 다음 token 예측에 긍정적인 영향을 미친다. full rank status: matrix의 rank가 그 matrix의 차원과 동일하다는 의미. $m×n$ matrix에서 $m≤nm$일 때, rank가 $m$인 경우, $m≥n$일 때, rank가 $n$인 경우 rank: matrix에서 서로 선형적으로 독립적인 행과 열의 최대 개수. “Multi-Head”는 여러 attention head들이 있을 때 각 head가 각각 고유한 Query, Key, Value set을 가지고 있어 입력 문장의 다양한 측면을 동시에 집중하여 sequence 내 token 간의 복잡한 관계를 잘 파악하도록 한다. 각 헤드는 고유한 Query, Key, Value 벡터 세트를 가지고 있어 입력 문장의 다른 측면에 동시에 집중할 수 있습니다. Reference What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? × Search',\n",
       "  'source_title': 'LLM architecture\\n      01 Oct 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/01/llm_architecture/'},\n",
       " {'text': '1. Introduction 인공지능은 인간의 지각 능력을 모방한 것이다. 일반적으로 modality는 특정한 센서를 통해 생성된 vison과 language와 같은 unique communication channel을 지칭한다. 인간은 세상과 상호작용할 때 다양한 modality 정보를 적절히 활용한다. 각 modality는 각각 다른 informationo source로 표현된다. 예를 들어 이미지는 수천개의 pixel을 통해 시각적으로 표현되고, 텍스트는 이산적인 단어들을 통해 표현된다. 인공지능 multimodal 구현을 위해서는 인간이 처리하는 각 modality 표현들과 유사한 정보를 데이터로 사용하고 각 정보들을 연결해야 한다. 본 논문은 Transformer를 이용한 Multimdoal Learning 방법론들에 대해 초점을 맞춘 Survey논문dmfh, 다양한 modality와 task를 transformer를 통해 구현하고 학습시키는 것과 관련된 다양한 정보에 대해 다룬다. 2. Transformers 번역과제를 위해 NLP 분야에서 나온 모델로, 최근 다양한 인공지능 모델에 변형되어 활용되는 모델이다.',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '2.1 Attention Transformers의 구조도 중요하지만 Transformers에서 사용된 Attention도 중요하다. Transformers에 크게 두 가지 Attention이 사용된다 1. Self Attention: 하나의 sequence 내에서 수행되는 attention modlity와 무관하게 입력된 sequence를 fully-connected graph로 처리할 수 있다.(graph의 노드는 tokenize된 token들이다.) 2. Encoder-Decoder Cross Attention: encoder에서 decoder로 넘어갈 때 사용되는 attention 2.2 Tokenization/Embedding Modality에 따라 Transformer의 Tokenization/Embedding 방식이 다르다. 1. Vanilla Transformer(NLP) Vanilla Transformer의 input은 token embedding과 position embedding 그리고 추가적으로 segment embedding과 같은 extra learnable embedding을 element wise로 더한 것이다. 2. Vision Transformer: ViT Vision Transformer(ViT)에서 Vanilla Transformer의 token embedding 과정에 해당하는 과정은 입력된 이미지를 고정된 크기의 patch로 잘라 Linear Projection layer에 넣어 flatten 시켜 patch embedding을 구하는 방식으로 수행된다. 그리고 Vanilla Transformer과 마찬가지로 positional embedding과 더하여 모델에 입력한다. Pinecone(https://www.pinecone.io/learn/series/image-search/vision-transformers/)',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '3. Multimodel Transformers 최근 Transformer를 활용한 Multimodal 연구가 활발히 이루어지고 있다. 연구 결과, discriminative task와 generative tasks에서 모두 여러 modlaity에 대해 호환되는 것으로 확인되었다. 3.1 Multimodal Input 3.1.1 Tokenization and Embedding Processing 1) input tokenizing : 입력된 input의 modality에맞게 tokenizing한다. 2) representation embedding space 선택: tokenizing된 것들으 token의 modality에 따라 그에 맞는 representation embedding space를 선택한다. 예를 들어 아래 표와 같이 token의 modality가 RGB이면 token의 단위는 patch, embedding은 linear projection. token의 modality가 text이면 token 단위는 word, embedding은 learned embedding(Model parameter를 통해 embedding한 결과) 아래 표와 같이 다양한 modality에 대해 다양한 tokenizing 방식과 embedding space가 존재한다. Multimodal Learning with Transformers:A Survey 3.1.1 Token Embedding Fusion 각 modality별로 tokenizing-embedding을 모두 마친 이후 중요한 것 각 modality별 embedding을 어떻게 합쳐서 multimodal 처리를 가능하게 할 것인가이다. 가장 단순한 fusion은 여러 modality들에 대한 embedding들을 token-wise sum하는 것이다. transformer는 하나의 position에 대해 여러 token embedding을 반영할 수 있기 때문에 가능한 방법이다. 예를 들어 BERT의 경우 한 position embedding에 token embedding과 segment embedding을 position을 기준으로 원소값별로 더하여(element-wise sum하여) 모델에 입력한다. 이를 시각적으로 표현하면 아래 이미지와 같다. Multimodal Learning with Transformers:A Survey(edited by author)',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '3.2 Cross-modal interaction Token Embedding Fusion외에도 다양한 modal간의 interaction 방법들이 있는데 multimodal Transformers에서는 self-attention과 변형된 self-attention을 통해 modality 간의 interaction을 수행한다. self-attention design 관점으로 Transformer의 multimodal modelling 방법을 나누면 크게 6가지로 분류할 수 있다. Multimodal Learning with Transformers:A Survey 3.2.1 Early summation (token-wise, weighted) 가장 단순한 Fusion 방식으로, token-wise sum 하는 방법이다. 3.1.1에서 다룬 방식과 동일. 각 modality의 token embedding들을 position을 기준으로 더하여 Transformers에 입력되는 방식. 단순한 방식으로 수행되는 만큼 계산 복잡도가 증가하지 않는다는 것이 주요 장점이다. 주요 단점은 가중치가 수동으로 설정되었다는 것이다. Multimodal Learning with Transformers:A Survey 3.2.2 Early concatenation(= all-attention, CoTransformer)',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': 'Modality들의 token embedding sequence concatenate하여 Transformer layer에 입력하는 방법이다. 이 방법은 modality들의 token position이 하나의 sequence로 처리되어 각 modality의 context를 반영할 수 있다는 장점을 가진다. VideoBERT는 multimodal Transformer의 초기 연구 중 하나로, 이와 같은 방식을 통해 video와 text를 결합하여 global multimodal context를 잘 encoding한 모델이다. 이 방법은 concatenation으로 길어진 Sequence로 인해 계산 복잡도가 증가된다는 단점을 가진다. 이는 Attention 자체의 한계점으로, sequence가 길어질수록 계산 복잡도가 높아진다. Multimodal Learning with Transformers:A Survey 3.2.3 Hierarchical attention (multi-stream to one-stream) Modality에 따라 각각 독립적인 Transformer를 통해 input을 encoding한 이후 각 Transformer stream들의 output embedding을 concat하여 하나의 또다른 Transformer에 입력함으로써 Embedding들을 융합시키는 방식이다. 이러한 방식은 late interaction/fusion로 분류될 수도 있고 early concatenation의 특수 케이스로 분류될 수도 있다. Multimodal Learning with Transformers:A Survey 3.2.4 Hierarchical attention (one-stream to multi-stream)',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '각 Modality들의 token embedding들을 concat한 이후 하나의 Transformer Layer에 입력하고 그 Transformer output을 각 modality별로 나누어 각각 별도의 Transformer에 다시 입력하는 방식이다. 이와 같은 방식은 one-stream 단계에서 각 modality들의 toekn embedding이 concat된 것이 Transformer에 함께 임력됨으로써 modality들 간의 self-attention이 수행되어 이로 인해 각 modality 간의 interaction이 반영된다. 그리고 multi-stream 단계에서는 각 modality별로 독립적인 Transformer에 입력되어 최종 embedding을 산출하기 때문에 uni-modal representation의 독립성을 유지할 수 있다는 장점이 있다. 이 방법론이 적용된 대표적인 모델은 InterBERT이다. Multimodal Learning with Transformers:A Survey 3.2.5 Cross-attention (= Coattention)',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '각 Modality별로 별도의 Transformer를 사용하고, Modality별 Q(Query) embedding을 cross-stream 방식으로 교환 및 교체하는 방식이다. 각 Modality의 최종 Embedding을 산출할 때 다른 Modality를 반영하면서도 계산 복잡도는 증가되지 않는다는 장점을 가진다. 하지만 각 Modality가 각각의 Transformer에 입력되기 전 cross-modal attention을 수행하지 못하여 전체 context를 놓칠 수 있다는 단점이 있다. 이 방법론은 VilBERT에서 처음 제안되었다. 해당 논문에서는 two-stream cross attention 은 modality 간의 interaction을 수행하긴 하지만 각 modality 내 자체 context에 대한 self-attention은 수행하지 않는다고 밝혔다. Multimodal Learning with Transformers:A Survey 3.2.6 cross-attention to concatenation Cross-attention의 단점을 보완하기 위해 나온 방법으로, Cross-attention에 Hierarchical 구조를 추가하여 global context를 반영할 수 있도록 만든 방법이다. 이는 Cross-attention을 통해 산출된 Embedding을 concat한 이후 또다른 Transformer layer에 입력한다. Multimodal Learning with Transformers:A Survey + Network Architectures 지금까지 알아본 multimodal Transformer는 Network Architecture와 interaction timing을 기준으로 아래 그림과 같이 분류할 수 있다. Multimodal Learning with Transformers:A Survey(edited by author) 4. Multimodal Pretraining task Taxonomy Multimodal Pretraining에 사용되는 task의 종류는 아래 표와 같이 매우 다양하다. task들은 크게 Masking, Describing, Matching, Ordering로 분류된다. Multimodal Learning with Transformers:A Survey(edited by author) 5. Challenges and Designs 5.1. Fusion',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '일반적으로 multimodal transformer 모델은 input단에서 하는 Fusion인 Early Fusion, Intermediate Representation단에서 Middle Fusion, Prediction단(Transformer의 output)에서 Late Fusion 이렇게 세 Level에서 modality간의 정보를 Fusion한다. 이에 대해 해당 연구진들은 Future Work로서 Multimodal Transformer의 Fusion을 조금 더 향상시키고, 해석 가능하게 하기 위하여 Modality간의 interaction이 어떻게 진행되는지를 연구하고 Modality간의 Fusion을 정량적으로 측정하는 것을 연구하는 것도 의미 있을 것이라고 한다. 5.2. Alignment Modality 간의 Alignment는 실제 Multimodal 활용 측면에서 매우 중요하다. 다양한 downstream task에 대해 연구가 활발히 진행되고 있다. (예를 들어 Text-to-Speech Alignment, Text-to-Video Retrieval 등) Paired Sample에 대한 Contrastive Learning을 통해 두 modality를 공통된 REpresentation Space로 매핑하는 것이 대표적인 방법이다. 그러나 이를 위해 수백 수십억개의 학습 데이터가 필요하기 때문에 막대한 cost가 필요하다는 것이 문제라 이 문제를 해결하기 위한 연구도 multimodal 연구에서 중요한 과제 중 하나일 것이다. 5.3. Transferabililty',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '서로 다른 데이터셋과 Application에 대해 Multimodal Model을 어떻게 Transfer 시킬 것인가가 Transferability이다. 모델 학습에 사용되는 Training data와 Practical(실제) data 간의 분포차이가 존재한다. (ML/DL에서는 일반적인 문제.) Label이 잘 되어 있거나 Align이 잘 되어 있는 Supervised data (well-labeled, well-aligned)를 통해 pretrain된 multimodal transformer를 실제로는 label이 없거나 align이 잘 되어 있지 않은 실제 데이터에 바로 적용한는 것은, 즉 transfer하는 것은 매우 어렵다. 실제 application 상황에서 예를 들어 이미지와 텍스트 정보를 이용해서 downstream task를 수행하는 모델을 가지고 사용하려 했는데 이미지 혹은 텍스트 둘 중 하나의 modality의 데이터가 없는 경우 실제 inference하는 것은 매우 어려운 일이다. 5.4. Efficiency',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '모든 transformer계열 모델들이 지적받는 문제인 효율성. model parameter 크기가 크기 때문에 학습에 많은 데이터를 필요로 한다. 왜냐하면 transformer라는 모델 자체가 layer를 많이 쌓을수록 downstream task에 대한 성능이 높아진다는 것이 그동안 다양한 연구를 통해 실증적으로 입증된 것이기 떄문이다. Multi modal transformer에서도 각 modality별로 별개의 transformer 방법론을 사용할 경우 이에따라 단일 modality보다 더 많은 모델 파라미터가 사용된다는 단점이 존재한다. 모델 파라미터가 더 많기 때문에 이에 준하는 더 많은 학습 데이터가 필요하게 된다. 그리고 Self-attention의 특성 상 입력 sequence의 길에에 따라 시간과 메모리 복잡도가 기하급수적으로 증가한다는 단점도 존재한다. 이전 연구들 중에서 이러한 efficiency 문제를 완화하기 위해 제안된 방법들은 주로 Knowledge Distillation 혹은 각 modality에 대해서 서로 다른 크기의 model을 사용하는 방법 혹은 self-attention을 효율적으로 전개하는 시도들이 있었다. 5.5. Robustness',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '대규모 데이터로 Pretrain된 multimodal transformer는 다양한 Multimodal application(downstream task)에서 SOTA를 달성한 반면 Robustness에 대한 연구는 아직 덜 진행되었다. 이에 따라 해당 연구진들은 Robustness를 어떻게 이론적으로 설명할 것이고 Robustness를 어떻게 증가시킬 것인가에 대한 연구도 multimodal 연구에서 중요한 방향이라고 말했다. 5.6. Universalness Multimodal Learning의 매우 다양한 Task와 Modality로 인해 Universlness는 Multimodal Transfer Model에게 매우 중요한 문제이다. Universalness와 관련하여 도전한 몇 가지 연구들이 있다.',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '(1) Uni-modal과 multimodal input 혹은 task에 대한 pipeline 통합하는 시도 실제 상황에서는 modality의 누락으로 인해 uni-modal data를 처리해야 할 수도 있기 때문에 uni-modal 그리고 multi modal 두 가지 pipeline을 통합하는 것도 매우 유의미한 연구이다. (2) Multimodal Understanding과 Generation을 위한 Pipeline을 통합하는 것이다. 자연어처리 측면에서 이해해 보자면 natural language understanding은 BERT와 같은 transformer encoder가 잘하는 task, 그리고 natural language generation은 GPT와 같은 transformer decoder가 잘하는 task이다. 이처럼 multimodal에서도 understanding과 generation을 둘 다 수행할 수 있도록 multi task learning을 시도하는 연구들이 있었다고 한다. (3) Task 자체를 통합하고 변형하는 시도 CLIP이 매우 대표적인 시도이다. CLIP은 Zero-shot Recognition을 Retrieval Task로 바꿔서 Model을 변경하지 않고도 해당 Task를 수행할 수 있도록 했다. 5.7. Interpretability 해석 가능성. 이는 multimodal transformer 뿐만 아니라 deeplearnign 모델들에 있어서도 공통적인 challange이다. transformer가 왜 그리고 어떻게 multimodal learning에서 좋은 성능을 보이는가에 대한 해석 가능성도 중요한 연구 방향임을 해당 연구진들은 밝혔다. Reference Multimodal Learning with Transformers:A Survey https://www.pinecone.io/learn/series/image-search/vision-transformers/ × Search',\n",
       "  'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "  'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/'},\n",
       " {'text': '실험 대상 pdf는 우리은행 (20240603) 금융시장 브리프.pdf로, Google에 “금융보고서”를 검색하여 랜덤하게 다운받은 pdf이다. 해당 pdf에서 차트와 표 그리고 텍스트가 고루 포함된 페이지를 대표 실험 대상으로 삼음. 해당 보고서 내용과 본 포스트는 무관함. 결론 문제점 pdf layout detection + 한국어 text extraction을 모두 잘하는 모델이 없음 table 정보를 한번에 잘 추출하는 모델이 없음 해결 방안 방법1 text: layout detection 이후 bbox내 text 추출, 각 단계마다 가장 괜찮은 방법은 아래와 같음: detection : PP-YOLOE text 추출 : PyMuPDF table: table은 경우에 따라 다르게 추출. 1) 표가 그림으로 삽입된 경우: 그림 추출 -> 그림에서 텍스트 추출 그림 추출 : PyMuPDF 그림에서 text 추출: pytesseract 2) 표가 문서 내 표로 삽입된 페이지: 이 경우 그림으로 표가 추출되지 않음. table 추출 : pdfplumber 방법2 Donut / Nougat 모두 한국어 text 추출에 취약함. 이는 Decoder 문제로 예상됨. Encoder로부터 bbox를 얻고, Decoder를 PyMuPDF로 대체하여 text 추출 Decoder model을 교체(한국어 잘하는 Transforemer decoder only model 들 중 하나로.)',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '최종 개발물 해결 방안 - 방법 1 채택 최종 개발물로 올린 데모 가로형 PDF 최종 개발물 데모 (가로형 PDF) 세로형 PDF 최종 개발물 데모 (세로형 PDF) Title과 Text 그리고 Figure 위치를 분리하여 추출 양호 추출된 layout 내 text 추출 성능 양호 개선 예정 table 내 한국어 추출 성능이 좋지 않아 이 부분 수정 예정 최종 개발물 데모 (table 추출) 1. PDF Layout detection 1.1 PP-YOLOE layout detection 확인한 것들 중 가장 잘 되는 모델 1.1.1 PP-YOLOE 기반 PDF Layout detection 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.1.2 PP-YOLOE + PyMuPDF 중국 모델로, layout 내 한국어 text 추출에 취약하여 text 추출은 PyMuPDF로 교체하여 실험한 결과 detection 부터 extraction까지 좋은 결과가 나옴. table과 image에 대한 처리는 추가 커스텀이 필요해 보임. PP-YOLOE로 layout detection 이후 PyMuPDF로 text extraction 1.2 microsoft/Florence-2-large 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 <반 PDF Layout detection 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '1.3 Detectron2 문제점: Detectron2 모델 사용에 필요한 layoutparser에서 제공하는 text extraction 모듈 TesseractAgent가 한국어를 지원하지 않아 text 추출이 제대로 되지 않음 Detectron2로는 text 위치만 파악하고, 얻어진 bbox를 활용하여 다른 text 추출 라이브러리 혹은 모델을 사용해야 할 것 같음 요약: Pirma가 가장 나음 다른 detection label이 함께 추출되어 layout만 잘 감지한다면 pdf 구조화 시 custom이 용이할 것으로 보임. e.g. Pirma label {1:”Page Frame”, 2:”Row”, 3:”Title Region”, 4:”Text Region”, 5:”Title”, 6:”Subtitle”, 7:”Other”} modelzoo에서 확인 가능 1.3.1 Pirma text dectection: PDF 내 텍스트 위치 파악 능력이 나쁘지 않음. 하지만 감지된 부분의 라벨 정보는 부정확함. table detection: 표가 그림으로 삽입된 경우, 표를 감지하지 못함. 표가 표로 삽입되었을 때는 잘 감지함. 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.2.2 HJDataset HJDataset은 거대 Aisan language dataset으로, 이를 학습한 모델로 문서 layout을 감지하는 task를 수행한다. text와 table 전반적으로 다 잘 감지하지 못함.',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.2.3 PubLayNet 결과가 포괄적인 경향이 있음. 제목 위치가 layout에 포함되지 않는 등의 문제가 있음. 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.4 PDF to XML PDF 구조를 XML로 변경하여 layout 파악(PDFQuery 사용) 1.4.1 PDF to XML 시각화 표가 그림으로 삽입된 페이지 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 1.4.2 PDF to xml + PyMuPDF PyMuPDF 일반 text는 대부분 잘 추출하는 편. 표가 그림으로 삽입된 페이지의 table data 추출 못함. 표가 문서 내 표로 삽입된 페이지의 table data의 경우 텍스트는 추출되지만 정리되지 않음. 표가 그림으로 삽입된 페이지 | 표가 문서 내 표로 삽입된 페이지 2. PDF text extraction 2.1 microsoft/Phi-3 image에서 한국어 인식 잘 못함. # Import necessary libraries from PIL import Image import requests from transformers import AutoModelForCausalLM from transformers import AutoProcessor from transformers import BitsAndBytesConfig import torch from IPython.display import display import time, tqdm # Define model ID model_id = \"microsoft/Phi-3-vision-128k-instruct\"',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '# Load processor processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True ) # Define BitsAndBytes configuration for 4-bit quantization nf4_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16, ) # Load model with 4-bit quantization and map to CUDA model = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"cuda:1\", trust_remote_code=True, torch_dtype=\"auto\", quantization_config=nf4_config, ) def model_inference(messages, path_image): start_time = time.time() image = Image.open(path_image) # Prepare prompt with image token prompt = processor.tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) # Process prompt and image for model input inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:1\") # Generate text response using model generate_ids = model.generate( **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=500, do_sample=False, ) # Remove input tokens from generated response generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :] # Decode generated IDs to text response = processor.batch_decode( generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False )[0] display(image) end_time = time.time() print(\"Inference time: {}\".format(end_time - start_time)) # Print the generated response print(response) prompt_cie_front = [{\"role\": \"user\", \"content\": \"<|image_1|>\\\\nOCR the text of the image. Most of the text is in Korean.\"}] path_image = \"./1.png\" model_inference(prompt_cie_front, path_image) microsoft/Phi-3-vision-128k-instruct OCR 결과 2.2 PdfReader + ocrmypdf pypdf.PdfReader로 text 추출 추출된 게 거의 없고, pdf 대부분이 imgage로 이루어져 있으면 ocrmypdf로 ocr한 이후 다시 pypdf.PdfReader로 text 추출 from pypdf import PdfReader import ocrmypdf',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': 'def extract_text_from_pdf(reader): full_text = \"\" for idx, page in enumerate(reader.pages): text = page.extract_text() if len(text) > 0: full_text += f\"---- Page {idx} ----\\\\n\" + page.extract_text() + \"\\\\n\\\\n\" return full_text.strip() def convert(pdf_file): reader = PdfReader(pdf_file) # Extract metadata metadata = { \"author\": reader.metadata.author, \"creator\": reader.metadata.creator, \"producer\": reader.metadata.producer, \"subject\": reader.metadata.subject, \"title\": reader.metadata.title, } # Extract text full_text = extract_text_from_pdf(reader) # Check if there are any images image_count = 0 for page in reader.pages: image_count += len(page.images) # If there are images and not much content, perform OCR on the document if image_count > 0 and len(full_text) < 1000: out_pdf_file = pdf_file.replace(\".pdf\", \"_ocr.pdf\") ocrmypdf.ocr(pdf_file, out_pdf_file, force_ocr=True) # Re-extract text reader = PdfReader(pdf_file) full_text = extract_text_from_pdf(reader) return full_text, metadata convert(\"/workspace/PDF_Parsing/(20240603) 금융시장 브리프.pdf\") 3. PDF table extraction 3.1 camelot-py 표가 그림으로 삽입된 페이지 테이블 감지는 잘 됨 테이블 내 text 추출이 잘 되지 않음 아래 그림을 보면 표 위치에서 감지하는 text가 없음 camelot-py table detection result(Illustrated by the author) camelot-py table text extraction 표가 문서 내 표로 삽입된 페이지 테이블 감지 잘 됨 테이블 내 text 가 추출 됨. 추가 후처리 모듈을 붙이면 깔끔하게 나올 것 같음 camelot-py table detection result(Illustrated by the author) camelot-py table text extraction 3.2 pdfplumber',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '표가 그림으로 삽입된 페이지는 추출되는 text가 없음 표가 표로 삽입된 페이지는 아래와 같이 잘 추출됨. pdfplumber table text extraction 4. PDF table extraction after image extraction 1단계. pdf에서 이미지 추출 import pymupdf def extract_images_from_pdf(pdf_path, output_folder): doc = pymupdf.open(pdf_path) for page_num in range(len(doc)): page = doc.load_page(page_num) image_list = page.get_images(full=True) for img_index, img in enumerate(image_list): xref = img[0] base_image = doc.extract_image(xref) image_bytes = base_image[\"image\"] # Save the image image_filename = f\"{output_folder}/page_{page_num + 1}_img_{img_index + 1}.png\" with open(image_filename, \"wb\") as image_file: image_file.write(image_bytes) 2단계. 이미지로부터 텍스트 추출 from PIL import Image from pytesseract import pytesseract image = Image.open(\\'/workspace/PDF_Parsing/Pix2Struct/images_pymu/page_5_img_2.png\\') image = image.resize((400,200)) text = pytesseract.image_to_string(image, lang=\\'kor\\') print(\\'detected text : \\',text) 결과 PDF table extraction after image extraction 5. SwinTransformer Encoder - BART Decoder (Donut / Nougat) Encoder로 layout detection 후 Decoder로 bounding box 내부 text 추출하는 모델 Donut Github: https://github.com/clovaai/donut/ Nougat Github: https://github.com/facebookresearch/nougat/tree/main?tab=readme-ov-file 5.1 Donut (naver clovaai) Donut text extraction result',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '추출되는 것이 거의 없음 5.2 Nougat (meta) 추출되는 것이 전혀 없음 5.3 보강 방안 두 모델 모두 한국어 text 추출에 취약함. SwinTransformer로 bbox 특정 후 아래 두 방법으로 한국어 추출 문제를 완화할 수 있을 것으로 보임 Decoder가 하는 역할을 Rulbase 한국어 추출을 잘 하는 라이브러리로 text를 따로 추출하는 방법 Decoder model을 변경하여 × Search',\n",
       "  'source_title': 'RAG : 한국어 PDF Parser 개발(layout detection + text extraction)\\n      02 Feb 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/02/02/pdf/'},\n",
       " {'text': '대한민국 갈등 사례(갈등 - 원인 - 이해관계자 -해결) 검색 chat을 위한 대한민국 갈등 사례 보고서 및 논문 (한국 행정 연구원으로부터 데이터 수령) 대한민국 국방 관련 검색 chat을 위한 국방 보고서 및 논문 (한국 국방 연구원으로부터 데이터 수령) 2.2 PDF data parsing parsing 방식: Rule-base parsing 사용 라이브러리 : langchain.document_loaders.PyPDFDirectoryLoader 3. Model openai gpt-4-turbo LLaMa2 -> fine tuning model (fine-tuning log) Mistral -> fine tuning model (fine-tuning log) Solar -> fine tuning model (fine-tuning log) 3.1 Embedding Model text-embedding-ada-002(chunk size=1000) 4. Chat application programming interface (API) gradio (LLM tuning 결과 개인 확인용 api) Fastapi (협업 백엔드 개발자 선호에 맞춰 개발) Flask (협업 백엔드 개발자 선호에 맞춰 개발) 5. Naive-RAG 이외 적용 기술 5.1 Pre-Retrieval binary vectorstore(document type) 5.2 Enhanced indexing strategy binary vectorstore(document type) document type에 따라 2-3개의 vector db에 분류하여 저장 metadata filtering을 위한 Hierarchical index retrieval chunk별 요약 결과 metadata로 저장 hierarchical similarity search -> 3단계 (1. query <-> document summary similarity search vectorstore내에서 1번에서 검색된 document를 기반으로 similarity search (속도 향상을 위한 filtering) retriever 정의 시 2번에서 검색된 document를 기반으로 similarity search) 5.3 Retrieval diverse Prompts(query type) query type별로 다른 prompt 적용 5.4 Post-Retrieval translate(en->ko) LLM 영어 답변 반환 시 번역 similarity-based reordering 6. Chat 관련 추가 기능',\n",
       "  'source_title': 'RAG\\n      02 Jan 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/01/02/rag/'},\n",
       " {'text': 'return source file 좋아요 싫어요 버튼 추가 [답변 일시, 소요 시간, 질문, 답변, 사용자 ip] 저장 기능 추가 RAG 기반 질문-대답 챗 Weaviate의 retrieval 결과를 받아 prompt와 함께 LLM에 입력 후 결과 반환 RAG 이후 LLM 답변 시 참조된 문서 요약(source 번호 입력) 질의응답 내용과 참조 문서 정보를 저장하여 참조 문서 번호 입력 시 해당 문서에 대한 요약 결과 반환 문서 제목 입력 후 문서 요약 -문서 제목 입력 시 문서 요약 결과 반환 문서 전체 길이에 따라 LLM 입력 횟수 증가+Json 형식 답변 반환 표 출력을 위한 json 형식 반환 “표“, “json” 포함 질문 입력 시 json 형식 반환 prompt와 함께 LLM에 입력 후 결과 반환 7. Problems and Solutions 속도 문제 LLM 추론 속도 문제: vllm 적용 similarity search 연산 속도 문제: 문서가 많아 시간이 생각보다 많이 소요되어 binary vectorstore, metadata filtering, hierarchical similarity search 적용 query 유형에 따라 LLM 추론 품질이 상이한 문제 query 유형 분류를 통해 다른 prompt를 타도록 pipeline 구성 LLM 영어 반환 문제 port-retrieval 과정으로 영어 답변 반환 시 번역 과정 추가',\n",
       "  'source_title': 'RAG\\n      02 Jan 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/01/02/rag/'},\n",
       " {'text': 'LLM 답변 정확도 및 품질 저하 문제 similarity-based reordering을 통해 검색된 chunk 중 가장 관련 있는 문서를 더 많이 참조하도록 조정 × Search',\n",
       "  'source_title': 'RAG\\n      02 Jan 2024',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2024/01/02/rag/'},\n",
       " {'text': '[Lora/ QLora / Full Fine tune] base model: LLaMa 2 13b → qlora LLaMa 13b → lora polyglot 12.8b → full fine tune / lora polyglot 5.8b → lora polyglot 3.8b → full fine tune polyglot 1.3b → full fine tune Related Paper Review (Mistral) Related Paper Review (LLaMa 2) LLM instruction tuning [vLLM RAG + DPO + Qlora] base model: Mistral 7B SOLAR-10.7B Mixtral-8x7B [Qlora] base model: LLaMa 2 13B Related Paper Review (Mistral) Related Paper Review (LLaMa 2) 2. DATA (총 923,590개 한국어 dolly 데이터) 2.1 instruction tuning data 2.1.1 형식 {\"instruction\":instruction, \"context\": context, \"response\":response, \"category\": category} 2.1.2 instruction tuning data로 변환한 데이터 목록 KoVicuna data 구성 : ko_dataset_chatgpt(2개), ko_alpaca_style_dataset KoVicuna data to Dolly data: gpt → instruction human→ response category → open_qa',\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': \"KoVicuna Data 예시(1세트) → gpt와 human 대화 KoVicuna Data sample: {'id': 'QWJhYvA_0', 'conversations': [{'from': 'human', 'value': '고객을 위해 이러한 전략과 전술을 구현하는 성장 마케팅 대행사와 관련하여 Jeff Walker의 제품 출시 공식의 주요 아이디어를 글머리 기호로 요약하세요...'}, {'from': 'gpt',\",\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': \"'value': '다음은 성장 마케팅 대행사가 고객을 위해 적용할 수 있는 제프 워커의 제품 출시 공식의 주요 아이디어입니다:\\\\r\\\\n\\\\r\\\\n1. 타겟 고객과 그들의 니즈를 파악합니다: 제품이나 서비스의 이상적인 고객을 파악하고 이들에게 공감을 불러일으킬 수 있는 메시지를 작성합니다.\\\\r\\\\n2. 사전 출시: 입소문을 만들고, 사용 후기 및 사례 연구를 수집하고, 소셜 미디어를 사용하여 인지도를 높여 출시에 대한 기대감과 흥미를 조성합니다.\\\\r\\\\n3. 출시: 잘 짜여진 출시 시퀀스를 사용하여 판매와 전환을 극대화하세요. 여기에는 보너스 제공, 희소성 창출, 마감일을 사용하여 긴박감을 조성하는 것이 포함될 수 있습니다.\\\\r\\\\n4. 출시 후: 고객과 후속 조치를 취하고, 피드백을 수집하고, 지속적으로 가치를 제공하여 고객의 참여와 충성도를 유지하세요.\\\\r\\\\n5. 제품군을 만듭니다: 초기 제품이 성공하면 동일한 고객의 다른 요구 사항을 해결하는 추가 제품 또는 서비스를 만들어\",\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': \"제품을 확장합니다.\\\\r\\\\n6. 지속적인 최적화: 출시 프로세스와 제품군을 지속적으로 모니터링하고 최적화하여 결과를 개선합니다.\\\\r\\\\n7. 커뮤니티 구축: 출시 프로세스를 사용하여 제품에 대한 열정이 있고 널리 알리는 데 도움을 줄 수 있는 고객 커뮤니티를 구축하세요.\\\\r\\\\n8. 자동화를 활용하세요: 기술 및 자동화를 사용하여 출시 프로세스를 간소화하고 효율성을 개선하세요.'}, ...\",\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': \"Alpaca data Alpaca data to Dolly data: instruction → instruction output→ response input → context category → open_qa Alpaca data 예시(1세트) → instruction, input, output Alpaca data sample: {'instruction': '다음 국가와 관련된 역사적 사건 3가지를 나열하십시오.', 'input': '캐나다', 'output': '치푸아츠(Fort Chippewyan)는 캐나다 알버타주의 따뜻한 해안에 위치한 미국 원주민 지역에 근접한 유일한 유럽인 거주지입니다. 캐나다 프랑스 식민지로 건설되었으며, 나스강을 따라 서부 지역으로 확장된 유일한 유럽인 거주지입니다. 1778 년 새로 지어진 건물에 현재는 휴게소가 있습니다. 시티오브 에드먼튼에서 선퍼드 언덕스를 향해 북쪽으로 4.5시간 떨어진 곳에 있습니다.'} Korquad 2.0 Korquad 2.0 data to Dolly data: Q → instruction A → response context → context category → open_qa\",\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': \"Korquad 2.0 data 예시(1세트) → Q, A, context Korquad 2.0 data sample: {'version': 'KorQuAD_2.0_train', 'data': {'title': '예고범', 'url': 'https://ko.wikipedia.org/wiki/예고범', 'context': WIKIPEDIA CONTEXT 'qas': [{'answer': {'text': '나카무라 요시히로, 히라바야시 카츠토시, 사와다 메구미', 'html_answer_start': 21842, 'html_answer_text': '나카무라 요시히로, 히라바야시 카츠토시, 사와다 메구미', 'answer_start': 6302}, 'question': '드라마 예고범의 감독은 누구일까?', 'id': '8089'}]}} Korquad 1.0 Korquad 1.0 data to Dolly data: Q → instruction A → response context → context category → open_qa\",\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': 'Korquad1.0 데이터 예시(1세트) → Q, A, context Korquad 1.0 data sample: {\\'qas\\': [{\\'question\\': \\'다테 기미코가 최초로 은퇴 선언을 한게 언제지\\', \\'answers\\': [{\\'answer_start\\': 260, \\'text\\': \\'1996년 9월 24일\\'}], \\'id\\': \\'9_f2_wiki_2822-1\\'}], \\'context\\': \"재팬 오픈에서 4회 우승하였으며, 통산 단식 200승 이상을 거두었다. 1994년 생애 최초로 세계 랭킹 10위권에 진입하였다. 1992년에는 WTA로부터 \\'올해 가장 많은 향상을 보여준 선수상\\'(Most Improved Player Of The Year)을 수여받았으며, 일본 남자 패션 협회(Japan Men\\'s Fashion Association)는 그녀를 \\'가장 패셔너블한 선수\\'(Most Fashionable)로 칭했다. 생애 두 번째 올림픽 참가 직후인 1996년 9월 24일 최초로 은퇴를 선언하였다. 이후 12년만인 2008년 4월에 예상치 못한 복귀 선언을 하고 투어에 되돌아왔다. 2008년 6월 15일 도쿄 아리아케 인터내셔널 여자 오픈에서 복귀 후 첫 우승을 기록했으며, 2009년 9월 27일에는 한국에서 열린 한솔 코리아 오픈 대회에서 우승하면서 복귀 후 첫 WTA 투어급 대회 우승을 기록했다. 한숨 좀 작작 쉬어!\"} 2.2 DPO train data 2.2.1 형식',\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': '{\"instruction\":instruction, \"context\": context, \"response\":response, \"category\": category, \"rejected\", rejected} 2.2.2 DPO traindata generate openai api 사용 LLM 학습 데이터의 영어 비율이 높아 한국어 데이터를 통한 tuning 이후에도 추론 시 영어를 반환하는 경우가 있어 rejected에 response의 영어 번역 결과를 입력했음 dataset 출처는 instruction data와 동일 3. Model 3.1 Model Architecture LLaMa 1,2 Mistral Solar Kopolyglot 3.2 Instruction/DPO tuning Hyperparameter 3.2.1 Lora r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\" 3.2.2 Quantization load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=False 3.2.3 Training per_device_train_batch_size=4, gradient_accumulation_steps=4, gradient_checkpointing=True, learning_rate=5e-5, lr_scheduler_type=\"cosine\", num_train_epochs=6, save_steps=5, logging_steps=1, optim=\"paged_adamw_32bit\", optim=\"adamw_torch\", warmup_steps=100, bf16=True, bf16=False, report_to=\"wandb\", save_total_limit=8 3.2.4 DPO trainer beta=0.1, max_prompt_length=1024, max_length=1536, 4. Train 4.1 Training server environment A100 x2 Ubuntu 22.04 5. Merge 5.1 Model Merge RAG 기반 검색형 챗봇의 표 생성 기능 구현을 위해 json format 답변이 필요하여 json format 데이터를 학습한 model을 merge. Model 1 Model 2 Model 2 선정 이유 Merge method mistralai/Mistral-7B-v0.1 DPO tuning (한국어 데이터) mychen76/mistral7b_ocr_to_json_v1 well-formed json format 데이터 학습 Moe 5.2 Model Merge 결과',\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': '한국어 답변을 안정적으로 반환하던 Model 1보다 한국어 추론 성능이 저하됨 Model 1보다 json 형식을 반환하는 비율이 높아졌지만 완전한 json 형식이 아닌 경우가 있어 json 형식 완성을 위한 후처리 모듈이 필요함 6. Problems and Solutions LLM 추론 시 한국어 입력에 대한 영어 답변 반환 현상 -> DPO trainset rejected에 영어 답변 입력 json format 답변 반환을 위해 json format output 데이터를 학습한 모델을 merge json 형식이 완전하지 않아 표 생성이 되지 않는 문제 -> json 형식 완성을 위한 후처리 모듈 생성 × Search',\n",
       "  'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/'},\n",
       " {'text': '중복 문장 제거 중복 문장 추출 후 분리하여 저장 -> 모델 1차 학습 후 autolabeling을 통해 class tagging 데이터 라벨 불균형 완화를 위해 수량이 적은 라벨 데이터 제외(1500건 이하 데이터 제외) BIO-tagging NER 학습 시 필요한 BIO tag 부착. BIO-tag: token classification task와 같이 다수의 token이 하나의 개체임을 표시하기 위해 일반적으로 사용되는 tag B(begin tag): 개체의 시작과 개체명 정보를 포함 I(inside tag): 개체 시작 이후 끝까지 개체 내부를 표시 O(outside tag): 개체외부 표 2.3 Data class [사람, 수, 음식, 직업/직책, 제작물/작품, 조직/기관, 시설물/건물, 국가명, 행정구역, 학문/이론, IT 용어, 제품, 동물 신체, 날짜, 질병/증상/증세, 모양 및 형태 표현, 세포/조직/기관, 동물 종류, 음료, 교통수단, 기간, 패션의류/잡화, 자연, 천체명칭, 기후/날씨, 식물, 스포츠, 계절, 언어, 문화/문명, 사건, 화폐(통화), 약] 3. Model 3.1 Model Architecture BERT Electra',\n",
       "  'source_title': 'NER(Named Entity Recognition)\\n      25 Sep 2022',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2022/09/25/NER/'},\n",
       " {'text': '위 classification model의 활성화 함수 결과를 CRF layer에 입력하여 가장 높은 확률을 가진 개체 sequence를 반환한다. CRF layer 추가: BERT 계열의 모델의 경우, sequence tag 추론 시 token간의 관계를 독립적으로 예측하는 경향이 있어 label간의 종속성을 명시적으로 모델링하기 위해 CRF layer 추가. 즉, label 간의 의존성을 고려한 결과를 반환 받기 위해 CRF layer를 추가. BiLSTM layer 추가: BERT 계열의 모델이 문맥 파악에 강점을 가지고 있지만 BiLSTM layer를 추가함으로써 문장 길이가 길어지거나 문장 구조의 복잡도가 증가했을 때 문맥 정보 파악 능력을 강화할 수 있다. 3.2 Fine-tuning Hyperparameter batch_sizemax_lenlearning_rateoptimizer 1281283e-5Adam 4. Train 4.1 Training server environment Quadro RTX 8000 x3 Ubuntu 20.04 5. Test result precision recall f1-score support accuracy 0.990485 0.990485 0.990485 0 macro avg 0.602809 0.604351 0.600704 434688 weighted avg 0.990405 0.990485 0.990415 434688 [PAD] 1 1 1 368711 O 0.968871 0.965086 0.966974 49149 [CLS] 1 1 1 3396 [SEP] 0.999706 1 0.999853 3396 국가명 0.804412 0.916786 0.854481 215 학문/이론 0.703287 0.782931 0.740854 304 수 0.784006 0.833969 0.808213 1180 사람 0.760027 0.776739 0.767727 2699 행정구역 0.740302 0.735174 0.737718 229 IT 용어 0.687826 0.717803 0.702494 230',\n",
       "  'source_title': 'NER(Named Entity Recognition)\\n      25 Sep 2022',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2022/09/25/NER/'},\n",
       " {'text': '패션의류/잡화 0.616484 0.629004 0.618074 97 음식 0.804267 0.859006 0.830722 656 식물 0.688934 0.589286 0.634223 66 날짜 0.872748 0.877016 0.874115 395 교통수단 0.678835 0.636967 0.656226 80 시설물/건물 0.579067 0.596769 0.587768 341 세포/조직/기관 0.720468 0.755682 0.736507 163 약 0.541667 0.484848 0.504348 26 모양 및 형태 표현 0.631931 0.647917 0.639721 156 직업/직책 0.768607 0.797931 0.782829 541 동물 신체 0.618803 0.741463 0.673407 131 천체명칭 0.889266 0.851875 0.869792 82 질병/증상/증세 0.825712 0.746615 0.783278 204 화폐(통화) 0.346154 0.25 0.290323 23 제품 0.591991 0.613933 0.602731 196 기간 0.769737 0.867371 0.815505 210 동물 종류 0.801515 0.743873 0.771213 164 계절 0.67 0.625 0.623188 25 자연 0.661321 0.599231 0.624736 76 스포츠 0.681624 0.597222 0.628788 62 음료 0.719608 0.783333 0.748832 117 기후/날씨 0.574797 0.59457 0.58127 125 사건 0.638889 0.632331 0.635325 54 제작물/작품 0.659329 0.673122 0.665963 518 언어 0.346154 0.428571 0.382979 24 조직/기관 0.667303 0.618192 0.641447 508 문화/문명 0.584399 0.48913 0.530319 47 6. Problems and Solutions 데이터 라벨 불균형 적은 라벨 데이터 제외 × Search',\n",
       "  'source_title': 'NER(Named Entity Recognition)\\n      25 Sep 2022',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2022/09/25/NER/'},\n",
       " {'text': '출처포맷수량 data labeling 업체 수집 데이터(raw data: web crawling data)txt302,028문장 data labeling 업체 수집 데이터(raw data: aihub 한국어 SNS, 자유대화 데이터)txt184,585문장 aihub data (한국어 단발성 대화 데이터셋: 기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립)txt37,000문장 aihub data (감성 대화 말뭉치: 기쁨, 당황, 분노, 불안, 상처, 슬픔)txt28,000문장 total 551,613문장 2.2 Data pre-processing 중복 문장 제거 중복 문장 추출 후 분리하여 저장 -> 모델 1차 학습 후 autolabeling을 통해 class tagging 이모티콘 제거. 텍스트 자체만 학습 (특수기호는 제거 x) 데이터 라벨 불균형 완화를 위해 “중립”에 속하는 데이터 일부 제거',\n",
       "  'source_title': 'Sentiment Analysis\\n      24 Sep 2022',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2022/09/24/SentimentAnalysis/'},\n",
       " {'text': '2.3 Data class { \"부정\":{\"분노\": [\"분노\", \"억울함\"], \"싫음\": [\"싫음\", \"지루함\", \"미워함\", \"짜증남\"], \"두려움\": [\"두려움\", \"창피함\", \"불안\", \"당황\"], \"슬픔\": [\"슬픔\", \"미안함\", \"안타까움\", \"외로움\", \"실망\", \"괴로움\", \"부러움\", \"후회\"]}, \"긍정\":{\"행복\": [\"행복\", \"그리움\", \"기쁨\", \"사랑\", \"황홀함\", \"감동\", \"즐거움\", \"홀가분함\", \"설렘\", \"만족\", \"자신감\", \"고마움\"], \"놀람\": [\"놀람\"]}, \"중립\":{\"중립\": [\"중립\", \"바람\", \"무관심\"]} } 2.4 Data augmentation method 1차 모델 학습 이후 autolabeling 3. Model 3.1 Model Architecture BERT Electra 3.2 Fine-tuning Hyperparameter versionbatch_sizemax_lenoptimizer 34 emotion6464Adam 8 emotion6464Adam 3 emotion6464Adam 4. Train 4.1 Training server environment Quadro RTX 8000 x3 Ubuntu 20.04 5. Test result versionaccf1 score 34 emotion77.5590.7756 8 emotion840.84 3 emotion790.79 6. Problems and Solutions 데이터 라벨 불균형 “중립” 문장 일부 제거 1차 학습 이후 web crawling 데이터 autolabeling 이후 부족한 라벨 보충 -> 예정 × Search',\n",
       "  'source_title': 'Sentiment Analysis\\n      24 Sep 2022',\n",
       "  'source': 'https://finddme.github.io/dev%20log/2022/09/24/SentimentAnalysis/'},\n",
       " {'text': '1. Introduction PLM은 광범위한 NLU task에 대해 성능을 향상시켰다. 널리 사용되는 방법인 fine-tuning은 PLM의 전체 parameter를 target task에 대해 update한다. fine-tuning은 좋은 성능을 내지만 모든 parameter에 대한 gradients와 optimizer state가 저장된다. 그리고 model의 parameter를 복사하여 가지고 있는 것은 PLM의 크기가 크기 때문에 매우 불편하다. 반면 Prompting은 PLM의 모든 parameter를 freeze시키고 prompt 형태의 query를 LM에 넣는 방식을 사용한다. 감성분석 task에 대해 예를 들어 보면 e.g. “Amazing movie!”라는 sample에 “This movie is [MASK]”라는 prompt를 붙여 PLM이 mask token이 “good”과 “bad” 중 어떤 token의 확률이 높은지 예측하도록 한다. Prompting은 따로 학습을 진행하지 않고 단순히 PLM을 copy하여 사용한다. 하지만 discrete prompting은 fine-tuning에 비해 suboptimal하다. Prompt tuning은 continuous prompt를 tuning하는 방식이다. Liu et al. (2021); Lester et al. (2021)은 original input sequence의 embedding(continuous prompt)에 대해 추가로 학습을 진행하는 것을 제안하였다. 이 방법론의 경우 학습 시 continuous prompt만 update된다. prompt tuning은 prompting이 다양한 task에서 좋은 성능을 내도록 했지만 model size가 충분히 크지 않을 때(10 billion 이하일 때)는 여전히 fine-tuning보다 성능이 좋지 않다. 특히 QA task와 같은 sequence labeling task에서는 fine-tuning보다 현저히 성능이 떨어진다.',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': '본 논문의 main contribution은 prompt tuning과 fine-tuning을 다양한 model scale과 NLU task에 대해 경험적으로 비교하는 것이다. P-tuning v2의 접근법은 기술적으로 새로운 것은 아니다. Deep Prompt Tuning을 더 최적화한 것이라고 볼 수 있다. 가장 주요한 개선 사항은 input layer뿐만 아니라 pretrain model의 모든 layer에 continuous prompt를 적용하였다는 것이다. Deep prompt tuning은 continuous prompt를 늘림으로써 작은 모델과 어려운 task에 대한 fine-tuning과의 격차를 줄였다.',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': 'model size 300M-10B으로 수행한 다양한 extractive QA task나 NER(named entity recognition)과 같은 hard sequence tagging task에 대해 fine-tuning과 P-tuning v2 성능 비교를 하였다. P-tuning v2은 training time memory cost와 per-task storage cost를 대폭 줄임으로서 fine-tuning의 0.1%-3% parameter를 가지고 학습한다. 2. Preliminaries 1) NLU task 본 논문에서는 NLU task를 simple classification tasks 와 hard sequence labeling tasks로 나누어 본다. simple classification tasks에는 간단히 label을 분류하는 task로, GLUE와 SuperGLUE에 있는 대부분의 task들이 여기에 속한다. hard sequence labeling tasks에는 sequence로 이루어진 token에 대한 classification task이다. 여기에는 extractive QA task나 NER(named entity recognition)이 속한다. 2) Prompt Tuning Notation $\\\\mathcal{V}\\\\text{ :vocabulary of a language model}$ $\\\\mathcal{M}\\\\text{ :language model}$ $\\\\mathbf{e}\\\\text{ : embedding layer of Language Mpdel}$ discrete promptingdml 경우 prompt token $\\\\text{{ “It”, “is”, “[MASK]” }}\\\\subset \\\\mathcal{V}$이 movie review classify에 일반적으로 사용될 수 있다. 예를 들어 “Amazing movie!”라는 input text $\\\\mathbf{x}$가 있을 때 input embedding은 $\\\\left [ \\\\mathbf{e}(\\\\mathbf{x}),\\\\mathbf{e}(\\\\mathbf{“It”}),\\\\mathbf{e}(\\\\mathbf{“is”}), \\\\mathbf{e}(\\\\mathbf{“[MASK]”}) \\\\right ]$로 표현된다. Lester et al. (2021)와 Liu et al. (2021) 논문에서는 PLM의 parameter는 freeze시켜 놓고 natural language prompts를 trainable continuous prompts로 교체한다.',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': 'trainable continuous embeddings $\\\\text{[}\\\\mathit{h}{0}, … , \\\\mathit{h}{i} \\\\text{]}$이 있을 때 input embedding sequences는 $\\\\text{[}\\\\mathbf{e}(\\\\mathbf{x}),\\\\mathit{h}{0}, … , \\\\mathit{h}{i} , \\\\mathbf{e}(\\\\mathbf{“[MASK]”}) \\\\text{]}$이다. Lester et al., 2021; Kim et al., 2021; Liu et al., 2021여구에서 10B parameter model로 수행한 Prompt tuning은 simple classification task에 대해 fine-tuning과 견줄 만하다는 것이 증명되었다. 3. P-Tuning v2 3.1. Lack of Universality Prompt tuning은 많은 NLP task에서 꽤 좋은 성능을 보인다는 것이 증명되었다. 하지만 모델 크기와 task와 관련한 보편성 문제로 여전히 fine-tuning을 대체하기에는 부족하다. 1) Lack of universality across scales 앞서 언급한 바와 같이 10B 정도 되는 크기의 model을 사용했을 때는 fine-tuning과 견주어 볼 만하지만 일반적으로 사용되는 medium-sized models을 사용했을 때는 fine-tuning보다 성능이 좋지 않다. 2) Lack of universality across tasks 몇몇 NLU benchmark에 대해서는 fine-tuning보다 우월하지만 hard sequence tagging tasks에 대한 성능은 확실하지 않다. input token들에 대해 sequence로 구성된 label을 verbalizer를 통해 예측하는 것은 어렵고 복잡하다. 본 논문의 실험에서도 sequence tagging tasks의 성능이 fine-tuning에 비해 좋지 않은 것을 이후 실험 결과에서 확인할 수 있다. 실험에 앞서 본 논문에서는 deep prompt tuning 개념을 차용한 P-tuning v2를 제안한다. 3.2. Deep Prompt Tuning',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': 'Lester et al., 2021과 Liu et al., 2021에서의 continuous prompts는 input embedding sequence에만 들어갔다. 이 방법론에는 두 가지 문제점이 있다. 1) sequence length로 인해 제한된다. 2) input embedding이 model 추론에 직접적인 영향을 주지 않는다. P-tuning v2에서는 위와 같은 문제를 해결하기 위해 deep prompt tuning의 개념을 끌어왔다. Figure 2에 표현된 것과 같이 다른 layer에 있는 prompt들은 prefix token처럼 추가했고 더 깊은 layer에 prompt를 추가하여 model 추론에 직접적인 영향을 주도록 하였다.',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': '3.3. Optimization and Implementation 최고의 성능을 내기 위한 optimization과 implementation에 대한 유용한 디테일들이 있다. 1) Reparameterization 기존 연구들은 trainable embedding으로 변환하기 위해 MLP와 같은 reparameterization encoder를 사용한다. 하지만 NLU task에서 그게 큰 도움이 되지 않는 것을 발견했다. RTE, CoNLL04 등 몇몇 dataset에 대해서는 MLP가 좋은 영향을 미치지만 다른 task에서는 영향이 미미하거나 부정적인 것으로 확인 되었다. 2) Prompt Length prompt 길이는 P-Tuning v2에서 중요한 역할을 한다. NLU task마다 최적의 prompt 길이가 다르다. 일반적으로 simple classification task의 경우 prompt 길이가 짧을 때, hard sequence labeling task의 경우에는 prompt 길이가 길 때 성능이 좋다. 3) Multi-task Learning Multi-task learning은 각 task에 대해 fine-tuning을 하기 전, 여러 task에 대해 continuous prompt를 공유하여 최적화하는 것이다. P-tuning v2에서 Multi-task는 선택사항이다. 4) Classification Head verbalizer를 예측하기 위해 LM head를 사용하는 것은 prompt tuning에서 주로 사용되는 방식이다. 하지만 이것이 full-data setting에서는 불필요하고 sequence labeling에 대해서는 호환되지 않는다. 그래서 P-tuning v2에서는 BERT와 같이 randomly-initialized된 classification head를 token 맨 앞에 붙이는 방식을 채택하였다. Table1은 기존의 prompt tuning과 P-tuning v2의 major contribution을 정리한 것이다.',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': '4. Experiments 본 논문에서는 일반적으로 사용되는 pre-trained model과 NLU task들을 기반으로 실험을 진행하였다. 그리고 fine-tuning을 제외한 모든 방법론은 frozen language model backbone을 기반으로 수행된다. 1) NLU Tasks P-tuning v2의 NLU ability를 확인하기 위해 SuperGLUE를 사용하였다. 추가적으로 NER(named entity recognition), extractive Question Answering, semantic role labeling과 같은 sequence labeling tasks에 대해서도 실험을 진행했다. 2) Pre-trained Models 평가를 위한 사전 학습 모델로는 BERT-large, RoBERTa-large, DeBERTa-xlarge, GLM-xlarge/xxlarge를 사용하였다. 위 모델들은 모두 NLU task를 위한 bidirectional model들로, 300M-10B 사이의 size의 버전들이 존재한다. 3) Multitask Learning multi-task setting을 위해 각 task type에 속한 모든 trainset을 모두 합치고 continuous prompt를 공유하는 동안 별도의 linear classifier를 둔다. 4.1 P-tuning v2: Across Scales',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': 'Table 2는 model scale에 따른 P-tuning 성능을 보여준다. SuperGLUE에 대해 model scale이 작을수록 P-tuning의 결과가 좋지 않은 것을 볼 수 있다. 반면 P-tuning v2는 작은 모델 기반에서도 모든 task에서 fine-tuning과 유사한 성능을 보인다. RTE에 대해서는 fine-tuning보다 더 좋은 성능을 낸 것을 확인할 수 있다. 2B-10B정도의 큰 scale model(GLM)로 진행한 실험에서는 fine-tuning과 P-tuning의 격차가 좁아졌다. 이전 연구들에서도 10B scale에서는 prompt tuning과 fine-tuning이 견줄만한 성능을 보였기 때문에 놀랍지 않지만 여기에서 중요한 것은 P-tuning v2의 경우에는 모든 scale에 걸쳐 fine-tuning과 유사한 성능을 보인다는 것이다.',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': '4.2 P-tuning v2: Across Tasks Tbale3에서 P-tuning이 모든 task에 대해 fine-tuning과 유사한 성능을 내는 것을 확인할 수 있다. P-tuning은 특히 QA task에서 좋지 않은 성능을 보인다. 그리고 SQuAD2.0에는 unanswerable question이 포함되어 있기 때문에 single-layer prompt tuning인 P-tuning으로 풀기에는 까다로웠을 것으로 예상한다. P-Tuning v2에서는 Multi-taks learning을 통해 QA를 제외한 모든 task에서 눈에 띄는 성능 향상이 이루어졌다. 4.3 Ablation Study 본 논문에서 제안된 요소들이 어떤 영향을 미쳤는지 확인하기 위해 해당 요소가 적용된 것과 아닌 것에 대한 비교를 해 본다. 1) Verbalizer with LM head v.s. [CLS] label with linear head. verbalizer를 예측하기 위해 LM head linear head를사용하는 것이 기존 prompt tuning의 일반적인 요소였지만 P-tuning v2에서는 linear head를사용하는 것이 classification head로서 붙이는 방식 사용하였다. 비교해 보니 큰 차이는 없다. 2) Prompt depth P-tuning과 P-tuning v2의 주요한 차이는 multi-layer continuous prompt이다. 해당 요소의 명확한 영향력을 확인하기 위해 prompt에 k개의 layer를 추가하였다. transformer layer가 많을수록 좋더라 5. Conclusions 모든 model scale, task에 거쳐 p-tuning v2를 통해 prompt tuning이 fine-tuning과 비슷한 성능을 낼 수 있다. Reference',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': 'Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang. “P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks,”2022 × Search',\n",
       "  'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/'},\n",
       " {'text': '(LaTaX 수식 오류 수정 예정) 1. Introduction / Motivation GPT 계열의 모델들은 fine-tuning 방식으로 NLU task를 수행했을 때 bert계열의 모델들보다 좋은 성능을 보이지 않는다. 이는 GPT 계열의 일방향적 특성 때문이라고 알려져 있다. GPT3의 경우에는 적절하게 만든 handcrafted prompt를 통해 NLU task를 풀긴 하지만 best-performing prompt를 handcrafting으로 찾는 것은 사막에서 바늘 찾는 것과 같다. 그리고 pattern을 manual하게 조정하게 되면 pattern(prompt)에 따라 성능 차이가 크다는 문제도 존재한다. Table 1은 prompt에 따라 성능차이가 심한 것을 보여준다. 그리고 GPT3와 같이 거대한 모델의 치명적인 단점은 poor transferability이다. many-shot setting 하에서fine-tuning을 한다 해도 거대 모델들은 fine-tuning sample을 빠르게 담아내기에 모델 자체가 너무 커서 제대로 되지 않는다.',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': '이러한 이유로 prompt를 자동으로 생성하는 연구들이 진행되었다. 하지만 이들은 모두 이산적인 prompt를 생성하는데 neural network 자체가 continuous하기 때문에 discrete한 정보는 neural network에게 sub-optimal하다. 따라서 본 논문의 저자들은 P-tuning을 제안하며 연속적인 공간에서 prompt를 자동으로 생성하여 GPT계열 모델들의 NLU application 수행문제를 해결하고자 하였다. Discrete: token/문자로 이루어져 이산적인 상태 Continuous: token이 embedding되어 연속적으로 존재하는 상태',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': '본 논문에서는 P-tuning을 이용했을 때 GPT 계열의 모델도 비슷한 크기의 BERT 계열 모델만큼 NLU task를 잘 수행할 수 있다는 것을 보여준다. 그리고 BERT도 p-tuning을 했을 때 fine-tuning보다 성능이 향상되었다고 한다. BERT와 같이 MLM task를 학습한 모델도 prompt를 활용하여 task를 해결할 수 있는 방법을 제안한 PET, iPET의 경우 labeled data가 많이 필요한 fine-tuning의 문제점을 완화시킬 방법을 제안했지만 두 방법론의 경우에는 unlabeled data가 많이 필요하고, 앞서 언급한 GPT의 prompt 생성과 동일하게 pattern을 manual하게 생성 및 조정해야 한다는 문제와 생성한 prompt가 discrete하다는 문제가 있다. 따라서 BERT 계열의 prompt based learning에서도 p-tuning을 통해 성능 향상이 이루어진 것으로 보인다 2. Method: P-tuning 2.1. Architecture',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': 'Figure 2의 (a)는 AdaPrompt, AutoPrompt와 같이 P-tuning 이전에 선행된 Prompt searching 방식을 표현한 것인데 모두 LM의 loss를 reward로 삼아 prompt generator를 통해 이산적인 prompt를 생성한다. 앞서 말했듯이 이산적인 정보는 neural network에게 sub-optimal하다. P-tuning은 선행 연구들의 문제를 해결하기 위해 Figure 2의 (b)와 같이 bi-LSTM으로 구성된 prompt encoder가 LM loss를 받아 학습하여 연속적인 prompt를 생성한다. prompt encoder 학습 시에는 PLM의 전체 weight에 대해 parameter update(fine-tuning)를 하는 것이 아니라 continuous prompt embedding만 tuning한다. P-tuning 관련 notation은 아래와 같다. \\\\begin{matrix} \\\\mathcal{M}\\\\text{ : pre-trained language model} \\\\end{matrix} \\\\begin{matrix} \\\\text{x}{1:n}\\\\text{= }\\\\left{ \\\\text{x}{0}, \\\\text{x}{1},\\\\cdots, \\\\text{x}{n} \\\\right} \\\\text{: a sequence of discrete input tokens} \\\\end{matrix} \\\\begin{matrix} \\\\mathbf{e}\\\\in\\\\mathcal{M}\\\\text{: pretrained embedding layer} \\\\end{matrix} \\\\begin{matrix} \\\\left{ e(\\\\text{x}{0}), e(\\\\text{x}{1}),\\\\cdots, e(\\\\text{x}_{n}) \\\\right} \\\\text{: input embeddings mapped by }\\\\mathbf{e}\\\\in\\\\mathcal{M} \\\\end{matrix} input token을 PLM의 embedding layer에 넣어 input embedding값을 만든다. pre-training의 경우 $text{x}$는 unmasked tokens이고 $text{y}$는 \\\\text([MASK]) token일 것이고 sentence classification task에서 $text{x}$는 sentence token, $text{y}$는 \\\\text([CLS]) token일 것이다. \\\\begin{matrix} \\\\mathbf{p}\\\\text{: function for creating prompts } \\\\end{matrix} \\\\begin{matrix} \\\\mathbf{x}\\\\text{: context} \\\\end{matrix} \\\\begin{matrix} \\\\mathbf{y}\\\\text{: target} \\\\end{matrix} \\\\begin{matrix} \\\\mathit{T}\\\\text{: template} \\\\end{matrix}',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': 'prompt를 만드는 함수 $\\\\mathbf{p}$에 $\\\\mathbf{x}$와 $\\\\mathbf{y}$를 넣어 template을 만든다. 예를 들어 수도를 예측하는 task가 있다고 가정했을 때, “The capital of … is … .”라는 prompt에 “Britain”이라는 context와 “[MASK]”라는 target을 넣으면 template “The capital of Britain is [MASK].”이 만들어진다. \\\\begin{matrix} \\\\mathcal{V}\\\\text{: vocabulary of a language model }\\\\mathcal{M} \\\\end{matrix} \\\\begin{matrix} \\\\text{[}\\\\text{P}_{i}\\\\text{]}\\\\text{: i}^{th}\\\\text{prompt token in a template }\\\\mathit{T} \\\\end{matrix} \\\\begin{matrix} \\\\mathit{T}\\\\text{=}\\\\left{\\\\text{ [}\\\\text{P}{0:i}\\\\text{]}, \\\\mathbf{x}, \\\\text{[}\\\\text{P}{i+1:m}\\\\text{]}, \\\\mathbf{y}\\\\right} \\\\end{matrix} LM의 vocab을 $\\\\mathcal{V}$, template을 구성하는 prompt의 $\\\\text{i}$번째 token을 ${P}$라 하고 주어진 template이 위와 같을 때 discrete prompt search 방식의 경우 $\\\\text{[}\\\\text{P}_{i}\\\\text{]}\\\\in\\\\mathcal{V}\\u200b$이기 때문에(prompt를 구성하는 token이 LM vocab 안에 있기 때문에) discrete prompt template은 아래와 같이 표현될 수 있다.',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': 'P-tuning의 경우에는 prompt token \\\\(\\\\text{[}\\\\text{P}_{i}\\\\text{]}\\\\)에 해당하는 것들이 LM의 vocab에 한정되지 않는 trainable embedding tensor $\\\\text{h}_{i}\\\\text{(0}\\\\leq \\\\text{i}\\\\leq \\\\text{m)}$ pseudo token으로 교체된다. 이를 통해 LM의 vocab을 넘어 더 나은 continuous prompts를 찾을 수 있다.',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': '이런 식으로 template에 대한 embedding값을 뽑으면 그걸 LM에 넣어 mask token($\\\\mathbf{y}$)에 대해 LM loss를 계산한 후에 그걸로 prompt encoder를 update시킨다. 즉, LM으로 추론하고 그걸 기반으로 prompt encoder(bi-LSTM)을 학습시킨다. 이때 LM은 parameter update를 하지 않기 때문에 연산량과 메모리를 줄일 수 있다. LM이 이 과정에서 학습을 하게 될 경우 backpropagation을 위해 gradient를 저장해 두어야 하고 방향을 기억(retain_graph=True)해야 하는데 이것이 필요 없기 때문이다. 2.2. Optimization continuous prompt 학습에는 두 가지 문제점이 있다. 1) Discreteness LM의 original word embedding $\\\\mathbf{e}$는 이미 pre-training 이후에 highly discrete된 상태이다. $\\\\mathit{h}$를 random initialized시킨 이후 SGD로 최적화하게 되면 optimizer는 local minima에 빠지기 쉽다. 2) Association 직관적으로 생각해 봤을 때 prompt embedding $\\\\mathit{h}_{i}$의 value들은 독립적이지 않고 서로 의존적이어야 한다. 따라서 prompt embedding 값들을 서로 연결시킬 어떤 mechanism이 있어야 한다. 저자는 위 두 문제를 해결하기 위해 lite한 neural network로 구성된 Prompt encoder를 통해 $\\\\mathit{h}_{i}$를 sequence로 modeling하는 방안을 제안한다. 본 논문에서는 bi-LSTM과 함께 ReLU activated two-layer multilayer perceptron (MLP)이 사용되었다.',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': '3. Experiments 본 논문은 두 가지 NLU task benchmark에 대해 실험을 진행하였다: LAMA knowledge probing, SuperGlue. 3.1. Knowledge Probing LAMA knowledge probing은 LM이 real-world knowledge에 대해 얼마나 잘 학습했는지 확인하는 task이다. 실험 결과 MP(Manual prompt : PET, iPET etc), discrete prompt보다 p-tuning 성능이 더 좋다. Table 2의 오른쪽 표에서 P-tuning 부분에 표기된 붉은 글씨는 MP와의 차이이다. 그리고 MP+FT(Manual prompt augmented fine-tuning)보다도 p-tuning 성능이 높다. Megatron의 경우는 parameter가 너무 많아서 fine-tuning이 불가해 관련 결과가 없지만 MP보다 p-tuning 결과가 좋은 것을 확인할 수 있다. Table3는 bert size와 비슷한 gpt2 모델을 사용하여 SuperGLUE에 대해 평가를 진행한 결과이다. GPT2-base의 P-tuning 부분에 표기된 붉은 글씨는 bert base와의 차이이다. 위 결과를 보면 전반적으로 bert보다 GPT2-base가 좋은 것을 확인할 수 있는데 이는 NLU task에 bert가 gpt계열보다 좋은 성능을 보이는 일반적인 결과와는 다르다. 그리고 앞선 실험 결과와 마찬가지로 fine-tuning보다 p-tuning 성능이 좋다. Table 4는 bert-large와 gpt2-medium과의 비교이다. base 모델의 결과와 유사하다. Table 5는 few-shot setting하에 진행된 실험 결과이다. 여기에서도 p-tuning 결과가 좋다. + P-tuning v2도 나왔다. Reference',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': 'Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang. “GPT Understands, Too,”2021 × Search',\n",
       "  'source_title': 'Prompt-Based Learning 2 | P-tuniung: GPT Understands, Too\\n      21 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/21/Ptuning/'},\n",
       " {'text': '(LaTaX 수식 오류 수정 예정) in-context learning을 통해 LM을 학습시키는 것만으로 fine-tuning 없이 downstream task를 풀 수 있는 방법론이 GPT2를 통해 처음으로 제안되었다. 이 방법론을 기반으로 GPT3는 few-shot setting으로도 다양한 task에 대해 fine-tuning 방법론보다 성능이 좋을 수 있다는 것을 증명하였다. 하지만 GPT와 같이 큰 모델을 학습시킬 수 없기 때문에 GPT보다 상대적으로 작은 bert계열의 LM 뒤에 특정 task를 수행할 layer를 추가하여 fine-tuning하는 방식이 많이 사용되는데, fine-tuning 학습을 위한 labeled data를 확보하기 위해서는 많은 비용이 든다는 문제가 있다. 이에 따라 in-context learning을 활용하여 Pre-trained LM을 효과적으로 사용하는 다양한 연구들이 진행되었는데, 그 중 최근 많은 관심을 받고 있는 연구가 Prompt based learning이다. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference 1. Pattern-Exploiting Training(PET)',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': '본 논문에서는 Prompt based learning 기법 중 하나인 PET(Pattern-Exploiting Training)를 소개한다. PET은 BERT 계열의 모델들이 prompt를 활용하여 task를 푸는 방법론이다. 이 방법론은 in-context learning의 개념을 차용해서 Language Model이 특정 task를 수행할 수 있도록 PVP(Pattern Verbalizer Pair)를 사용하여 Pre-training Task와 동일한 형태, 즉 cloze-style phrase 형태로 문제(input)를 재정의하는(e.g. “the correct answer is __”와 같은 cloze question 붙임) 것이다.',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': 'PET와 유사한 접근법으로는 LAMA가 있다. Figure 1은 sentiment classification에 적용한 것이다. Pattern-Exploiting Training을 위해서는 PVP(pattern-verbalizer pair)가 필요하다. PVP notaion은 아래와 같다. \\\\begin{matrix} \\\\text{M : Masked Language Model}\\\\ \\\\end{matrix} \\\\begin{matrix} \\\\text{V : Vocabulary} \\\\end{matrix} \\\\begin{matrix} \\\\underbar{ }\\\\underbar{ }\\\\underbar{ }\\\\underbar{ }\\\\in\\\\text{V : Mask Token} \\\\end{matrix} \\\\begin{matrix} \\\\text{A : target classification task} \\\\end{matrix} \\\\begin{matrix} \\\\mathcal{L}\\\\text{: a set of labels for our target classification task A} \\\\end{matrix} \\\\begin{matrix} \\\\textbf{x}\\\\text{= (}\\\\text{s}{1}\\\\cdots\\\\text{s}{k}\\\\text{) : input sequence for task A} \\\\end{matrix} \\\\begin{matrix} \\\\text{s}_{1}\\\\in\\\\text{V}^{*} \\\\end{matrix} \\\\begin{matrix} \\\\textbf{P}\\\\text{: pattern} \\\\end{matrix} \\\\begin{matrix} \\\\text{P(}\\\\mathrm{x}\\\\text{)}\\\\in\\\\text{V}^{*}\\\\text{: takes }\\\\mathrm{x}\\\\text{ as input and outputs a phrase or sentence} \\\\end{matrix} \\\\begin{matrix} \\\\mathit{v}\\\\text{ : verbalizer} \\\\end{matrix} \\\\begin{matrix} \\\\mathcal{L}\\\\rightarrow \\\\text{V : verbalizer maps each label to a word from {M}’s vocabulary} \\\\end{matrix} \\\\begin{matrix} \\\\text{(P,}\\\\mathit{v}\\\\text{) : pattern-verbalizer pair}\\\\mathbf{(PVP)} \\\\end{matrix} 위 수식을 보면 A는 target classification task, x는 task A에 대한 input이고 그 input은 phrase s로 구성되어 있다. 그리고 phrase s를 이용하여 문제를 변형하는 것을 Pattern이라고 한다. 본 논문에서는 pattern을 manual하게 만들어내지만 pattern을 모델을 통해 생성해 내는 연구들도 이후 많이 진행되었다. 위와 같이 input phrase a와 b 자체는 유지하면서 mask를 적절히 삽입하여 pattern을 만든다. 아래는 그 예시이다.',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': '그 다음으로 M은 Language Model, L은 classification task의 label, V는 M의 vocab이다. L(label)을 M(LM)의 V(vocab)내에 있는 자연스러운 token으로 대체하는 mapping function을 $\\\\mathit{v}$(verbalizer)라고 한다. task $\\\\mathit{A}$를 풀기 위해 pattern에 masking된 부분에 들어갈 자연스러운 token을 예측하는 방식으로 task를 변형시키게 되는데 이때 $\\\\text{P}$(pattern)와 v(verbalizer)는 서로 종속되기 때문에 pattern-verbalizer pair, 즉 PVP$\\\\text{(P,}\\\\mathit{v}\\\\text{)}$라고 한다. 아래 그림은 PVP를 포함하여 prompt based learning 이해를 위해 알아야 할 용어들을 시각적으로표현한 것이다. 1.1 PVP Training and Inference PVP를 활용한 Inference는 MLM을 통해 전체 vocab에 대한 logit을 구한 이후 verbalizer를 통해 선정한 특정 token에 대해서만 softmax를 수행하는 방식으로 이루어진다. 이 과정을 수식적으로 설명하도록 하겠다. MLM이 masked position에 할당한 token을 $\\\\mathit{w}$ 그리고 mask token을 포함한 sequnece를 $\\\\mathit{z}$라 하고 MLM에 넣은 후 나온 결과를 $\\\\text{M(}\\\\mathit{w}\\\\mid \\\\text{x)}$ 이렇게 표현했을 때, PVP 변형한 문제를 MLM에 넣은 후 나온 logit을 수식화하면 아래와 같다. 그리고 이걸 softmax 함수에 넣은 것은 아래 수식이다.',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': '1.2 Auxiliary Language Modeling 이 방법론은 PVP에 대해 PLM을 finetune하는 것이라 볼 수 있다, 즉 MLM에 대해 추가 학습을 수행하는 것이라 할 수 있다. 그렇기 때문에 catastrophic forgetting이 발생할 수 있는데 저자는 language modeling을 auxiliary task로 활용함으로써 해결하였다. 이를 수식화 하면 아래와 같다. \\\\(\\\\text{L}_{ce}\\\\) 는 cross-entropy loss로, pattern exploit training에 대한 loss이고, $\\\\text{L}_{MLM}$은 기존 LM이 masked language modeling task에 대해 학습한 loss이다. 기존에 학습된 MLM을 추론에 바로 사용할 수도 있지만 추가적인 학습을 진행할 때에는auxiliary LM loss를 부가하여 사용하면 좋다고 한다. 저자는 실험을 통해 $\\\\alpha=\\\\text{10}^{-4}$일 때 좋은 성능을 보이는 것을 발견했다고 한다. 지금까지 설명한 개념들을 정리하면 아래와 같이 표현할 수 있다. 1.3 Combining PVPs 여러 개의 PVP 학습 결과를 combining하여 사용하는 것이 pattern exploit training이다. PVP를 활용한 접근에서 중요한 것은 어떤 PVP가 해당 task에 잘 맞는지 알아내는 건데, 이 문제를 해결하기 위해 본 논문에서는 knowledge distillation과 유사한 전략을 사용하였다.',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': 'knowledge distillation : 미리 잘 학습된 큰 모델(Teacher network)의 지식을 증류하여 실제로 사용하고자 하는 작은 모델(Student network)로 transfer하는 것 task A에 대해 여러 PVP를 사용하는 과정을 살펴보겠다. \\\\begin{matrix} \\\\mathcal{P}\\\\text{: set of PVP} \\\\end{matrix} \\\\begin{matrix} \\\\text{M}_{p}\\\\text{: Language Model} \\\\end{matrix} \\\\begin{matrix} \\\\mathcal{M}\\\\text{: ensemble of finetuned model} \\\\end{matrix} \\\\begin{matrix} \\\\mathcal{M}\\\\text{= M}_{p}\\\\mid \\\\mathbf{p}\\\\in\\\\mathcal{P} \\\\end{matrix} $\\\\text{M}_{p}$(LM)에 대해 $\\\\mathbf{p}$(pattern)을 받아서 $\\\\mathcal{M}$(ensemble of finetuned model)을 만든다. 이때 2.1에서 언급한 logit에 가중치$\\\\mathit{w}\\\\mathbf{(p)}$를 줘서 새로운 logit을 계산한다. (가중치는 1을 주어 가중평균을 낼 수도 있고 train 이전의 acc를 쓸 수도 있다고 한다.) 일반적으로 ensemble할 때 새로운 logit을 만들기 위해 voting방식을 사용하는데 여기에서는 가중치를 줘서 새로운 logit을 만들었다. 아래 그림은 pattern exploiting training process를 도식화한 것이다.',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': '\\\\begin{matrix} \\\\tau \\\\text{: training dataset} \\\\end{matrix} \\\\begin{matrix} \\\\mathcal{D}\\\\text{: unlabeled data} \\\\end{matrix} (1) trainset($\\\\tau$ : PVP형식으로 변형한 labeled classification dataset)를 PLM($\\\\text{M}$)에 넣어 학습시킨다.(Auxiliary Language Modeling) (2) 각 모델들을 ensemble하고, unlabeled data($\\\\mathcal{D}$)을 넣어 추론을 해서 soft-labeled dataset($\\\\tau_{c}$)로 만든다. (3) soft-labeled dataset($\\\\tau_{c}$)을 $\\\\mathit{C}$(classifier)에 넣어 학습시킨다. soft label vs hard label soft label은 확률을 사용하여 분류된 것으로, 하나의 데이터가 여러 클래스에 해당할 수 있다. hard label은 하나의 데이터가 하나의 클래스로 분류되는 것이다. 1.4 Iterative PET (iPET)',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': '앞서 소개한 모든 개별 모델을 distilling을 하여 single classifier $\\\\mathit{C}$로 knowledge를 넘기는 것은 각 모델들이 서로에 대해 모른다는 것을 의미한다. 어떤 pattern이 다른 pattern보다 좋지 않은지 모르기 때문에 final model인 $\\\\mathit{C}$에는 많은 mislabeled examples가 포함된다. 이러한 문제를 해결하기 위해 서로 다른 pattern에 대해 학습한 독립적인 LM($\\\\text{M}$)들이 상호작용할 수 있도록 앞서 설명한 pattern exploiting training process를 반복하는 iPET이 제안되었다. iPET은 trainset($\\\\tau$)을 PLM($\\\\text{M}$)에 넣어 학습시키고 unlabeled data($\\\\mathcal{D}$)에 대해 추론을 하면 거기에 labeling이 된 새로운 data($\\\\tau$)가 만들어지는 것을 반복한다. 2. Experiments and Result 1 본 연구의 실험은 RoBERTa large를 기반으로 다양한 dataset에 대해 실험을 진행하였다. 아래는 실험 결과이다. PET와 iPET가 대부분 좋은 성능을 내는 것을 확인할 수 있다. 아래는 data augmentation에 의존하는 semi-supervised learning 중 sota를 기록한 UDA와 MixText와 비교한 결과이다.',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': 'It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners 3. PET with Multiple Masks PET 방법론의 한계점 중 하나는 verbalizer($\\\\mathit{v}$)가 single token에 대해서만 mapping된다는 것이다. 이는 여러 task를 수행할 수 없게 한다. 즉, 정답이 single token이지 않을 수 있다. 예를 들어 아래 예시처럼 vocab에 “terrible”이 “terri##”+”##ble”로 있을 수도 있다. 그래서 이러한 경우를 위해 mask를 여러 개 마련해 놓고 확률값이 높은 순서대로 mask를 채워 나가 token을 완성하는 방법을 제안하였다. 4. Experiments and Result 2 본 논문의 실험은 PET의 성능을 GPT3와 비교하기 위해 ALBERT-xxlarge-v2를 기반으로 다양한 SuperGLU 대해 진행되었다. few shot setting하에(data 32개) Figure 1과 같이 parameter 수가 월등히 많은 GPT3보다 대체적으로 좋은 성능을 보였다. Reference Timo Schick, Hinrich Schütze. “Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference,”EACL2021 Timo Schick, Hinrich Schütze. “It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners,”NAACL2021 https://github.com/timoschick/pet#-train-your-own-pet Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. “Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing,”(https://arxiv.org/pdf/2107.13586v1.pdf) × Search',\n",
       "  'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/'},\n",
       " {'text': '최근 자연어처리 분야에서는 대용량의 corpus를 학습한 LM 뒤에 downstream task를 위한 layer를 붙여 특정 task를 풀 수 있도록 하는 방법론이 많이 사용된다. Language Model(LM)은 대용량의 unlabeled data를 학습하여 언어 자체에 대한 feature를 학습한 모델이다. 최근 LM의 학습에 사용되는 task는 크게 masked language modeling과 language modeling이 있다. 전자의 경우는 입력된 text의 일부 token에 대해 masking처리를 한 후, 앞뒤 문맥을 통해 masking token의 original token을 예측하는 task로, bert계열의 model들이 이러한 방식으로 학습된다. 후자는 token들을 하나씩 집어넣어서 첫 번째 token을 기반으로 다음 token을 예측하고 그 token을 가지고 다음 token을 예측하는 것을 sequence가 끝날 때까지 반복하는 방식으로 학습하는 task이다. 이 방법론은 gpt계열의 model에서 주로 사용된다. 그리고 bert와 gpt 계열 모델의 구조는 대부분 각각 transformer의 encoder와 decoder 기반으로 형성되어 있다. 따라서 bert 계열에서는 self-attention을, gpt 계열에서는 masked-self-attention을 사용한다. Unidirectional Language Model(Auto-regressive model)',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': '다음 token이 뭔지 쭉쭉 맞추는 방식으로 학습하니까 일방향적 LM. 보통 Auto-regressive model이라고 한다. 이전 time step의 출력이 다음 time step의 입력으로 들어가는 점이 자기 회기적이기 때문에 이렇게 부른다. 여기에 속하는 모델들은 transformer의 decoder 구조를 기반으로 하기 때문에 masked-self-attention을 사용한다. masked-self-attentiondms 해당 time step의 오른쪽에 있는 단어들은 masking해 놓은 상태에서 attention을 계산하여 다음 단어를 예측하기 때문에 생성 과제에 강점을 보인다. GPT : Improving Language Understanding by Generative Pre-Training transformer의 decoder 구조를 기반으로 한 모델. language modeling task로 학습. 이 task는 sequence를 이루는 token들을 차례로 하나씩 예측하기 때문에 이 학습 방식을 autoregressive하다고 표현함. GPT2 : Language Models are Unsupervised Multitask Learners GPT2에서는 in-context learning을 처음으로 제안하였다. in-context learning은 문맥 내에서 풀고자 하는 task를 학습하는 것을 의미하는데, language modeling task의 trainset으로 Table 1과 같은 data를 사용하면 translation이 가능한 LM을 만들 수 있다는 개념이다. 이는 앞선 sequence를 기반으로 다음 sequence를 예측하는 language modeling objective의 특성을 통해 프랑스어로 번역하라는 task description 추가하여 번역과제를 수행하는 mechanism으로 이해할 수 있다.',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': '이 방법론을 통해 fine-tuning과정으로 downstream data에 대한 parameter를 따로 update하지 않고도 task에 대한 정답을 추론할 수 있는 zero-shot inference가 가능하다는 것을 밝혔다. zero-shot inference는 task description을 준 이후 prompt를 던져주고 정답을 맞추라고 하는 것이다. Scaling Laws for Neural Language Models OpenAI는 Jared et al. 2020(Scaling Laws for Neural Language Models)를 통해 model size, data size 그리고 연산량을 증가시키면 LM의 performance는 계속 향상한다는 것을 실험적으로 밝혀냈다. 하지만 이 각각은 모두 bottleneck이 되지 않는 적정 정도 안에서 증량하는 것이 중요하다. Model size나 연산량만 커지면 overfitting, data size만 커지면 underfitting이 발생하기 때문이다.',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': 'GPT3 : Language Models are Few-Shot Learners GPT3는 기존의 GPT모델 사이즈를 엄청 키운 것이다. layer수는 GPT2보다 2배 더 많고 총 parameter 수는 175B이다. Jared et al. 2020(Scaling Laws for Neural Language Models)를 통해 언어 모델의 scaling law가 실험적으로 입증된 이후 본격적으로 거대 언어 모델들이 등장하는데 그 대표적인 모델이 GPT3이다. 모델의 구조는 기존 GPT에서 layer normalization이 각 블록 뒤에 있던 게 앞에 위치한 것 외에는 모두 동일한데 size를 많이 키웠다. $\\\\text{n}_\\\\text{params}$ = $175B$ $\\\\text{n}_\\\\text{layers}$ = $96$ $\\\\text{d}_\\\\text{model}$ = $12288$ $\\\\text{n}_\\\\text{heads}$ = $96$ $\\\\text{d}_\\\\text{head}$ = $128$ 전체 파라미터 수가 175B이기 때문에 이걸 사용하려면 billion을 대략 gigabyte로 보고 4byte(32 bit) 이상의 tensor를 사용한다고 가정하면 gpu 700GB 이상이 필요하다 모델이 커서 fine-tuning은 힘들지만 GPT3 논문에서는 GPT2에서 소개된 zero shot 외에도 task description과 input 사이에 downstream task에 대한 정보를 예제 형식으로 주는 one shot, few shot learning을 소개하며 fine-tuning이 필요 없다는 것을 보여줬다.',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': 'zero shot, one shot, few shot 모두 task description만 주고 prompt의 정답 값을 예측하는 것인데 zero shot은 예시를 안 주는 거고 one shot은 예시를 하나 주는 거, few shot은 예시를 k개 주는 거다. 이게 가능한 이유는 in-context learning에 있다고 한다. 즉, in-context learning을 통해 LM이 task를 풀 능력을 학습했다는 전제 하에 위와 같은 zero, one, few shot이 가능한 것이라고 한다. T5 : Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Encoder와 Decoder로 구성된 모델. Transformer의 구조에서 약간 변형만 준 모델. Layer Mormalization 시 bias는 주지 않고 rescale만 진행 Transformer는 sinusoidal position encoding 대신 relative positional embedding을 적용(BERT의 경우 Absolute positional embedding을 사용하였는데, 최근 모델들은 NSP를 제거하는 추세이기 때문에 absolute positional embedding으로 chunk strat position에 의존적인 absolute positional embedding은 적합하지 않아 relative positional embedding을 사용하는 모델이 늘어나는 중이라고 함: Improve Transformer Models with Better Relative Position Embeddings) Model layer 전체에서 position embedding parameter를 sharing Relative positional embedding Relative positional embedding : self attention 수행 시 offset boundary 내의 token에 대해 relative position 값을 부여하는 것. 예를 들어 offset=4, time step=3인 경우 relative position 값이 아래 그림 같이 부여되고 offset=2,time step=3인 경우에는 아래와 같이 된다. 범위를 넘어갈 경우 가장 바깥쪽 index의 position값을 동일하게 부여한다.',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': 'Relative positional embedding에 대한 자세한 내용은 https://medium.com/@init/how-self-attention-with-relative-position-representations-works-28173b8c245a 여기에 잘 정리되어 있다. def relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128): ret = 0 n = -relative_position if bidirectional: num_buckets //= 2 ret += (n < 0).to(torch.long) * num_buckets n = torch.abs(n) else: n = torch.max(n, torch.zeros_like(n)) # now n is in the range (0, inf) # half of the buckets are for exact increments in positions max_exact = num_buckets // 2 is_small = n < max_exact # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance val_if_large = max_exact + ( torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact) ).to(torch.long) val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1)) ret += torch.where(is_small, n, val_if_large) return ret T5는 text-to-text framework를 기반으로 pre-training과 fine-tuning을 수행한다. T5의 text-to-text는 GPT3의 task description-prompt와 유사하다. input sentence 뒤에 task description이 prefix로 붙어 이것이 하나의 input text로 모델에 들어가 task의 정답을 text로 내놓는 것이다. input : task description(task 정보) + input sentence output : text T5 task 처리 방식',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': 'Classification task : input으로 분류하고자 하는 문장, output으로는 분류 label. (label은 model vocab내에 있는 토큰 중 하나로 추론되기 때문에 label 목록 외의 것이 나올 경우에는 틀린 것으로 간주) Regression task : STS-B(semantic textual similarity : 텍스트 의미적 유사도 예측 과제)와 같은 regression task의 경우 특정 단위로 나누어서 그걸 라벨로 취급하여 classification task처럼 처리 (e.g. 1-5사이 스코어 추론 과제: 0.2 단위로 1, 1.2, 1.4, 1.6, …으로 나눔)',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': 'T5에 대한 자세한 내용은 논문을 보는 게 좋고 간략한 내용은 Google Research Blog를 보는 것이 좋다. Closed-Book Question Answering demo applicaion도 체험할 수 있다 Bidirectinoal Language Model mask token에 들어갈 token을 앞뒤 token을 기반으로 예측하는 방식으로 학습하니까 양방향 LM 여기에 속하는 모델들은 transformer의 encoder 구조를 기반으로 하기 때문에 self-attention을 사용한다. self-attentiondms 해당 time step의 양 옆의 단어들을 모두 고려하여 attention을 계산하기 때문에 gpt계열보다 sequence의 맥락 정보를 더 잘 학습한다. BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding transformer의 encoder구조를 기반으로 만들어진 모델. mlm과 nsp task로 학습됨. mlm : input문장의 특정 token에 mask를 취하고 mask token에 위치할 실제 token을 예측하는 task nsp: 문장 두 개를 입력 받아 두 문장이 뒷 문장이 앞 문장과 이어지는 문장인지 아닌지 예측하는 task bert 계열의 모델들은 위와 같은 task를 통해 사전학습을 충분히 거친 이후 task specific한 head를 붙이는 방식으로 downstream task를 푼다.(e.g. classification task: classification head / qa: span prediction head) RoBERTa : A Robustly Optimized BERT Pretraining Approach Roberta는 BERT계열의 모델이다. BERT와 구조는 거의 같은데 BERT가 under training되었다고 주장하며 이를 해결하기 위한 네 가지 방법을 제안했다.',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': '1) 더 많은 데이터를 더 큰 batch로 더 오래 학습시킨다. 2) sequence length를 늘려 학습한다. 3) BERT가 수행하는 task(MLM, NSP) 중 MLM만을 사용하여 학습한다. 4) MLM 수행 시 masking pattern을 매 epoch마다 다르게 준다. ELECTRA : Efficiently Learning an Encoder that Classifies Token Replacements Accurately Transformer의 Encoder를 기반으로 한 모델로, 기존 Language Model에서 많이 사용된 MLM이 아닌 replaced token detection task를 적용하여 연산량을 줄임으로써 효율을 높인 Language Model 기존 LM에 많이 사용된 MLM보다 효율적인 방식인 replaced token detection task를 제안한 모델로, 이 방식은 연산량이 적은 것이 특징이다. 해당 task는 example당 token의 15%만 학습하는 bert의 mlm과 달리 일부 token을 다른 token으로 교체하여 모든 token에 대해 original token인지 replacement token인지 예측하는 방식으로 학습한다. 이러한 학습 방식은 모든 input token에 대해 학습을 하기 때문에 연산 효율이 높다. 모델의 구조는 아래와 같다:',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': 'ELECTRA모델은 generator와 discriminator로 구성되어 있다. discriminator에서는 token이 generator에서 다른 token으로 대체되었는지를 예측한다. generator와 discriminator의 크기가 동일하다면 weight sharing이 가능하다. Hybrid Language Model XLNet × Search',\n",
       "  'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/'},\n",
       " {'text': '1. Introduction 본 논문은 특정 task 혹은 domain에 대해 맞춤 학습을 수행한 LM(Language Model)이 더 좋은 성능을 보일 것이라는 전제를 기반으로 하며, 이를 다양한 실험을 통해 입증하였다. 최근 자연어처리 분야에서는 LM을 기반으로 downstream task에 대해 fine-tuning을 진행하는 방식이 많이 사용된다. 즉, LM에서 학습된 representation을 기반으로 해당 모델 뒤에 특정 task를 수행할 layer를 추가하여 학습하는 방식이 많이 사용된다. 수행하고자 하는 task가 LM을 기반으로 하는 만큼 fine-tuning model의 성능은 LM의 영향을 받게 된다. 이는 곧 LM의 data distribution이 downstream task 결과에도 영향을 준다는 의미로 이어질 수 있다. 오늘날 LM은 일반적으로 백과사전, 뉴스 기사 등 웹 크롤링 데이터를 학습한다. 1.1 Data distribution Figure 1은 data distribution을 그림으로 표현한 것이다.',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': 'Figure 1 중앙에 위치한 짙은 색의 타원 부분은 실제 풀고자 하는 task의 distribution을 나타내고, 연한 회색 부분의 경우에는 해당 task와 관련된 data의 분포를 나타낸다. 양 옆으로 넓게 퍼진 영역 중 오른쪽에 위치한 주황색 부분은 original LM의 분포이고 왼쪽의 파란색 부분은 task data가 속한 domain 전반에 대한 data distribution을 나타낸다. task distribution은 original LM과 target domain distribution 영역에 걸쳐 있는 것을 확인할 수 있다. 따라서 본 논문은 특정 task 혹은 domain-specific unlabeled data에 대해 추가 학습을 한 LM을 기반으로 fine-tuning을 했을 때 더 좋은 성능을 보일 것이라는 전제 하에 다양한 실험을 진행하였다. 여기에서 진행한 실험들은 모두 Roberta를 기반으로 진행된다. 1.2 Related studies',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': '앞서 본 논문은 target task 혹은 domain data를 통해 추가 학습시킨 LM이 downstream task 성능 향상에 영향을 준다는 전제를 가지고 있다고 언급한 바 있다. 이러한 전제는 특정 domain을 위한 pre-train 모델을 생성하여 좋은 성능을 보인 사례들을 기반으로 성립된 것으로 보인다. 해당되는 사례에는 LEGAL-BERT, Bio-BERT, SCI-BERT 등이 있다. 본 논문에서는 Bio-BERT를 언급하며 continued pretraining 기법을 통해 좋은 성능을 보이긴 했지만 해당 기법이 labeled task data의 양 그리고 target domain과 original pretraining corpus의 유사도에 얼마나 영향을 받는지에 대한 설명이 부족하다고 지적했다. 1.3 Experiments setting 본 논문에서 진행된 실험은 domain-specific한 large corpora 그리고 task와 직접적으로 관련된 unlabeled data를 사용하여 진행된다. 전자를 사용하여 진행한 continued pretraining 방법론을 DAPT(domain-adaptive pretraining)라 칭하며, 후자를 사용하여 진행한 continued pretraining 방법론은 TAPT(task-adaptive pretraining)라고 한다.',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': 'LM의 추가학습이 downstream task에 미치는 영향과 그 이유를 분석하기 위한 다양한 실험들이 진행된다. 우선 4 가지 domain(biomedical, computer science publications, news , reviews)을 기반으로 각 domain마다 두 개의 classification task, 총 8개의 task에 대한 실험을 진행했으며, low resource와 high resource setting에서의 실험도 진행했다. 위 4가지 domain을 선정한 이유는 각 domain마다 text classification data가 있어서 DAPT와 TAPT에 대한 성능 비교가 용이하기 때문이다. 또 다른 실험으로는 DAPT 및 TAPT 수행 시 타 domain이나 task에 대해서도 knowledge transfer 되는지에 대한 실험이 있다. 마지막으로 인간이 만든 dataset과 자동으로 선택된 unlabeled dataset으로 진행한 pretraining에 대한 실험도 진행했다. 2. Domain-Adaptive Pretraining Domain-Adaptive Pretraining실험에는 4개의 domain(biomedical, computer science publications, news , reviews)에 대해 각각 biomedical (BIOMED) papers, computer science (CS) papers, newstext from REALNEWS 그리고 AMAZON reviews data가 사용되었다. 2.1 Analyzing Domain Similarity DAPT를 수행하기 전, 앞으로 진행할 실험 결과에 대한 명확한 분석을 위해 각 domain data와 Roberta가 학습한 data(original LM data)의 유사도를 확인하였다. Figure 2는 각 domain vocab에서 stopword를 제외하고 가장 많이 등장한 10000개의 단어에 대한 유사도를 측정한 결과이다.',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': '위 결과를 보면 Roberta pretraining domain은 news와 review domain과의 유사도가 높은 반면 CS와 BioMed에 대해서는 유사도가 낮은 것을 확인할 수 있다. original LM data domain과의 유사도가 낮을수록 DAPT를 통한 성능 향상 가능성이 높다고 볼 수 있다. 2.2 Experiments Table 1은 각 domain에 대해 추가 학습을 진행 한 결과를 보여준다. 본 논문의 실험에 사용된 back born model은 160G data를 학습한 Roberta base이며, 각 domain에 대해 해당 모델을 12.5k step씩 추가 학습시켰다. 학습 결과를 보면 news domain를 제외한 다른 domain에서 DAPT 수행 후 MLM loss가 감소한 것을 확인할 수 있다. 이전에 분석한 domain 유사도 분석 결과를 바탕으로 실험 결과를 분석해 보면 original LM과의 유사도가 낮을수록 DAPT 적용 후 MLM loss 감소율이 더 큰 것을 확인할 수 있다. 앞서 언급한 바와 같이 추가학습한 모델을 기반으로 각 domain마다 두 가지 classification task에 대한 fine-tuning을 진행하였다. 아래 Table 2로 downstream task dataset 정보를 확인할 수 있다.',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': '2.3 Domain Relevance for DAPT downstream task 결과를 통해 domain relevance가 DAPT에 어떤 영향을 주는지 확인하는 실험을 진행하였다. Tabel 3에 나타난 결과에서 CHEMPROT와 RCT의 평가 metrics은 micro-f1 score이고 나머지는 mecro-f1 score이다. 평가지표를 다르게 적용한 이유는 data의 class imbalance 문제 때문으로 보인다. mecro f1 score의 경우에는 class 균형을 고려하지 않고 결과를 산출하는 평가지표로, 이를 적용한 데이터는 class imbalance 문제가 심하지 않아 이를 적용한 것으로 생각된다. DAPT를 수행한 이후 모든 결과들에 대해 성능이 향상된 것을 확인할 수 있다. 하지만 이러한 성능 향상이 데이터 증가로 인한 것인지 확인하기 위해 해당 표에는 관련 없는 domain data로 수행한 DAPT의 결과도 제시되어 있다. 관련 없는 domain data로 수행한 DAPT의 결과는 CS domain을 제외하고는 모두 성능이 감소되어 domain과 관련된 data가 DAPT로 인한 성능 향상에 중요한 역할을 한다는 것이 증명되었다.',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': '2.4 Domain Overlap 각 domain이 공유하는 data boundary 파악을 위한 정성적 실험을 진행하였다. Figure 2를 보면 Review와 News domain의 data 유사도는 40%이다. 이 두 domain에 대한 domain overlap실험 결과가 Table 4에 정리되어 있다. 해당 table에서 주황색으로 강조된 것들이 overlap된 부분이다. 이 실험을 통해 domain이 다른 data 간에도 겹치는 부분이 있다는 것을 알 수 있다. 3. Task-Adaptive Pretraining 앞서 설명한 DAPT와는 달리 TAPT(Task-Adaptive Pretraining)는 task와 관련된 unlabeled data에 대해 추가학습을 진행하는 것이다. task 관련 범위는 domain의 범위보다 작기 때문에 상대적으로 적은 data로 학습은 진행할 수 있다. Table 5는 original Roberta와 DAPT수행, TAPT수행, DAPT+TAPT 수행 이후 진행한 downstream task 결과이다. TAPT 수행 결과, 우선 baseline보다는 모두 향상했고, PAT보다 적은 data를 사용했음에도 RCT, HYPERPARTISAN, AGNEWS, HELPFULNESS 그리고 IMDB에 있어DAPT보다 높은 성능을 보인 것을 확인할 수 있다. 3.1 Combined DAPT and TAPT DAPT+TAPT는 DAPT 적용 이후 TAPT까지 추가적으로 수행한 것으로, 이 방법론이 모든 task에 있어 가장 좋은 성능을 보였지만 catastrophic forgetting에 대한 우려가 있다고 언급하며 이에 대한 연구를 future work로 남겨 두었다. 3.2 Cross-Task Transfer',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': '또한 같은 domain에 속한 다른 task간의 knowledge transfer가 되는지 확인한 실험도 진행하였다. 예를 들어 RCT unlabeled data로 학습한 LM(RCT에 대한 TAPT)으로 CHEMPROT labeled data에 대한 fine-tuning을 진행하는 방식의 실험인데 이러한 방법론을 본 논문에서 Transfer-TAPT라고 칭한다. Table 6에 해당 실험에 대한 결과가 정리되어 있다. 전반적으로 성능이 좋지 않은데, 이는 동일한 domain에 속해도 각 task의 data distribution이 다를 수 있다는 것을 보여준다. 더 나아가 이 실험의 결과를 통해 왜 domain 지식에 대한 adapting만으로는 충분하지 않은지, 그리고 왜 DAPT 이후에 TAPT를 수행해야 하는지도 설명할 수 있다. 4. Augmenting Training Data for Task-Adaptive Pretraining 더 많은 task distribution에 속한 unlabeled data가 더 많은 상황에 대한 결과를 확인하기 위한 실험도 진행하였다. task data augmentation 방식으로는 task와 관련된 unlabeled data를 사람이 선별하는 방식과 자동으로 고르는 방식이 제안되었다. 4.1 Human Curated-TAPT 명확한 source로부터 사람이 직접 다량의 unlabeled data를 수집하여 task training data distribution과 유사한 분포의 dataset을 생성하여 TAPT를 수행하는 것을 Human Curated-TAPT라 부른다. 1) Data (Table 2 참고)',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': 'RC-500: low-resource setting된 RCT data. 18040개의 RCT data 중 500개만 labeled data로 사용하고 나머지는 unlabeled data로 TAPT학습에 사용한다. (동일한 dataset에서 빼 온 것이기 때문에 동일한 task distribution에 속한다) HYPERPARTISAN: 5000개의 unlabeled data를 TAPT에 사용 IMDB: task annotator가 50000개의 unlabeled data를 직접 수집 2) Results',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': 'Table 7를 보면 Curated-TAPT, TAPT, DAPT+TAPT에 대한 실험 결과가 비교할 수 있다. Curated-TAPT 결과가 이전에 실험 결과가 가장 좋았던 DAPT+TAPT보다 좋게 나온 것을 확인할 수 있다. 그리고 DAPT이후 Curated-TAPT를수행한 결과가 모든 task에서 가장 높은 성능을 보였다. 이러한 결과는 task distribution에 속한 다량의 curating data가 end-task 성능에 좋은 영향을 미친다는 것을 시사한다. 4.2 Automated Data Selection for TAPT 두 번째 증강 방법은 다량의 in-domain corpus에서 task distribution과 연관된 unlabeled text를 찾아 다량의 unlabeled data를 자동으로 생성하는 방식이다. 조금 더 구체적으로 말하자면, embedding space 내에 task 와 domain이 공유하는 space에서 domain data 중 task-relevant data를 찾아 후보를 선별하여 data를 증강시키는 것이다. 이 실험에서 중요한 것은 다량의 문장을 embedding하기 위해 embedding 모델은 가벼우면서 빨라야 한다는 것이다. 저자는 이러한 조건을 충족시키는 모델인 lightweight bag-of-words language model인 VAMPIRE를 사용하여 (i) task sentence 근처에 있는 k개의 domain data 50/100/150개(kNN-TAPT), (ii) 랜덤하게 선별한 data 50개(RAND-TAPT)를 뽑았다.',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': 'Table 8에 나타난 실험 결과를 보면 RAND-TAPT의 성능이 가장 낮고, kNN-TAPT에서는 k가 클수록 높은 성능을 보이는 것을 확인할 수 있다. 4.3 Computational Requirements Table 9는 BIOMED domain의 RTC-500에 대해 지금까지 수행한 모든 adaptation technique의 computational requirements이다. Reference Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith.”Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,”ACL. 2020 × Search',\n",
       "  'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/'},\n",
       " {'text': 'BERT는 2018년 10월에 SQuAD에서 기존의 앙상블 모델들의 기록들을 앞지르며 등장한 언어 모델로, Human Performance를 능가하여 큰 관심을 받은 모델이다. 그리고 pre-trained된 BERT모델에 classification layer만 추가하면 다양한 자연어처리 과제를 수행할 수 있어 데이터셋 환경이 좋은 영어권에서는 11개의 NLP task에 대해 SotA(State of the Art)를 달성하였다. BERT는 Bi-directional Encoder Representations form Transformers의 약자로, 이름 그대로 이전 게시물에서 소개한 Transformer모델의 encoder부분만 사용한 모델이다. Transformer에 대한 자세한 설명은 이전 게시물에 있기 때문에 해당 게시물에서는 Transformer설명과 중첩되는 부분은 생략하고 지나가겠다. BERT의 학습은 pre-training step과 pre-trained model을 가지고 fine-tuning을 하는 step으로 나뉜다.',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': '첫 번째 학습 과정인 pre-training step에서는 semi-supervised방식으로 학습이 진행된다. 해당step에서는 unlabeled data인 BooksCorpus나 Wikipedia등의 대형 corpus를 이용하여 masking된 단어와 다음 문장을 예측하는 모델을 동시에 학습한다. 이렇게 특정 목적 없이 학습이 이루어지는 것을 Pre-training이라고 부른다. 보통 pre-training task는 보편적으로 $x_1$, $x_2$,…,$x_{i-1}$을 통해 $x_i$를 예측하는 language model을 학습한다. 하지만 BERT는 pre-training task로 language model이 아닌 masked language model을 학습한다. 이러한 특별한 pre-training을 위해 BERT의 input부터 다른 기존의 모델들과 차이를 보인다. 이는 이후 더 자세히 설명하도록 하겠다. 그리고 fine-tuning step에서는 supervised training방식으로 학습이 진행된다. 해당 step에서는 pre-trained model parameter를 사용하여 다른 down-stream task를 수행한다. Architecture of BERT 모델의 전체 구조는 Transformer의 Encoder부분과 동일하다. Transformer encoder block에 대한 설명은 이전 게시물에 자세히 서술되어 있다. Illustrated by the author',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'BERT에서는 위와 같은 transformer encoder layer를 N개(BERT-base는 12개, BERT-large는 24개) 사용한다. encoder layer를 여러 개 사용함으로써 단어들 간의 복잡한 관계를 더 잘 포착할 수 있는 결과를 얻을 수 있다. 여기에서 중요한 점은 각 layer들은 weight를 공유하지 않는다는 것이다. Encoder layer가 더 많아짐에 따라 encoder내부에 존재하는 attention head와 $d_{model}$에도 변화가 생긴다. 기존 Transformer 논문에서는 6개의 encoder layer를 사용하였고, 모델 내부에 흐르는 벡터의 크기($d_{model}$)은 512였으며 attention head는 8이었지만 BERT-base와 BERT-large의 attenetion layer는 각각 12개와 16개고, $d_{model}$은 각각 768과 1024이다. $\\\\text{BERT}_\\\\text{BASE}$: $(L=12, H=768, A=12, \\\\text{Total Parameters} = 110M)$ $\\\\text{BERT}_\\\\text{LARGE}$: $(L=24, H=1024, A=16, \\\\text{Total Parameters} = 340M)$ Layer들 간의 흐름은 다음 그림과 같이 이전 layer의 output이 다음 layer의 input으로 들어가는 구조를 가진다: Illustrated by the author Input과 Output의 크기는 Transformer와 마찬가지로 같은 크기를 지니며 모델 내부적으로도 항상 같은 크기의 벡터가 흐른다. Input embeddings',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'BERT는 Transformer모델과 상이한 embedding과정을 지닌다. Embedding과정에 대한 전체적인 흐름은 다음과 같다: 1.Token embeddings input으로 두개의 sentence를 받아서 이를 token단위로 잘라서 각 token들로 token sequence를 만든다. 학습 데이터를 만들 때 앞문장과 뒷문장의 조합은 학습 corpus 내에서 원래 연속적으로 존재하던 두 문장일 수도 있고 아니면 완전히 다른 곳에서 random하게 가져온 문장일 수 있다. 이 확률은 50:50이다. tokenizing 방법으로는 word-piece tokenizing 방식을 사용한다. 이제 input으로 받은 두개의 sentence를 붙여서 사용하는데 첫 번째 문장의 앞에는 special classification token[CLS] (Classification)를, 그리고 각 문장의 끝에는 special token[SEP] (Separate)을 붙인다. 그리고 위와 같은 과정을 거쳐 준비된 input을 token embedding matrix와 내적하여 해당 input에 대한 token embedding값을 lookup한다.(special token들에 대한 embedding vector더 따로 존재하여 special token들도 함께 embedding을 한다.) 이 과정을 거치면$\\\\text{seq_len}$ $\\\\times$ $d_{model}$ 크기의 행렬이 만들어진다. 2.Segment embeddings 두개의 sentence가 input으로 들어왔기 때문에 두 문장을 구분해주기 위해 첫 번째 문장에 대해서는 $E_A$를, 두 번째 문장에 대해서는 $E_B$라는 segment embeddings를 lookup한다. 3.Position Embeddings',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': '그리고 token들의 순서 정보를 반영해주기 위해 position embedding을 해준다. 이렇게 총3단계의 embedding과정을 거친 input이 transformer의 encoder부분과 같은 구조를 가진 모델에 들어가게 된다. Pre-training 앞서 BERT는 pre-train을 위한 semi-supervised step과 특정 task 수행을 위한 supervised step으로 나뉘어 진행되는 것을 언급한 바 있다. Pre-training의 목적은 학습된 모델이 다른 특정 task들을 수행할 때 이미 학습된 모델을 사용함으로써 모델의성능을 높이는 것이다. 따라서 pre-trained된 모델은 다른 과제 수행 시 성능 향상에 도움이 될 만한 과제를 학습하는 것이 좋다. BERT에서는 pre-training task로 masking된 단어를 예측하는 과제와 다음 문장을 예측하는 과제를 동시에 수행한다. Task1 : Masked Language Model(MLM)',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'Bi-directional Encoder Representations form Transformers(BERT)의 이름에서도 알 수 있듯이 BERT는 bidirectional self-attention 연산을 수행한다. 이러한 bidirectionality가 BERT의 핵심이라고 볼 수 있다. 하지만 양방향 연산 시 앞,뒤 단어를 간접적으로 참고할 수 있다. 따라서 양방향 학습을 위해 문장을 그대로 입력하지 않고 입력 token의 15%에 대해 [MASK]token을 부여하여 masking처리를 해준다. 하지만 pre-train과정에서는 [MASK]token이 사용되지만 fine-tuning과정에서는 사용되지 않아 bias가 생긴다. 이러한 이유로 masking방법에 작은 변화를 주는데 예를 들어 “hairy”라는 단어가 masking처리될 단어로 선택되었다면, 전체 corpus에 존재하는 “hairy”라는 단어들 중 80%는 [MASK]token으로 대체하고 10%는 임의로 선정된 다른 단어로 해당 단어를 대체한다. 그리고 남은 10%에 대해서는 그대로 해당 단어를 입력한다.',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': '80% : “My dog is [MASK]” 10% : “My dog is apple” 10% : “My dog is hairy” Task2 : Next Sentence Prediction 인간의 언어에서 앞뒤 문장의 상관관계는 전체 맥락의 이해에 큰 영향을 미친다. 따라서 모델이 연속된 두 문장의 상관관계를 파악할 수 있도록 task2를 이전에 소개한 task1과 함께 동시에 수행한다. 이전에 두개의 문장으로 이루어진 학습데이터 생성 시 50%는 corpus내에 실제 연속적으로 존재하는 문장들로 이루어져 있고 50%는 corpus내에서 임의로 선정된 두 문장을 붙여서 만든다는 것을 언급했었다. 이에 대한 정보는 [CLS]token에 “IsNExt”인지 “NotNext”인지 labeling되어 있다.',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'Pre-training procedure - Training options 본 논문에서 학습 시 사용한 corpus와 설정한 hyperparameter들은 다음과 같다: Corpus : BooksCorpus(800M words) + English Wikipedia(2500M words) Tokens : 37000WordPiece tokens Train batch size : 256 sequences (256sequences *512tokens = 128000 tokens/batch) Steps :1M Epoch : 40epochs Adam learning rate : 1e-4 Weight decay : 0.01 Drop out probability : 0.1 Activation function : GELU - Environmental setup Bert base : 4Cloud TPU(16TPU chips total) Bert large : 16Cloud TPU(64 TPU chips total) ≈72 P100 GPU Training time : 4days Fine-tuning pre-trained 모델에 classification layer를 추가하여 fine-tuning을 통해 특정 task를 수행한다. 이렇게 학습된 BERT의 parameter는 똑같이 사용고 task에 맞는 output layer만 추가하면 다양한 task를 하나의 모델을 통해 수행할 수 있다.이것이 BERT가 대부분의 NLP task에 대해 SotA를 달성 할 수 있었던 핵심으로 생각된다. fine-tuning시에는 input으로 목적에 맞는 labeled dataset을 사용해 fine-tuning하여 최종 모델을 만든다. 해당 논문에서 실험한 fine-tuning task들은 아래 그림에서 확인할 수 있다:',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': '- Task(a) Task(a)는 두 문장간의 상관관계를 파악하는 과제이다. 예를 들어 앞 문장과 뒷 문장의 흐름이 자연스러운지, 두 문장의 의미가 유사한지를 파악하는 것이다. 해당 task에 사용되는 dataset은 SWAG(The Situations With Adversarial Generations)와 GLUE dataset에 속하는 MNLI(Multi-Genre Natural Language Inference), QQP(Quora Question Pairs), QNLI(Question Natural Language Inference), STS-B(The Semantic Textual Similarity Benchmark), MRPC(Microsoft Research Paraphrase Corpus), RET(Recognizing Textual Entailment)이다. SWAG : 현재 문장 다음에 이어질 자연스러운 문장을 선택하기 위한 dataset MNLI : 현재 문장 다음에 이어지는 문장이 문맥상 이어지는 문장인지, 반대되는 문장인지, 상관 없는 문장인지 분류하기 위한 dataset QQP : 두 질문이 의미상 같은지 다른지 분류하기 위한 dataset QNLI : 질의응답 dataset STS-B : 두 문장의 유사성을 파악하기 위한 dataset MRPC : 뉴스의 내용과 사람이 만든 문장이 의미상 같은 문장인지 비교를 위한 dataset RET : MNLI와 유사하나 상대적으로 훨씬 적은 학습 dataset - Taks(b) Taks(b)는 input으로 하나의 문장을 입력하고 분류하는 것이다. 예를 들어 영화 리뷰 데이터를 입력한 후 해당 문장이 긍정인지 부정인지 혹은 중립인지를 판별하는 것이다.',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': '해당 Task에 사용되는 dataset은 GLUE dataset에 속하는 SST-2(The Stanford Sentiment Treebank)와 CoLA(The Corpus of Linguistic Acceptability)이다. SST-2 : 영화 리뷰 문장에 관한 감정분석을 위한 데이터셋 CoLA : 문법적으로 맞는 문장인지 틀린 문장인지 분류를 위한 데이터셋 Task(a), (b)같은 경우에는 sentence classification으로, 출력 부분을 보면 “Class Label”이 있는 것을 볼 수 있다. 이는 [CLS]token에 대한 마지막 hidden state 값(모델의 출력값)을 뜻한다. 따라서 classification task같은 경우에는 classification해야하는 label의 수(K)를 설정하고 [CLS]의 output vector($C\\\\in{R}^{H}$)를 사용하여 모델에 classification layer($W\\\\in{R}^{K\\\\times H}$)만 추가하면 되기 때문에 비교적 간단한 task에 속한다. Fine-tuning과정에서 matrix W와 다른 parameter들은 모델과 함께 학습된다. 해당 task에서는 Batch size : 32, Epoch : 3epoch로 실험을 진행하였다. - Task(c) Task(c)는 SQuAD(The Stanford Question Answering Dataset)가 대표적인 dataset이다. input으로 질문과 해당 질문에 대한 답을 포함하고 있는 paragraph가 입력된다. 질문에 대한 답은 tokenizing된 paragraph내에 존재하는 답의 index값이 출력된다. 따라서 해당 Task같은 경우 정답의 index를 출력하도록 학습이 진행된다. SQuAD : 질의 응답 데이터셋 - Task(d)',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'Task(d)는 각 token들에 대한 정보를 출력한다. 즉 “아이폰11은 크고 무겁지만 이뻐서 참는다”라는 문장을 입력하면 해당 문장은 tokenizing과정을 거치는데 tokenizing된 문장의 각 token들에 대한 정보를 출력한다. 예를 들어 bio tag를 할 수 있다. 해당 Task에 사용되는 dataset은 CoNLL-2003 Named Entity Recognition (NER)이다. CoNLL-2003 Named Entity Recognition (NER) : 개체명 분류 데이터셋 Fine-tuning에서 사용되는 모델의 hyperparameter들은 batch size, learning rate 그리고 training epoch를 제외한 나머지는 pre-training에서 사용된 것과 대부분 동일하지만 fine-tuning과정에서는 특정 task를 수행하기 때문에 해당 task에 맞는 최적의 hyperparameter는 task에 따라 상이하다. Reference',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'Jacob Devlin et al.”BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” Google AI Language. Oct 2018. https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73 http://jalammar.github.io/illustrated-bert/ https://github.com/google-research/bert × Search',\n",
       "  'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/'},\n",
       " {'text': 'Transformer는 이전 게시물에서 소개한 기존 Attention기법을 발전시킨 모델로 Attention Is All You Need(2017)에서 처음 소개되었다. Transformer network의 가장 큰 특징은 자연어 처리과제에 보편적으로 사용되는 RNN계열의 모델을 전혀 사용하지 않고 Attention만으로 작업을 수행한다는 것이다. 기존 기계번역에서 사용되던 RNN기반의 encoder-decoder는 RNN을 사용하기 때문에 순차적으로 연산이 진행되어 문장내 단어의 순서나 위치는 잘 반영하나 연쇄적인 연산으로 인해 첫 입력 값과 마지막 입력 값의 거리가 멀어질수록 초반에 입력된 정보에 대한 손실이 생기며 학습이 느리다는 문제가 있었다. 그리고 Transformer가 나오기 이전에 사용된 Attention mechanism은 기존 RNN based encoder-decoder에 Attention을 추가한 것이기 때문에 여전히 연산 속도는 느리다는 문제가 있었다. 반면 Transformer는 RNN을 완전히 제거하고 각 단어에 대한 중요 정보 인코딩을 행렬곱을 통해 한번에 연산함으로써 학습 속도를 올렸다. 또한 추가적으로 다양한 기술들을 적용하여 성능을 높여 큰 주목을 받았다. Architecture of Transformer',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '아래 그림은 Transformer의 전체 구조이다. 해당 그림을 보면 encoder에 input(source language)이, decoder에는 right shifted된(start token이 있는) output(target language)이 들어가 이를 통해 최종적으로 output probability(다음에 오게 될 단어에 대한 예측 확률)가 도출되는 것을 볼 수 있다:',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Illustrated by the author - Hyperparameter of Transformer 구조에 대한 이해를 돕기 위해 논문에서 설정한 Transformer의 주요 hyperparameter들에 대해 먼저 짚어보겠다. transformer encoder-decoder의 입,출력 크기는 항상 같은 차원을 지니며 내부적으로도 항상 같은 차원의 벡터가 흐르는데 이를 $d_{model}$라 부르며 논문에서는 512로 설정하였다. encoer-decoder를 여러번 수행하는데 이들의 layer 수는 6개로 설정하였다. attention연산도 병렬적으로 여러번 수행되는데 논문에서는 8개의 attention을 사용하였다. feedforward의 hidden state 크기($d_{ff}$)는 feedforward의 입출력 크기와 다르다. Feedforward의 입출력 크기는 $d_{model}$크기이지만 $d_{ff}$ 는 해당 논문에서 2048로 설정하였다.',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Transformer는 Attention을 발전시킨 모델이기 때문에 기존 encoder-decoder모델처럼 encoder와 decoder역할을 하는 부분이 존재한다. 하지만 기존 모델과는 달리 encoder-decoder layer N개를 연속으로 붙여서 사용하는데 해당 논문에서는 encoder와 decoder layer를 각각 6개를 사용하였다. 그리고 Transformer 내부에서는 Encoder와 Decoder layer 전반에 걸쳐 입력부터 출력까지 계속 같은 차원의 벡터($d_{model}$)가 흐른다. Attention에 사용되는 weight는 attention head마다 다르고, fully-connected feed forward neural network에 사용되는 weight는 encoder나 decoder layer 내부적으로는 같은 weight을 사용하지만 layer간에는 다른 weight 로 학습이 이루어진다. - Encoder Illustrated by the author Encoder layer는 위 그림에서 볼 수 있듯이 self-attention과 feed forward 이렇게 두 sub-layer로 이루어져 있다. 위에서 언급했듯이 N개의 encoder layer가 사용될 수 있는데 해당 논문에서는 6개의 layer를 사용했다. 그리고 self-attention이 Multi-Head Attention이라고 표현되어 있고, feed forward neural network는 position wise feed forward neural network라고 표현되어 있다. 이를 통해 attention과 feed forward layer가 여러 번 사용되었다는 것을 알 수 있다. Encoder 내부의 흐름은 다음과 같이 진행된다: Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '위 그림과 같이 여러 layer를 거친 후 마지막 layer의 출력 값이 encoder의 최종 출력 값이 되어 decoder로 넘어간다: Illustrated by the author - Decoder Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Decoder는 self-attention, encoder-decoder attention 그리고 feed Forward, 이렇게 총 3개의 sub-layer로 구성되어 있다. Decoder의 self-attention은 현재 단어 이후에 나올 단어들에 attention이 적용되지 않도록 masking을 한 masked multi-head attention을 사용한다. 그리고 이후 seq2seq에서 사용된 attention과 유사한 encoder-decoder attention도 사용된다. Decoder도 encoder와 마찬가지로 N개의 layer가 사용될 수 있으며 해당 논문에서 encoder와 같이 6개의 layer를 사용하였다. 그리고 attention과 feedforward layer 또한 encoder처럼 여러 번 사용되었다. 1. Embedding Embedding부터 차례대로 살펴보겠다. 위에서 언급했 듯이 encoder에는 input이, decoder에는 right shifted된 output이 입력된다. Embedding은 일반적으로 자연어처리 과제 수행에 사용되는 Embedding Algorithm이 사용된다. 문장이 입력된 후에는 token별로 분리한 후 embedding과정을 거치는데 embedding시 중요한 것은 embedding vector의 차원이 $d_{model}$차원과 같아야 한다는 것이다. embedding vector의 차원이자 모델 내부에서 흐르는 행렬의 크기는 항상 ($\\\\text{seq_len}$, $d_{model}$)인데, $\\\\text{seq_len}$의 크기는 사용자가 설정할 수 있는 hyperparameter로, 일반적으로 train dataset에서 가장 긴 문장의 길이를 크기로 설정한다. 2. Positional Encoding',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'RNN모델이 자연어처리 과제에서 보편적으로 활용될 수 있었던 이유는 해당 모델이 단어의 위치 및 순서 정보를 반영한다는 특징 때문이었다. 하지만 RNN을 전혀 사용하지 않는 Transformer는 따로 위치 정보를 반영해주어야 한다. Positional Encoding은 위와 같은 이유로 Embedding된 encoder와 decoder의 입력 값에 sinusoid function을 사용하여 positional encoding값을 더해 위치정보를 주는 과정이다. 단어별(벡터별) positional encoding수행 과정을 그림으로 표현하면 다음과 같다:',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Illustrated by the author 위 그림은 이해를 용이하게 하기위해 embedding벡터와 positional encoding값이 더해지는 과정을 단어 하나하나 더해지는 것처럼 나타냈지만, 실제 문장을 처리할 때에는 embedding된 단어들은 각각 모두 벡터이고, 문장은 행렬이기 때문에 다음과 같이 embedding벡터가 모여서 만들어진 행렬과 positional encoding행렬이 더해지게 된다. Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '위에서 언급했듯이 positional encoding은 sine과 cosine함수를 통해 위치정보를 가진 encoding값을 만들어 embedding 벡터와 더하는 과정이다. 이를 위한 sinusoid function은 다음과 같이 정의된다: \\\\begin{matrix} PE_{(pos,2i)}=\\\\sin(pos/10000^{2i/d_{\\\\text{model}}}) PE_{(pos,2i+1)}=\\\\cos(pos/10000^{2i/d_{\\\\text{model}}}) \\\\end{matrix} sinusoid function을 사용하여 positional encoding을 해주는 이유는 인코딩 값이 -1부터 1사이의 값이 나오게 되고, 학습데이터보다 긴 문장이 입력돼도 오류 없이 상대적인 encoding 값을 줄 수 있다는 장점이 있기 때문이다. 3. Multi-Head Attention Embedding과 positional encoding과정을 거친 후에는 본격적으로 encoder와 decoder에 입력이 되어 attention layer에 들어간다. 해당 모델에서는 Multi-head attention을 사용했는데 말 그대로 head가 여러개인 attention을 쓴다는 것이다. Transformer에서는 기존 seq2seq 모델에서 사용되던 source-target attention과는 다른 종류의 attention을 사용한다.',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '두 Attention의 차이를 설명하기 전, 이해를 돕기 위해 Query($Q$), Key($K$) 그리고 Value($V$)에 대해 먼저 언급하겠다. 우선 $Q$, $K$, $V$가 모두 벡터라는 것을 염두에 두어야한다. $Q$는 현재의 단어를 표현하는 벡터이다. 그리고 $K$와 $V$는 해당 단어와 다른 단어 간의 상관관계를 파악하기 위해 필요한 벡터이다. $K$는 $Q$에 대한 상대 단어이기 때문에 $K$와 $Q$의 곱(행렬곱)은 attention weight가 되며 $V$는 hidden vector역할을 한다. 3.1 Self-Attention - Source-target attention : Source-target attention은 attention input인 $Q$, $K$, $V$에 대해 $Q$는 target에서, $K$와 $V$는 source에서 받는 것이다. 즉 $Q$는 decoder의 hidden vector(s)이고, $K$와 $V$는 encoder의 hidden vector(h)이다. - Self-attention : Source-target attention과 달리 $Q$, $K$, $V$는 모두 동일한 곳으로부터 입력 받는다. 그리고 기존 attention의 $K$, $V$와 달리 해당 두 벡터의 쓰임을 구분한다. $K$는 attention weight를 도출하는데 사용되고, $V$는 기존 attention의 hidden state vector와 같은 역할을 한다. 이렇게 동일한 곳에서 온 벡터를 $Q$, $K$, $V$로 나누는 이유는 단순히 나눠서 성능이 좋아졌기 때문이다.',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '이제 self-attention연산에 필요한 $Q$, $K$, $V$를 나누는 과정을 설명하겠다. 우선 embedding과 encoding이 완료된 벡터가 입력으로 들어오면 각 단어 벡터마다 각각 다른 가중치인 $W^{Q}$, $W^{K}$, $W^{V}$를 곱하여 각각의 $Q$, $K$, $V$를 얻는다($W^{Q}$, $W^{K}$, $W^{V}$는 훈련 과정 속에서 훈련되는 가중치 행렬이다). \\\\begin{matrix} W_i^Q\\\\text{with dimensions} \\\\ d_{model}\\\\times d_q \\\\end{matrix} \\\\begin{matrix} W_i^K\\\\text{with dimensions} \\\\ d_{model}\\\\times d_k \\\\end{matrix} \\\\begin{matrix} W_i^V\\\\text{with dimensions} \\\\ d_{model}\\\\times d_v \\\\end{matrix} 각 단어 벡터는 $d_{model}$의 크기를 가지며, $Q$, $K$, $V$는 $d_{model}$을 attention layer 수로 나눈 만큼의 차원을 갖는다. 해당 논문에서 $d_{model}$은 512이고 attention layer 수는 8이었기 때문에 $Q$, $K$, $V$는 각각 64차원의 크기를 갖는다.',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Illustrated by the author 위 그림은 단어 벡터 하나에 대한 $Q$, $K$, $V$벡터를 구하는 과정을 표현한 것이다. 하지만 이전에도 언급했 듯이 문장은 단어벡터들을 합친 행렬이기 때문에 실제 연산은 다음과 같이 행렬곱으로 진행된다: \\\\begin{matrix} xW_i^Q=q_i\\\\text{with dimensions} \\\\ \\\\text{seq_len}\\\\times d_q \\\\end{matrix} \\\\begin{matrix} xW_i^K=K_i\\\\text{with dimensions} \\\\ \\\\text{seq_len}\\\\times d_k \\\\end{matrix} \\\\begin{matrix} xW_i^V=V_i\\\\text{with dimensions} \\\\ \\\\text{seq_len}\\\\times d_v \\\\end{matrix} Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '3.2 Scaled dot-product attention 이제 위와 같은 과정(self-attention 방식)을 거쳐 $Q$, $K$, $V$벡터를 얻은 후 attention score function을통해 attention연산을 해야 한다. 이전 게시물에서 attention score를 산출하는 attention score function의 방식이 많이 제시되어 있다고 언급했다. Self-attention에서는 제시된 function중 scaled dot-product attention 함수를 사용하는데 해당 함수를 언급하기 전에 dot product attention에 대해 간단히 설명하겠다. Dot-product attention 연산은 매우 간단하다. 이름 그대로 곱해주면 된다. scaled dot-product attention도 말 그대로 dot product attention에 scaling작업을 추가한 것이다. dot product attention으로 도출된 attention score를 $k$벡터 차원 크기의 루트값으로 나눠 scaling을 해주는데 이렇게 하는 이유는 $k$벡터의 차원이 커질수록 dot product계산시 값이 너무 커지는 문제를 보완하기 위함이다. 아래 수식은 scaled dot-product attention score를 표현한 것이다: \\\\begin{matrix} \\\\text{Score}(Q, K) =\\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\end{matrix} Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '이제 attention weight를 구해야 한다. 기존 seq2seq에 사용되던 attention에서 사용된 함수에서도 attention score를 구하여 softmax를 적용하여 attention distribution을 구한 후 hidden state vector와 weighted sum을 하여 attention score를 구했다. scaled dot-product attention도 마찬가지이다. 위에서 구한 attention score에 softmax를 적용하여 attention distribution을 구한 후 hidden vector역할을 하는 $V$를 곱하면 attention weight를 구할 수 있다. 이를 수식으로 표현하면 다음과 같다: \\\\begin{matrix} \\\\text{Attention}(Q, K, V) =\\\\text{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}})V \\\\end{matrix} Illustrated by the author 3.3 Masked Multi head attention Decoder layer에는 특이하게 Masked Multi head attention이 있는데 이는 output probability를 도출하기 위해서는 아직 출력되지 않은 단어에 attention이 적용되는 것을 막기 위해 현재 step까지 나온 단어를 제외한 다른 단어들을 masking하여 attention과정을 하는 것이다. 그리고 decoder의 Multi-head attention은 encoder의 multi-head attention과 조금 다른데, 현재 decoder의 입력 값을 Query로 사용하고 encoder의 최종 출력 값을 Key 와 Value로 사용한다. 즉, encoder의 최종 출력 값에서 중요 정보를 도출하고 decoder에서는 다음 단어에 적합한 단어가 무엇인지 encoder의 출력 값을 통해 추론해내는 것이다. 3.4 Why Multi-head attention? - Parallelization Multi-head attention은 이름에서도 알 수 있듯이 Attention을 병렬적으로 여러 번 수행하는 것을 뜻한다.',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Illustrated by the author 여러 번 수행한 attention의 결과들(attention weight들)은 concatenate한 후 가중치 행렬 $W^{o}$와 내적하면 Multi-head attention의 최종 결과 값이 나온다. $W^{o}$도 다른 가중치들과 같이 모델과 함께 학습된다. \\\\begin{matrix} \\\\text{Attention}(Q, K, V) =\\\\text{Concat}({head}_ {1}, \\\\dots, {head}_ {h}){W}^{o} \\\\ where \\\\ {head}_ {i}=\\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\end{matrix} Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Multi-head attention의 경우 여러 layer에 대해 행렬 연산들이 모두 병렬적으로 동시에 수행되는데 이러한 병렬화는 하나의 문장을 여러 관점에서 바라볼 수 있다는 장점을 지닌다. 자연어 문장에는 공지시관계의 단어들이 빈번히 사용되며, 모호성과 중의성 또한 존재하는데 이러한 자연어의 특징을 기계가 이해하는 데에 병렬화가 중요한 역할을 한 것이다. 병렬적 연산이 어떤 결과를 주는지 예시와 함께 살펴보면 도움이 될 것이다. 예를 들어 “The animal didn’t cross the street because it was too tired.”라는 문장에서 “it”이 지시하는 것이 무엇인지 쉽게 알아낼 수 있지만 기계의 경우에는 그렇지 않다. 따라서 기계는 해당 문장을 다각에서 바라본 후 확률적으로 “it”과 가장 연관성이 가장 높은 단어를 알아내는 과정을 거쳐야한다. 다음 그림은 해당 과정을 시각화한 것이다:',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '위 그림은 5번째 encoder의 첫 번째 attention layer에서 “it”을 encoding하는 것을 나타낸 그림이다. 그리고 아래 그림은 attention을 병렬적을 처리했을 때 하나의 문장을 다양한 관점으로 볼 수 있다는 것을 증명하는 그림이다: 위 그림을 보면 첫 번째 attention에서는 “it”이 “the animal”과 관련 있다는 결론이 도출되었지만 또 다른 attention layer에는 ”it”이 “tired”와 연관성이 높다는 결론이 도출되었다. 이렇게 다양한 관점에서 정보를 수집하게 되면 더 수준 높은 encoding 결과를 얻을 수 있다. 위 시각화는 Tensor2Tensor Colab에서 구현해볼 수 있다. 4. Residual connection(Add) & Normalization(Norm) Transformer 전체 구조를 표현한 그림을 보면 attention과 feed-forward neural network이후 residual connection과 layer normalization작업을 수행하는 것을 확인할 수 있다. residual connection을 해주는 이유는 word embedding 이후에 positional encoding을 해주었는데 학습을 진행하다 보면 backpropagation(역전파)에 의해 positional encoding 값이 손실될 가능성이 있기 때문이다. 간단히 말하자면 해당 정보에 대한 손실을 줄이기 위해 입력 값을 다시 더해준 것이다. Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '그리고 해당 모델 내부에서 Residual 연산이 계속되기 때문에 모델 전반에 흐르는 차원을 $d_{model}$(=512)로 고정하여 연산을 간편하게 하였다. Residual connection이후에는 layer normalization을 통해 학습의 효율을 높였다. 5. Position-wise Feed forward neural network(fully connected layer) 이 과정은 point wise로 진행되는 일반적인 feed forward network이다. 수식은 다음과 같다: \\\\begin{matrix} \\\\text{FFNN}(x)=\\\\text{MAX}(0, xW_1+b_1)W_2+b_2 \\\\end{matrix} 위 식을 보면 0보다 작으면 0을 내보내고 0보다 크면 weight bias 값을 내보낸다. 이 부분이 흡사 relu activation과 닮아 있는 것을 알 수 있다. 아래는 수식 이해를 돕기 위한 그림이다: Illustrated by the author',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '따라서 해당 과정은 relu activation function을 사용하여 feed forward해주는 것으로 생각하면 된다. 6. Linear Function 이렇게 attention score를 구하고 난 후에는 나온 값들을 모두 concatenate하여 linear function을 통과시킨다. Linear function을 적용한 이유는 $d_{model}$ size와 word size가 다르기 때문에 이 크기를 linear mapping해주기 위함이다. 7. Softmax Linear를 거치고 softmax를 적용해 확률 값으로 변환하게 된다. Softmax를 적용한 확률 값은 모델이 알고 있는 모든 단어들에 대한 확률 값이며 키에 해당하는 단어가 현재 단어에 대한 연관성을 나타낸다. 따라서 가장 높은 확률이 나온 단어가 다음 단어가 되는 것이다. 예를 들어 softmax의 결과가 0.96이 나왔다면 그 단어가 현재 단어와 96퍼센트 연관성이 있다는 것이기 때문에 해당 단어가 다음 단어가 되는 것이다. 8. Train/Test softmax이후의 작업은 train과정과 test과정으로 진행 방식이 나뉜다. - Train',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Train과정에서는 이전에 encoder와 decoder의 각 layer를 거칠 때 output으로 나온 것이 다음 단계에 다시 입력되지 않고 label이 또다시 입력된다. 그리고 최종적으로 나온 output들을 모두 concatenate한 후 벡터로 전환된 label과 비교하여 그들의 차이를 줄이는 방향으로 학습을 진행한다. 해당 논문에서 학습 시 사용된 optimizer는 adam이고, 정규화를 위해 residual과 attention에 대해 dropout(0.1)을 해주었다. 그리고 추가적으로 성능향상을 위해 label smoothing(0.1)작업도 수행하였다.',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Label smoothing이란, 일반적으로 label은 one-hot-vector로 표현되는데 label smoothing을 통해 정답은 1에 가까운 값으로, 오답은 0에 가까운 값으로 표현하도록 하는 것이다. 이는 label의 noise현상을 방지해주는 기술이다. 해당 작업이 필요한 이유는 자연어 번역 시 번역의 결과가 다양할 수 있기 때문이다. 예를 들어 ‘sleepy’라는 단어를 한국어로 ‘잠이 온다’와 ‘졸리다’로 번역이 가능한데 상이한 결과가 one-hot-vector로 표현될 경우 전혀 다른 one-hot-vector가 정답 값으로 동시에 존재하기 때문에 학습에 혼동을 줄 수 있다. 따라서 label smoothing을 통해 학습의 혼돈을 막고 효율성을 높이는 효과를 볼 수 있다. - Test 반면 test과정에서는 output이 나오면 그 output을 다시 입력하여 또 다른 output을 도출시킨다. 따라서 최종적으로 도출된 벡터는 문장 속에서 존재하는 단어벡터가 되어 문맥을 고려한 하나의 벡터가 된다. 즉 같은 단어여도 문맥에 따라 달리 embedding된다. Reference',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.”Attention Is All You Need,”Cornell(2017) http://jalammar.github.io/illustrated-transformer/ http://nlp.seas.harvard.edu/2018/04/03/attention.html https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XdO8r1czY2x × Search',\n",
       "  'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/'},\n",
       " {'text': '이전 게시물에서는 seq2seq모델에 대해 살펴보았다. 이제 seq2seq모델의 한계점 중 하나인 장기 기억력 문제를 해결하기 위해 고안된 Attention Mechanism을 seq2seq에 추가한 방식을 다룬 논문 ‘NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE‘에 대해 설명하도록 하겠다. Attention Mechanism은 현재 다양한 deep learning model에 활용되고 있지만 기계번역을 위한 seq2seq에 가장 처음 도입되어 소개되었다. 기존 seq2seq모델에서는 Encoder에서 나온 모든 state값을 활용하지 않고 단순히 마지막에 나온 hidden vector를 하나의 고정된 context vector로 사용하였다. 이는 seq2seq model이 장기 기억 한계에 부딪힌 가장 큰 원인으로 볼 수 있다. 왜냐하면 context vector는 Encoder부분의 정보를 압축하고 있는 것인데 문장이 길어질 수록 정보 압축 시 정보 손실이 발생하기 때문이다. 이러한 문제를 해결하기 위해 고안된 Attention Mechanism에서는 Encoder에서 나온 각각의 state값을 모두 활용하여 Decoder부분에서 dynamic하게 context vector를 만들어 하나의 고정된 context vector의 사용으로 인해 발생한 seq2seq model의 문제를 해결하였다. 즉, 각각의 state 별로 context vector를 새롭게 만들어내 seq2seq model의 한계를 개선한 것이다.',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': 'Attention Mechanism에 대한 이해를 돕기 위해 간단한 비유로 해당 Mechanism의 목적을 짚고 넘어가도록 하겠다. 인간이 대화를 하거나 문서를 읽는 것을 생각해보면 입력 정보 전체를 기억하지 않고 중요한 문장이나 단어에 집중하여 기억하고 이해한다. 이 처럼 어떤 것이 중요한지, “Attention”이라는 의미 그대로 중요한 것을 기억하며 학습하는 구조가 Attention Mechanism이다. Architecture of Attention',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': 'Illustrated by the author',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': '이제 위 그림을 보며 Attention Mechanism의 진행 방식을 천천히 살펴보겠다. 1. 우선 Encoder는 Input $x_i$받아 그에 해당하는 hidden state vector $h_j$를 만든다. $h_j$는 정방향, 역방향 hidden state vector들을 concatenate하여 구한다. 2. 구해진 $h_j$는 다시 attention score를 구하는 데에 사용된다. 수식을 살펴보면 $s_{i-1}$ 이 있는 것을 확인 할 수 있다. 이는 Decoder부분에서 나온 hidden state vector이다. 이전 step의 decoder hidden state vector와 현재 step의 encoder hidden vector를 linear function에 넣어 attention score를 구하는데 이는 이후 더 자세히 언급하도록 하겠다. 3. 이제 구해진 attention score에 softmax를 적용하여 확률값을 도출해낸다. 여기에서 나온 확률 값이 attention weight이다. Attention weight는 현재 스텝에서 가장 중요하게 영향을 미친 부분에 가중치를 준 값이다. 이를 통해 Decoder가 어느 입력 부분에 집중해야 할지를 알려주는 것이다. 4. 이제 지금까지 준비해온 정보들을 하나로 합치는 단계다. 어텐션의 최종 결과값을 얻기 위해 각 인코더의 hidden state와 방금 구한 attention weight값들을 각각 모두 곱하고, 최종적으로 모두 더한다. 즉, Weighted Sum을 한다.',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': '3번과 4번을 정리하자면, step별로 확률 값(이 값들의 총 합은 1이다)이 나오는데 이를 attention weight라 부른다. attention weight들이 나오면 각 step에 해당하는 encoder hidden vector을 곱한다. 예를 들어 h1x0.9라면 첫 번째 step에 90% focus한 값이 context vector로 나오게 된다. 이렇게 구해진 값이 바로 context vector(attention value)이다. 따라서 context vector가 각 step별로 decoding할 때마다 새롭게 나오게 되는 것이다. 5. context vector를 구한 후에는 $s_{i−1}$ 과 context vector를 우선 concatenate하고 Decoder연산을 한다(seq2seq decoder와 같다). 지금까지 Attention Mechanism을 진행 순서에 따라 살펴보았는데 이전에 언급한 바와 같이 Encoder의 각 step을 전부 활용하여 context vector에 적용되는 것을 볼 수 있다. 이는 seq2seq모델만 사용할 때 보다 더 방대한 양의 information을 함축하는 데에 있어 더 효율적이다. 아래는 두 번째 단어 예측 시 적용되는 수식을 풀어 놓은 것이다:',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': '- Attention score Attention score는 어떤 입력 step에 집중할 것인가에 대한 정보를 담고 있는 점수이다. Attention score에 관련된 식을 이해하기 위해 우선 해당 수식의 첨자를 이해해야한다. 위 수식을 보면 $i$와 $j$, 두 첨자가 존재하는 것을 확인할 수 있다. 먼저 첨자 $i$는 context vecore, decoder hidden state, output을 순차적으로 구할 때 해당 step에 대한 위치 정보가 반영된 첨자이다. 쉽게 말해 예측하려는 것에 대한 위치 정보이다. 그리고 첨자 $j$는 hidden state와 x에 대한 정보를 담고 있는 첨자이다. 그리고 함수$a$는 이전 step의 decoder hidden state vector와 현재 step의 encoder hidden vector가 들어가는 linear function이다. 이는 아래와 같이 굉장히 다양한 방식이 제시된 상태이다: - Decoder hidden state',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': '이 부분이 Attention을 적용한 seq2seq의 가장 핵심 부분으로 볼 수 있다. Decoder의 hidden state값은 이전 Decoder hidden state와 직전 step의 output 그리고 Attention output vector를 이용하여 출력하는데 위에 제시된 두 수식을 보면 fixed-length vector가 attention mechanism 적용 후 각 index마다 다르게 반영된다는 것을 알 수 있다. Experiment - Dataset 해당 모델의 성능 평가를 위해 시행한 실험에 대해 간단히 언급하겠다. 성능 평가를 위한 실험으로 영어-프랑스어 번역을 과제로 삼았으며 dataset은 WMT’14 English-French corpora(totaling 850M words)이다. 그리고 데이터 크기를 줄이기 위해 data selection method (by Axelrod et al. (2011))를 이용했으며, 이전에 언급한 parallel corpora이외에 monolingual data는 사용하지 않았다. - Result Reference Dzmitry Bahdanau, KyungHyun Cho.”NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,”ICLR(2015) × Search',\n",
       "  'source_title': 'Attention in sequence to sequence\\n      12 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/12/Attention/'},\n",
       " {'text': '해당 게시물에서 소개할 Seq2Seq모델은 기계번역에 사용되는 것으로 잘 알려져 있다. 따라서 모델 설명 이전에 가볍게 번역 과제에 대해 언급하도록 하겠다. 1.Machine Translation 1.1 Why is it so hard? 번역 과제는 source language의 의미를 target language로 옮겨 다른 두 언어가 같은 의미를 지니게하도록 하는 작업이다. 해당 작업은 여러 자연어처리 과제 중 어려운 과제에 속한다. 아래는 번역 과제가 어려운 이유이다: 1) 인간의 언어는 단순히 합성성의 원리로만 이해될 수 없다. 즉 의미를 지닌 단어들을 쭉 나열한다고 해서 나열된 단어들의 전체 의미가 각 단어들의 의미의 합과 같지 않다는 것이다. 대표적으로 관용구를 예로 들 수 있다. 2) 함축을 활용하여 다양한 정보를 전달한다.',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '인간은 말해진 것 이상의 의미(대화에서 내포하고있는 의미)를 주고 받는다. 이를 해석하기 위해서는 협력원리가 작용한다. 협력의 원리에는 양, 질, 관계, 태도 이렇게 네 가지의 대화의 격률이 존재하는데 대화과정에서 대화격률이 의도적으로 위반되는 경우에도 협력원리가 유지된다는 가정하에 화자는 특정한 의미를 함축하게 된다. 또한 청자도 화자의 발화로부터 전달되는 문자적 의미와 배경지식 등을 고려하여 화자의 함축된 의미를 추론함으로써 의사소통을 한다. 자연어의 표현은 의미가 고정되지 않고 대화 문맥에 따라 의미가 달라진다. 항진명제와 은유가 빈번히 사용된다.',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '3) 특히 한국어 같은 경우 생략 현상과 강세 변화에 따른 의미 변화가 다른 언어에 비해 더 많이 나타난다. 1.2 History 이어서 기계번역의 역사에 대해서도 짚어보고 가겠다. 1) RBMT(Rule-Based Machine Translation) : 전통적인 번역 방식인 규칙 기반 기계번역이다. 말 그대로 형태, 통사, 의미 정보가 포함된 광범위한 어휘소와 대규모 규칙을 필요로 하는 번역 방식이다. 해당 규칙은 사람이 만들어야 하기 때문에 많은 자원과 시간이 소모된다. 또한 언어는 매일매일 지속적으로 변화한하기에 번역 언어쌍 확장은 필수적으로 이루어져야 하는데 이때 번역 품질 향상을 위해 전문가가 매번 새로운 규칙을 찾아내 적용해야한다. 따라서 지속적이 투자가 필요하기 때문에 실제 산업에서는 사용률이 저조했다. 2) SMT(Statistical Machine Translation) : 통계 기반 기계번역으로 대량의 양방향 코퍼스에서 통계를 얻어내 번역 시스템을 구성한다. 언어쌍 업로드가 RBMT에 비해 비교적 빠르고 간단하다. 3) NMT(Neural Machine Translation) :',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '딥러닝 기반의 기계번역. 해당 방식의 도입으로 인해 기계번역 발전에 박차를 가할 수 있었다. 이전까지 연구된 기계번역 방식보다 NMT가 잘 작동하는 이유는 다음과 같다:',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': 'End-to-end Model $\\\\rightarrow$ SMT의 경우 여러가지 모듈로 구성되어 시스템이 복잡해 훈련에 어려움이 있었던 반면, NMT는 단 하나의 모델을 사용함으로써(전체를 하나의 모델로 한번에 학습함으로써) 시스템을 단순화 하여 성능을 높였다. Better language model $\\\\rightarrow$ 신경망 언어모델(Neural Network Language Model)을 기반으로 하는 구조이기에 기존의 SMT에서 사용하던 n-gram 방식의 언어 모델보다 강력하다. 희소성(sparseness)문제가 해결 되었으며, 자연스러운 번역을 가능하게 한다. (간단히 말하자면 모델 자체가 더 나아졌다는 것이다.) Great context embedding $\\\\rightarrow$ Neural Network모델은 데이터를 압축시키는 것에 있어 좋은 성능을 자랑한다. 따라서 문장의 의미를 벡터화(vectorize)하는 능력이 뛰어납니다. 따라서, 노이즈나 희소성(sparseness)문제를 이전보다 잘 해결할 수 있다.(간단히 말하자면 임베딩 기능이 향상되었다는 것이다.)',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '2. Sequence to Sequence Model 이제 본격적으로 Seq2Seq이라는 모델에 대해 소개하도록 하겠다. 일반적인 Deep Neural Network같은 경우 다양한 분야에서 좋은 성과를 보였으나 sequence data처리에는 큰 성과를 보이지 못 했다. 이는 input dimension과 output dimension이 고정되어 있기 때문이다. 물론 convolution연산의 경우 input의 크기가 정해져 있지 않지만 Multi-Layer-Perceptron의 경우에는 입력의 차원과 출력의 차원을 정한 후에 학습을 시작한다. 이러한 방식은 이미지 데이터와 같이 크기가 고정된 데이터를 처리할 경우에 유용하게 사용될 수 있다. 하지만 인간이 주고받는 언어의 입력과 출력 크기는 상황에 따라, 사람에 따라 매우 상이하다. 따라서 위에서 언급된 모델들로는 인간의 언어를 학습시키는 것에 어려움이 있다. 이러한 문제점을 해결하기 위해 고안된 모델이 오늘 설명할 seq2seq모델이다.',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '위에서 언급한 바와 같이 seq2seq은 입, 출력의 크기가 정해지지 않아도 학습이 가능한 모델이다. 이는 모델의 구조를 살펴보면 그것이 가능한 이유를 알 수 있다. seq2seq 구조는 크게 Encoder와 Decoder로 나뉜다. Encoder는 입력문장을 받아 그것을 벡터로 변환하는 역할을 하고, Decoder의 경우에는 Encoder를 통해 만들어진 벡터를 사용하여 다시 또 다른 문장을 만들어 내는 것이다. Decoder 부분에서 Generator라는 모듈을 따로 떨어트려 살펴보면 이는 Decoder의 출력 벡터들을 Softmax를 통과시켜 가장 높은 확률을 가진 단어 하나를 추출해 내는 작업이 이루어지는 곳이다. Seq2seq모델을 수식화 하면 다음과 같이 표현할 수 있다: \\\\begin{matrix} {\\\\theta}^{ * } \\\\approx argmax \\\\ P(Y|X ;\\\\theta) \\\\ where\\\\ X=\\\\ { x_1, x_2, x_3, …, x_n\\\\ } , Y=\\\\ { y_1, y_2, y_3, …, y_m\\\\ } \\\\end{matrix} 위 수식을 통해 Seq2seq은 모델의 파라미터($\\\\theta$)가 주어졌을 때 source data($X$)를 받아서 target data($Y$)를 가장 높은 확률의 값으로 반환할 때 해당 확률을 최대로 올리는, 즉 $P(Y|X ;\\\\theta)$ 를 최대로 하는 모델 파라미터를 학습하는 모델이라는 것을 나타낸다.',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': 'Seq2seq모델은 말 그대로 sequence data가 들어가서 sequence data가 나오는 모델로, 간단히 말하자면 many-to-many방식의 RNN인데 encoder, decoder, generator를 통해 높은 확률의 결과 값을 도출하도록 학습하는 모델이다. 2.1 Architecture of seq2seq model',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': 'Illustrated by the author',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '위 그림을 보며 지금까지 개괄적으로 언급한 내용에 대해 조금 더 자세히 살펴보도록 하겠다. $\\\\circ$ Encoder 이곳에서는 $P(X)$를 모델링한다. 위 그림을 보면 $x_1$ (I), $x_2$ (love), $x_3$ (you)라는 입력이 순차적으로 입력되며 각각 embedding layer를 거치며 벡터화(vectorize) 되어 Stacked RNN Cell에 들어가는 것을 확인할 수 있다. 여기에서 Stacked RNN Cell은 Multi layer RNN이라고 생각하면 된다. 이곳에는 Vanilla RNN, LSTM, GRU, Bidirectional-RNN과 같은 RNN계열의 모델들이 사용될 수 있다(논문 Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation에서는 GRU를, Sequence to Sequence Learning with Neural Networks에서는 LSTM을 활용하였다). 각 입력에 대한 hidden state값들은 일반적인 RNN계열 모델들과 같이 다음 state로 넘어가서 해당 state의 input 값과 함께 입력된다. 그렇게 Encoder의 마지막 입력 값이 들어오면 padding 작업 후 Encoder부분의 마지막 hidden state값을 context vector(fixed-length vector)로 활용하여 Decoder로 넘긴다. 따라서 context vector는 Encoder정보를 함축하여 담고 있는 vector이다. Encoder를 수식으로 표현하면 다음과 같다. \\\\begin{matrix} { h }_ { t }^{ src }={ RNN }_ { enc }({ emb }_ { src } (x_t), { h }_ { t-1 }^{ src }) \\\\end{matrix} \\\\begin{matrix} {H}^{ src }= [h_1^{src}; {h}_ {2}^{ src }; \\\\dots ; {h}_ {n}^{ src }] \\\\end{matrix}',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '위 수식은 time-step별로 RNNcell을 통과시킨 것을 나타낸다. ${ h }_{ t }^{ src }$는 $t$번째 source state의 hidden vector값을 뜻한다. 이는 Encoder부분의 RNN cell을 지나는데 해당 RNN cell에는 source data인 $x_t$의 embedding 값과 이전 hidden vector가 들어간다. 이렇게 나온 각 state의 hidden vector값은 모두 concatenate되어 ${ H }^{ src }$가 도출된다. 이는 사실상 코딩을 할 경우 아래와 같은 수식이 된다. \\\\begin{matrix} {H}^{src}= {RNN}_ {enc}({emb}_ {src}(X),{ h }_ { 0 }^{src}) \\\\end{matrix} $\\\\circ$ Decoder Decoder부분의 경우 문장의 시작을 알리는 $BOS$(begin-of-sentence)token과 끝을 알리는 $EOS$(end-of-sentence)token이 존재한다. Decoder의 시작부분 같은 경우에는 context vector와 임의의 초기값으로 설정한 $y$(이전 state의 출력 값)와 $s$(이전 state의 hidden vector값)가 입력되어 첫 출력 값을 도출한다. 이후에는 context vector와 이전 state의 출력 값으로 나온 $y$ 즉, ${y}_{i-1}$와 이전 state의 hidden vector인 $s$ 즉, ${s}_{i-1}$이 Encoder에서와 같이 RNN계열의 모델에 입력된다. 여기에서 context vector는 항상 같은 값이 사용된다. seq2seq모델의 진행 방식을 time-step에 대해 수식화 하자면 다음과 같다. \\\\begin{matrix} {P}_ { \\\\theta }(Y│X)=\\\\prod_{t=1}^{m} {P}_ { \\\\theta }(y_t |X, y_{<t}) \\\\end{matrix} \\\\begin{matrix}',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '\\\\log\\u2061 P_{ \\\\theta }(Y|X)=\\\\sum_{t=1}^{m} \\\\log\\u2061 P_{ \\\\theta }(y_t|X, y_{<t}) \\\\end{matrix} 위 수식은 이전 time-step의 결과와 encoder로부터 도출된 결과($X$)를 기반으로 하여 target값($Y$)을 유추해내는 작업을 표현하고 있다. 아래 수식은 Decoder부분을 수식으로 표현한 것이다. \\\\begin{matrix} { h }_ { t }^{ tg\\u2061t }={ RNN }_ { dec } ({ emb }_ { tgt }({ y }_ { t-1 }), { h }_ { t-1 }^{ tgt }) \\\\ where \\\\ { h }_ { 0 }^{ tg\\u2061t } = { h }_ { n }^{ src } \\\\ and \\\\ y_0=BOS \\\\end{matrix} 위 수식을 살펴보면 ${ h }_{ t }^{ tg\\u2061t }$ 는 $t$번째 target state의 hidden vector인데 이는 Decoder부분의 RNN cell에 이전 state의 결과 값($y_{t-1}$)을 embedding한 값과 이전 state에서 나온 hidden vector값이 입력되어 도출된다. 여기에서 주의 깊게 살펴볼 점은 decoder부분의 첫 시작에서는 $y$의 초기 값으로써 $BOS$를 입력한다는 것이다. $\\\\circ$ Generator Generator부분에서는 Decoder부분의 RNN Cell을 거쳐 나온 값들이 softmax를 거쳐 0부터 1사이의 확률 값으로 변환되어 가장 높은 확률 값이 해당 state의 출력 값으로 도출된다. 아래는 Generator 부분을 수식화 한 것이다. \\\\begin{matrix} \\\\hat{ y }_ { t }=softmax({ linear }_ { hs\\\\rightarrow \\\\left\\\\vert { V }_ { tgt } \\\\right\\\\vert }({ h }_ { t }^{ tg\\u2061t })) \\\\ and \\\\ \\\\hat{ y }_ { m }=EOS \\\\end{matrix} Where $hs$ is hidden size of RNN, and $\\\\left\\\\vert { V }_{ tgt } \\\\right\\\\vert$ is size of output vocabulary.',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '위 수식에서 softmax는 비선형 함수임에도 그 속에 선형 함수가 들어있는 것을 볼 수 있다. 이는 decoder에서 도출된 각 state의 hidden vector값들을 0-1사이의 확률 값으로 변환하기 위해 차원을 늘려주는 작업이다. 그리고 여기에서 특기할 점은 decoder부분의 첫 입력 값으로 계산의 시작을 나타내는 초기값 $BOS$가 있듯이 decoder 계산의 종료를 알리는 $EOS$ token이 존재한다는 것이다. 2.2 Experiment 지금까지 seq2seq모델의 구조를 하나하나 뜯어 살펴보았는데 위와 같은 방식은 특히 긴 문장을 처리할 때 일반적인 RNN계열의 모델만 단순히 사용했을 보다 더 좋은 성능을 보이게 된다. 왜냐하면 RNN은 일반적인 feedforward neural network인데 즉, input($x_1$, $x_2$, $x_3$ $\\\\dots$ $x_n$)에 대해 아래 수식을 반복하며 output($y_1$, $y_2$, $y_3$ $\\\\dots$ $y_T$)을 도출해내는 구조를 가진다. 따라서 이전 state의 정보를 고려할 수 있다는 장점이 있지만 입, 출력의 길이가 상이할 경우에는 적용하기 어렵다는 한계가 있기 때문이다. \\\\begin{matrix} h_t=\\\\sigma(\\\\mathbf{W}^{hx}x_t+\\\\mathbf{W}^{hh}h_{t-1}) \\\\end{matrix} \\\\begin{matrix} y_t=\\\\mathbf{W}^{yh}h_t \\\\end{matrix}',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '이제 2014년도에 작성된 논문’Sequence to Sequence Learning with Neural Networks‘에서 시행한 실험에 대해 간단히 언급하도록 하겠다. - Differences from the models described above: three important ways 1. Sequence to Sequence Learning with Neural Networks논문에서 진행된 실험에서는 RNN cell에 LSTM을 적용하였으며 input sequence와 output sequence에 대해 다른 LSTM을 사용하였다. 즉 두개의 다른 LSTM을 사용하여 Encoder와 Decoder의 네트워크를 분리시켰다. 왜냐하면 모델의 파라미터 수가 늘어나며 더 많은 양의 학습이 가능하기 때문이다. 2. 또한 위 그림(1)에서도 알 수 있듯이 seq2seq에서는 일반적으로 layer를 깊게 쌓은 모델을 사용하는데 이 논문에서는 4개의 layer를 사용하였다.',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '3. 그리고 마지막으로 중요한 특이 사항은 input sequence의 순서를 거꾸로 입력했다는 것이다. 예를 들어 a, b, c가 입력되어 $\\\\alpha$,$\\\\beta$,$\\\\gamma$값을 예측하는 작업에서 입력 순서를 뒤집어 c, b, a를 입력한 것인데 이러한 작업을 통해 모델의 성능이 높아지는 결과를 얻었다. 이를 통해 모델의 성능이 높아졌는데 그 이유는 이러한 data transformation을 통해 데이터에 변형을 주기 전보다 a가 예측해야 하는 $\\\\alpha$와, b가 예측해야 하는 $\\\\beta$와 가까워져 SGD(Stochastic Gradient Descent)를 위한 input과 output간의 “상호 교신(“establish communication)”이 수월해졌기 때문이다.(data transformation은 학습할 때만 적용했다.) - Dataset details: 160,000 input vocabulary, 80,000 output vocabulary',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '해당 논문에서는 위 모델을 WMT’14 English to French MT(machine translation) task에 적용하였다. 이 실험을 위해 WMT’14 English to French dataset의 12M문장 중 348M 프랑스 단어와 304M의 영어 단어 데이터가 사용되었다. 그리고 각 단어들을 벡터로 표현하기 위해 두 언어에 대해 각각 고정된 vocabulary를 사용했는데 여기에는 source language에서 가장 빈번히 사용되는 160,000개의 단어와 target language에서 빈번히 사용되는 80,000개의 단어가 포함되어 있다. 그리고 out-of-vocabulary에 대해서는 “$UNK$”라는 token을 부여하도록 했다. - Decoding and Rescoring: 1000 cell at each layer, 1000dimensional word embeddings 이 실험의 핵심은 많은 문장 쌍들에 대해 깊은 LSTM을 사용했다는 것이다. 해당 모델의 학습은 source sentence $S$가 정확히 번역된 target sentence $T$(정답 값이라 생각하면 된다)의 $log$ 확률 값을 최대화하는 과정이다. 따라서 학습에 대한 수식은 다음과 같다: \\\\begin{matrix} 1/\\\\vert S\\\\vert\\\\sum_{(T,S)\\\\in S}\\\\log p(T\\\\vert S) \\\\end{matrix} 위 수식에서 $S$는 training set이다. 이와 같은 학습이 완료된 후에는 LSTM에 의해 가장 비슷하게 번역된 것을 찾는 작업을 하는데 해당 작업에 대한 수식은 다음과 같다: \\\\begin{matrix} \\\\hat{T}=\\\\arg\\\\max_T p(T\\\\vert S) \\\\end{matrix}',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '해당 논문에서는 simple left-to-right beam search방식을 decoder에서 사용하였다. 즉, 가정할 단어 수를 B개라고 가정한 후(후보 단어의 수를 B개로 설정한 후) time-step마다 vocabulary에 존재하는 가능한 각각 다른 단어들을 추가하여 이들 중 모델의 $log$확률값이 가장 높은 B개를 제외한 후 나머지는 모두 버린다. (본 논문에서는 1000개의 best-list를 사용하였고 모델은 위에서도 언급했듯이 LSTM을 사용하였다.)그리고 $EOS$ token이 나오면 문장에 이를 추가한 후 완성된 문장을 만든다. - Reversing the Source Sentence: data transformation',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '앞서 언급했 듯이 source sentence를 뒤집어 입력했을 때 더 좋은 성능을 보였는데 해당 방법을 통해 perplexity는 5.8에서 4.7로 떨어졌고, BLEU score은 25.9에서 30.6로 오르는 결과를 보였다. 현재 이러한 현상에 대한 구체적인 이유는 알 수 없지만, 데이터에 변형을 주지 않을 경우 source sentence와 target sentence를 concatenate할 때 source sentence에 있는 각 단어들은 target sentence에 있는 관련된 단어와 멀어지게 된다. 이는 minimal time lag문제를 야기시키는데 Source sentence를 뒤집음으로써 서로 관련 있는 source language와 target language의 평균 거리에는 변화가 없지만, 앞쪽 몇몇 단어들의 거리가 굉장히 가까워져 minimal time lag문제가 눈에 띄게 개선된 것을 확인할 수 있었다. 해당 문제가 개선될 수 있었던 이유는 backpropagation할 때 source sentence와 target sentence간의 “상호 교신(establishing communication)”이 수월해졌기 때문이다. - Result',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '2.3 Model Analysis 위 그림은 context vector(fixed-length vector)를 PCA를 통해 2차원 공간에 투사한 결과이다. 해당 그림을 보면 문장 순서에 따라 값이 크게 달라지는데 능동과 수동문에 대해서는 값의 차이가 크지 않은 것을 확인할 수 있다. 2.4 Teacher Forcing 추가적으로, 학습 시 활용될 수 있는 Teacher Forcing기법에 대해 설명하도록 하겠다. Illustrated by the author 위 그림과 같이 만약 prediction값이 틀렸을 경우에, 틀린 값이 prediction으로 나왔는데 이걸 다시 fully connected에 넣으면 문제가 생긴다. 그렇기 때문에 이런경우를 대비하여 prediction 값 대신 정답을 Decoder의 Input으로 입력하여 이전에 잘못된 결과가 나왔더라도 정답을 그 다음 step의 입력으로 넣으으러써 학습을 더 빠른고 효율적으로 만들어준다: Illustrated by the author 2.5 Applications of seq2seq 지금까지 Seq2seq모델에 대한 설명과 그에 대한 실험을 간략하게 살펴보았다. 본 모델은 기계번역을 포함하여 아래와 같이 다양한 분야에서 사용될 수 있다: Applications of seq2seq Tasks Neural Machine Translation (NMT) Source language문장을 입력으로 받아 같은 의미의 target language문장을 출력',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': 'Chatbot 문장을 입력으로 받아 그에 대한 대답을 출력 Summarization 긴 문장을 입력으로 받아 그를 요약하여 출력 Automatic Speech Recognition (ASR) 말소리를 입력으로 받아 해당 언어를 문자로 변환하여 출력 Image Captioning 이미지를 입력으로 받아 해당 이미지에 대한 설명을 문자로 출력(변형된 seq2seq 사용)',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '2.6 Limitation Seq2seq모델의 한계에 대해서도 알아보겠다. 1) Memorization Neural Network모델은 데이터를 압축시키는 것에 있어 좋은 성능을 자랑한다. 하지만 데이터를 압축시키는 것에도 한계가 있다. 예를 들어 본 글에서 지금까지 나열한 수많은 정보를 단 한 줄로 요약하라면 많은 정보의 손실이 있게 될 것이다. 따라서 문장이 길어질수록 성능이 저하되는데 이러한 문제를 해결하기 위해 고안된 대표적인 Mechanism이 Attention이다. 2) Lack of Structural Information 현재 Deep-learning NLP는 문장을 단순히 sequential data로서 처리하는 경향이 있다. 이러한 상황에서는 sequential data를 처리하는 것에 높은 성능을 보이는 seq2seq모델은 성공적이지만 NLP의 발전을 위해서는 문장을 구조를 체계적으로 이해하는 모델이 필요하다. 3) Conversation',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': '인간과 기계 간의 대화가 가능하기 위해서는 단순한 질문-대답 형식을 구현하는 모델에서 해당 형식의 대화보다 더 많은 대화 형식을 구현할 수 있는 모델이 필요하다(화용론 하위 주제인 CA(conversation analysis)참고). 대화의 경우에는 대화의 흐름에 따라 새로운 정보가 계속 추가된다. 번역이나 요약의 경우 새로운 정보에 대한 고려 없어도 문제가 없지만 chatbot같은 경우에는 더 발전된 architecture가 필요하다. Reference',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': 'Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio.”Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,”(2014) Ilya Sutskever, Oriol Vinyals, Quoc V. Le.”Sequence to Sequence Learning with Neural Networks,”(2014) CS224D Lecture 8 - 2015 - Fancy Recurrent Neural Networks for Machine Translation Kim, Ki Hyun.Natural Language Processing with PyTorch.seoul:Hanbit Publishing Network,2019 × Search',\n",
       "  'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/'},\n",
       " {'text': 'GloVe는 기존의 단어 embedding 방법의 한계를 개선하고자 제안된 방법이다. 기존의 단어 embedding방법은 크게 Matirx Factorization Method와 Shallow Window-Based Method가 있다. 우선 전자에 해당하는 것에는 대표적으로 LSA가 있는데 이 방법은 카운트 기반으로, corpus의 전체 통계 정보도 고려하고 학습도 빠르지만, 단어 간의 관계를 통해 의미를 유추하거나 단어간의 유사도를 구하기 어렵다는 한계를 지니고 있다. 그리고 후자에 해당하는 방법에는 NNLM과 Word2Vec(CBOW, Skip-gram)이 있는데 이 방법은 단어 간의 유사도도 구할 수 있고 성능도 좋지만 window를 이용해 주변 단어들만 살피기 때문에 전체 corpus의 통계정보를 반영하지 못한다는 문제가 있다. GloVe는 위와 같이 이전에 제안된 word embedding 방법들의 한계점들을 단어간의 동시 등장 확률(co-occurrence probapility)를 사용하여 개선하였다. Co-occurrence Matrix 우선 단어 간의 등장 확률을 구하기 위해 같은 문맥에서 함께 등장한 단어들의 등장 횟수를 기록한 동시 등장 행렬(Co-occurrence matrix) $X(V \\\\times V)$를 만든다: Ich Keks liebe fliegen Haribo geniessen Leben Ich 0 2 1 0 0 0 0',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': 'Keks 2 0 0 1 0 1 0 liebe 1 0 0 0 0 0 1 fliegen 0 1 0 0 1 0 0 Haribo 0 0 0 1 0 0 0 geniessen 0 1 0 0 0 0 0 Leben 0 0 1 0 0 0 0 아래는 center word $i$가 속한 context에 등장하는 전체 단어의 등장 횟수를 표현한 수식이다: \\\\begin{matrix} X_i=\\\\sum_k X_{ik} \\\\end{matrix} Objective function - Co-occurrence probability 위 행렬$(X)$을 기반으로 동시 등장 확률(co-occurrence probability)을 구한다. \\\\begin{matrix} P_{ij}=P(j|i)=\\\\frac{X_{ij}}{X_j} \\\\end{matrix} - Ratios of co-occurrence probabilities 위 수식은 center word $i$에 대해 context word $j$가 등장할 확률을 표현한 것이다. 해당 수식에서 $X_{ij}$는 context word $j$가 center word $i$가 속한 context에 등장한 횟수를 가리킨다. 이렇게 정의한 확률을 사용하여 두 단어(각각 독립적인 단어 $i$와 $j$) 간의 상관관계를 파악하기 위해 임의의 단어 $k$에 대한 동시 등장 확률 비율(ratios of co-occurrence probabilities)을 정의해보도록 하겠다. $i$와 $j$에 대해 그들 간의 동시 등장 확률이 아닌 동시 등장 확률의 비율이 필요한 이유를 설명하기 위해 $i$를 ice로, $j$를 steam으로 가정하여 간단한 예시를 들도록 하겠다.',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': '위 표는 지금부터 설명할 예시의 결과이다. 해당 표를 보면, ‘ice’와는 관련 있지만 ‘steam’과는 무관한 ‘solid’를 $k$로 정의한 경우, ‘ice’가 주어졌을 때 ‘solid’가 등장할 확률이 ‘steam’이 주어졌을 때 ‘solid’가 등장할 확률보다 높은 것을 확인할 수 있다. 이에 따라 확률들의 비율인 $P(solid|ice)/P(solid|steam)$의 값이 8.9로 굉장히 높게 나오게 된다. 다음으로, ‘steam’과 관련 있는 단어 ’gas’를 $k$로 정의한 경우에는 확률 비율이 이전보다 확연히 작아진 것을 볼 수 있다. 마지막으로, 세 번째 결과와 네 번째 결과는 비율이 모두 1에 가깝게 도출되었는데 이는 ‘ice’와 ‘steam’ 모두 관련 있는 단어 ‘water’와 두 단어 모두와 관련 없는 단어 ‘fashion’에 대한 결과이다. 이렇게 간단한 예시를 통해 동시 등장 확률이 아닌 동시 등장 확률 비율이 독립적인 두 단어의 관계를 더 잘 파악할 수 있게 해준다는 것을 알 수 있다.',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': '이제 위에서 설명한 바와 같이 세개의 단어($i$, $j$, $k$)간의 상호 비율 정보를 도출하기 위한 목적함수를 정의해 보겠다. 우선 $k$에 대한 $i$와 $j$의 등장 확률 비율 $F$는 다음과 같이 정의될 수 있다: \\\\begin{matrix} F({ w }_ { i },{ w }_ { j },\\\\tilde { { w }_ { k } } )=\\\\frac{ { P }_ { ik } }{ { P }_ { jk } } \\\\end{matrix} 위 수식에서 $w\\\\in\\\\mathbb{R}^d$은 word vector($i$, $j$)이고 $\\\\tilde{w}\\\\in\\\\mathbb{R}^d$는 separate context vector($k$)이다. 그리고 $F$는 확률에 대한 비율(ratio $\\\\frac { P_{ ik } }{ P_{ jk } }$)정보를 담고 있어야 하고, 해당 비율 정보는 단어 벡터 공간 안에 표현되어야 한다. 벡터 공간은 본질적으로 선형 구조이기에 비율 정보를 벡터 공간 안에 표현하는 것은 벡터 간의 차이로 이루어 낼 수 있다. 이러한 목적을 위해 위 수식을 아래와 같이 변형한다: \\\\begin{matrix} F({ w }_ { i }-{ w }_ { j },\\\\tilde{ { w }_ { k } })=\\\\frac{ { P }_ { ik } }{ { P }_ { jk } } \\\\end{matrix} 하지만 $F$의 인자 값은 vector인데 우변의 결과는 scalar값이다. 이 경우 $F$는 신경망을 혼란스럽게 만들기 때문에 아래 와 같이 좌변의 인자 값을 내적한 것으로 수식을 또 다시 수정한다: \\\\begin{matrix}',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': 'F(({ w }_ { i }-{ w }_ { j })^T\\\\tilde{ { w }_ { k } })=\\\\frac{ { P }_ { ik } }{ { P }_ { jk } } \\\\end{matrix} 여기에서 주의할 점은 word-word co-occurrence matrices에서 center word($w_i$, $w_j$)와 context word($\\\\tilde{ w }_{ k }$)는 임의로 결정되기 때문에 두 단어의 역할은 자유롭게 상호 교차($w\\\\leftrightarrow\\\\tilde{w}$)가 이루어질 수 있어야 하고, co-occurrence matrix $X$는 $X\\\\leftrightarrow X^T$관계여야 한다는 것이다. 최종 모델은 재라벨링(relabeling)을 해도 변하지 않아야 한다. 하지만 위에 정의된 마지막 수식은 그렇지 않다. 라벨링에 따라 변하지 않기 위해 우선 함수 $F$는 Homomorphism(준동형)해야 한다. 이 조건을 만족시키기 위해 식은 다시 다음과 같이 변형된다: \\\\begin{matrix} F((w_i-w_j)^T\\\\tilde{w}_ {k})=\\\\frac{F(w_i^T\\\\tilde{w}_ {k})}{F(w_j^T\\\\tilde{w}_ {k})} \\\\end{matrix} 위 식은 아래 식을 만족한다: \\\\begin{matrix} F(w_i^T\\\\tilde{w}_ {k})=P_{ik}=\\\\frac{X_{ik}}{X_i} \\\\end{matrix} 이제 마지막 단계로 함수 $F$를 정의해야 하는데 함수 $F$는 위에서 말했 듯이 Homomorphism이기 때문에 이 조건을 만족시키는 exponential function(지수함수)를 사용하여 $F=exp$로 정의할 수 있다. 이에 따라 함수 $F$를 $exp$로 치환하면 다음과 같이 정의될 수 있다: \\\\begin{matrix}',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': 'F(w_i^T\\\\tilde{w}_ {k})=P_{ik} \\\\end{matrix} \\\\begin{matrix} exp(w_i^T\\\\tilde{w}_ {k})=P_{ik} \\\\end{matrix} \\\\begin{matrix} w_i^T\\\\tilde{w}_ {k}=\\\\log(P_ {ik})=\\\\log(X_ {ik})-\\\\log(X_i) \\\\end{matrix} 하지만 위에서 언급했 듯이 center word($w_i$ ,$w_j$)와 context word($\\\\tilde{w}_k$)간의 상호 교차는 자유롭게 이루어질 수 있어야 하는데 위 수식의 $log(P_{ik})$는 $log(P_{ki})$과 상호 교차될 수 없다. 즉 전자와 후자는 각각$log(X_{ik})-log(X_i)$와 $log(X_{ki}) - log(X_k)$인데 두 수식이 서로 다르기 때문에 상호 교차가 불가능하다. 이를 해결하기 위해 $log(X_i)$에 대해 $w_i$는 bias $b_i$로, $\\\\tilde{ w }_{ k }$는 bias $\\\\tilde{ b }_{ k }$로 처리한 후 각 변에 해당 상수항을 다시 더한다: \\\\begin{matrix} { w }_ { i }^{ T }\\\\tilde { { w }_ { k } } =\\\\log { X_ { ik }-{ b }_ { i }-\\\\tilde { { b }_ { k } } } \\\\end{matrix} \\\\begin{matrix} { w }_ { i }^{ T }\\\\tilde { { w }_ { k } } +{ b }_ { i }+\\\\tilde { { b }_ { k } } =\\\\log { X_ { ik }} \\\\end{matrix} 이제 $d$차원 벡터 공간에 우변과의 차이를 최소화한 좌변의 값이 embedding된다. 즉 $d$차원 벡터 공간 안에 동시 등장 확률이 높은 단어 vector간의 거리는 가깝게, 낮은 단어 vector간의 거리는 멀게 배치된다. 이를 수식으로 정의하면 다음과 같다: \\\\begin{matrix}',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': 'J=\\\\sum _ { i,j=1 }^{ V }{ { ({ w }_ { i }^{ T }\\\\tilde { { w }_ { j } } +{ b }_ { i }+\\\\tilde { { b }_ { j } } -\\\\log { X_{ ij } } ) }^{ 2 } } \\\\end{matrix} 하지만 위 수식의 $log{X}_{ik}$부분에서 문제가 발생한다. $log{X}_{ik}$은 co-occurrence matrix에 로그를 취한 것인데, 행렬 값이 0이 될 경우에는 $log0$이 되어 무한 발산을 하게 된다. 그리고 적게 등장하는 단어와 너무 자주 등장하는 단어에 대한 값을 안정시키기 위해 weighting function $f(x)$를 추가하여 최종 목적함수(objective function)을 만든다: \\\\begin{matrix} J=\\\\sum _ { i,j=1 }^{ V }{ { f\\\\left( { X }_ { ij } \\\\right) ({ w }_ { i }^{ T }\\\\tilde { { w }_ { j } } +{ b }_ { i }+\\\\tilde { { b }_ { j } } -\\\\log { X_ { ij } } ) }^{ 2 } } \\\\ where\\\\quad f(x)=\\\\begin{cases} { (\\\\frac { x }{ { x }_ { max } } ) }^{ \\\\alpha } \\\\ 1\\\\quad otherwise \\\\end{cases}if\\\\quad x<{ x }_ { max } \\\\end{matrix} Model Training 마지막으로 이제 학습을 진행할 것인데 우선 행렬 $w_{i}^{T}$와 $\\\\tilde{ w }_{j}$를 랜덤 초기화시킨 후 gradient descent를 사용해 최종 목적함수를 최소화시키는 방향으로 ${w}_{i}^{T}$와 $\\\\tilde{ w }_{j}$를 업데이트하며 학습을 진행한다.즉 행렬분해를 통해 최종 목적함수를 최소화하며 학습을 진행한다. 학습 후 나온 vector $w$를 embedding vector로 사용하면 된다.',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': 'Illustrated by the author Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Jeffrey Pennington, Richard Socher, Christopher D. Manning.”GloVe: Global Vectors for Word Representation,”(2014) Lee,Gichang.Sentence embeddings using korean corpora.seoul:acornpub,2019 × Search',\n",
       "  'source_title': 'GloVe(Global Word Vectors)\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/GloVe/'},\n",
       " {'text': 'FastText는 2015년 Facebook에서 발표한 라이브러리로, Word2Vec과 같이 distribution representation을 이용하여 비슷한 의미를 지닌 단어들은 유사한 word representation vector를 갖도록 하는 embedding방법이다. 해당 라이브러리는 Word2Vec을 제안한 T. Mikolov가 빈번히 등장하지 않는 단어(infrequent word)와 모르는 단어(OOV)에 대한 Word2Vec의 단점을 보완하여 새롭게 제시한 word representation방법이다. sampling train dataset',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': '발표된 라이브러리는 Word2Vec처럼 단어의 유사도를 cosine similarity를 사용하여 구하는 등 대부분의 내용은 Word2Vec과 유사하지만 단어를 word2vec보다 더 여러 부분으로 분리하여 다룬다는 차이점이 있다. 더 자세히 말하자면 Word2Vec은 단어를 분리 가능한 가장 작은 단위로 간주하여 한 단어의 vector값을 통으로 학습하지만, FastText는 분리 가능한 가장 작은 단위를 문자(character)로 간주하여 subword vector들의 합으로 단어를 표현한다. 따라서 FastText는 word vector가 아닌 subword vector를 학습하는 것이라 말할 수 있다. 즉 한 단어를 설정한 window size에 따라 문자 단위로 분리한 후, 그것들의 합으로 단어를 표현하여 분리된 각 vector들을 학습한다. 예를 들어 window size를 3으로 정한다면 하나의 단어를 다음과 같이 tri-gram으로 나타내게 된다(FastText모델에서 ‘<’와 ’>’는 각각 단어의 시작과 끝을 나타내는 특수 기호인데 이 기호를 포함하여 subword를 분리시켜야 한다.):',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': '위와 같이 FastText모델에서 “백설공주”라는 단어는 단어르 window size에 따라 분리시킨 것과 “백설공주” 전체를 embedding한 것의 합으로 표현된다. 이는 다음과 같이 정의될 수 있다: \\\\begin{matrix} { u }_ { 백설공주 }={ z }_ { <백설 }+{ z }_ { 백설공 }+{ z }_ { 설공주 }+{ z }_ { 공주> }+{ z }_ { <백설공주> } \\\\end{matrix} \\\\begin{matrix} u_t=\\\\sum_{ {g} \\\\in { G_t } }{z_g} \\\\end{matrix} ($G_t$는 target word($t$)에 속한 문자 단위 n-gram집합을 가리킨다.) Model Training \\\\begin{matrix} L(\\\\theta)=logP\\u2061(+|{ c }_ { P },{ o }_ { p })+\\\\sum_{i=1}^klogP\\u2061(-|{ c }_ { n_i },{ o }_ { n_i }) \\\\end{matrix} 위 식은 한 iteration마다 모델 파라미터($\\\\theta$)가 업데이트되는데 이때 한 쌍의 positive sample ($c_P$, $o_p$ : 실제 같이 쓰이는 단어쌍들의 집합)과 $k$쌍의 negative sample($c_{n_i}$, $o_{n_i}$ : 코퍼스 전체에서 랜덤 추출한 함께 사용되지 않은 단어 쌍)이 학습된다는 것을 표현한 log-likelihood function이다. Skip-gram의 log-likelihood function과 같지만 center word($c$)와 context word($o$)를 학습할 때 center word로부터 나온 문자(character)단위의 n-gram vector인 $z$의 등장으로 Skip-gram과 확률이 다르게 정의된다.',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': '위 식에서 $P(+|c_P,o_p)$는 FastText Model에서 positive sample인 center word($c$)가 output context word($o$)와 함께 쓰일 확률이다. 확률은 다음과 같이 정의될 수 있다: \\\\begin{matrix} P(+|c_P,o_p)=\\\\frac{ 1 }{ 1+exp\\u2061({ -u }_ { c }{ v }_ { o }) }=\\\\frac{ 1 }{ 1+exp\\u2061(-\\\\sum_{ {g} \\\\in { G_t } }z_g^{\\\\intercal}v_o)} \\\\end{matrix} 해당 모델의 목표는 예측된 positive sample인 center word와 context word쌍이 positive일 확률이 높이는 것이기 때문에 위 식의 결과를 최대화하는 방향으로 학습이 진행돼야 한다. 이를 위해 분모지수를 최소화해야 하는데 이는 벡터 $z$과 $v_o$의 내적 값을 높임으로써 가능하다. 벡터의 내적 값은 cosine similarity와 비례하기 때문에 내적 값이 커지면 벡터 간의 유사도 또한 높아지는 결과를 얻을 수 있다. 예를 들어 ‘백설공주’가 $c$이고 ‘먹었다’가 $o$일때 $z$는 “<백설”, “백설공”, “설공주”, “공주>”, “<백설공주>”이니, $z$와 $v_o$($o$에 해당하는 단어벡터)간의 유사도를 높이는 것이 해당 모델의 학습 목표라 생각하면 된다.',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': '$L(\\\\theta)$를 구하는 식에서 $P\\u2061(-| c_{n_i},o_{n_i})$는 negative sample인 $c_n$ 와 $o_n$가 negative sample일 확률이다. 따라서 negative sample을 negative sample이라고 얼마나 잘 맞췄는지에 대한 확률이기 때문에 이 값 또한 최대화하는 방향으로 학습이 이루어져야 한다. 확률은 다음과 같이 정의될 수 있다: \\\\begin{matrix} P(-|c_n,o_n)=1- P(+|c_n,o_n)=\\\\frac{ exp\\u2061({ -u }_ { c }{ v }_ { o }) }{ 1+exp\\u2061({ -u }_ { c }{ v }_ { o }) }=\\\\frac{ exp\\u2061(-\\\\sum_{ {g} \\\\in { G_t } }z_g^{\\\\intercal}v_o) }{ 1+exp\\u2061(-\\\\sum_{ {g} \\\\in { G_t } }z_g^{\\\\intercal}v_o)} \\\\end{matrix} 해당 식을 최대화 하기 위해서는 우변의 분자지수는 키우고 분모지수는 줄여야 한다. 이를 위해서 $z$와 $v_o$의 내적 값을 줄여야 하는데 위에서 언급했 듯 벡터의 내적은 cosine similarity와 비례하므로 두 벡터의 내적 값이 줄어들면 두 벡터의 유사도 또한 낮아진다.',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': '이렇게 $L(\\\\theta)$를 최대화하는 학습 과정은 positive sample 단어쌍의 유사도는 높이고 negative sample 단어쌍의 유사도는 낮추는 방향으로 학습이 진행된다. 결국 FastText도 Skip-gram과 같이 center word와 주변 단어 쌍이 positive sample에 속하는지 negative sample에 속하는지 이진분류(binary classification)하는 과정에서 학습이 진행되고 학습을 통해 도출된 word embedding vector는 유사성에 관한 정보를 지니게 되는 것이다. Reference',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': 'Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Lee,Gichang.Sentence embeddings using korean corpora.seoul:acornpub,2019 P. Bojanowski, E. Grave, A. Joulin, T. Mikolov. Enriching Word Vectors with Subword Information,2017 × Search',\n",
       "  'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/'},\n",
       " {'text': '위 그림을 보면 Skip-gram모델은 입력 층 - 가중치행렬$W$ - hidden layer - 가중치 행렬 $W’$ – 출력 층으로 이루어져 있다. - Parameter 그림(1)에서 $W$의 차원과 $W’$의 차원은 서로 전치된 차원이라는 것을 확인할 수 있는데 $W$는 center word vector에 대응하는 행렬이고, $W’$는 output context word vector와 대응하는 행렬로 $W$와 $W’$는 모양만 전치된 형태일 뿐 다른 행렬이다. 그림을 보면 input layer의 차원은 $1 \\\\times V$($V$= 임베딩하려는 단어의 수)이고 hidden layer 노드 수($N$)는, 곧 embedding 차원 수로, 사용자가 임의로 설정이 가능하다. input layer의 차원과 hidden layer의 각 노드가 1:1대응이 되야 하기 때문에 그들 가운데에 위치한 $W$의 차원은 $V \\\\times N$이 된다. - skip-gram model works 이제 모델의 전체적인 구조가 진행되는 과정을 수식으로 살펴보겠다. 1. 우선 Center word를 one-hot-vector로 만든다. (문서 $d$에 사용된 embedding하려는 단어들을 모두 추출하여 one-hot-encoding한다.) \\\\begin{matrix} x\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} 2. Embedding matrix $W$와 이전에 one-hot-encoding한 center word를 곱하여 embedded word vector를 구한다. \\\\begin{matrix} v_c=\\\\mathbf{W}x\\\\in\\\\mathbb{R}^n \\\\end{matrix}',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': '이 작업은 embedding matrix W에서 해당 단어에 해당하는 행(column)을 look up해 오는 것이라 말할 수 있다.',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': '3. Embedded vector를 context matrix $W’$와 곱해서 score vector를 구한다. \\\\begin{matrix} z=\\\\mathbf{W}^{\\\\prime}v_c \\\\end{matrix} 4. Score vector를 확률 값으로 만든다. \\\\begin{matrix} \\\\hat{y}=\\\\text{softmax}(z) \\\\end{matrix} 5. $\\\\hat{y}$(예측 값(확률 값)과 각 위치의 $y$(정답 값)를 cross-entropy(loss-function) $H$를 사용하여 loss를 줄여가며 학습한다. \\\\begin{matrix} J=\\\\sum^{2m}_ {i}H(\\\\hat{y},y_{i}) \\\\end{matrix} Model Training 이제 모델의 학습이 진행되는 과정에 대해 설명하겠다. 해당 모델 학습의 목표는 다음 식을 최대화하는 것이다: \\\\begin{matrix} L(\\\\theta)=logP\\u2061(+|{ c }_ { P },{ o }_ { p })+\\\\sum_{i=1}^klogP\\u2061(-|{ c }_ { n_i },{ o }_ { n_i }) \\\\end{matrix} 위 식은 한 iteration마다 모델 파라미터($\\\\theta$)가 업데이트되는데 이때 한 쌍의 positive sample ($c_P$, $o_p$ : 실제 같이 쓰이는 단어쌍들의 집합)과 $k$쌍의 negative sample($c_{n_i}$, $o_{n_i}$ : 코퍼스 전체에서 랜덤 추출한 함께 사용되지 않은 단어 쌍)이 학습된다는 것을 표현한 log-likelihood function이다. 위 수식에서 $P(+|c_P, o_p)$는 positive sample에서 center word($c$)가 output context word($o$)와 함께 쓰일 확률이다. 이 확률은 다음과 같이 정의될 수 있다: \\\\begin{matrix} P(+|c_P,o_p)=\\\\frac{ 1 }{ 1+exp\\u2061({ -u }_ { c }{ v }_ { o }) } \\\\end{matrix}',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': '해당 모델의 목표는 예측된 positive sample인 center word와 context word쌍이 positive일 확률이 높이는 것이기 때문에 위 식의 결과를 최대화하는 방향으로 학습이 진행돼야 한다. 해당 수식에서 $u_c$는 $W$의 column vector, 그리고 $v_o$는 $W’$의 row vector이다. 이 수식을 최대화하기 위해서는 분모인 $1+exp\\u2061(-u _cv_o)$값을 줄여야 한다. 따라서 두 벡터($u_c$ 와 $v_o$)의 내적 값을 키워야 하는데 해당 벡터의 내적은 곧 cosine similarity라 할 수 있으므로 해당 값이 커지면 positive sample에서 두 벡터간의 유사도가 높아지는 것과 같다. $L(\\\\theta)$를 구하는 식에서 $P\\u2061(-| c_{n_i},o_{n_i})$는 negative sample인 $c_n$ 와 $o_n$가 negative sample일 확률이다. 따라서 negative sample을 negative sample이라고 얼마나 잘 맞췄는지에 대한 확률이기에 이 값 또한 최대화해야 한다. 해당 확률은 다음과 같이 정의될 수 있다: \\\\begin{matrix} P(-|c_n,o_n)=1- P(+|c_n,o_n)=\\\\frac{ exp\\u2061(-u_c v_o ) }{ 1+exp\\u2061(-u_c v_o ) } \\\\end{matrix}',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': '위 식을 최대화 하기 위해서는 우변의 분자지수는 키우고 분모지수는 줄여야 한다. 따라서 $u_c$ 와$v_o$의 내적 값을 줄여야 하는데 위에서 언급했 듯이 벡터의 내적은 cosine similarity와 비례하므로 두 벡터의 내적 값이 줄어들면 두 벡터의 유사도 또한 낮아진다. 지금까지 log-likelihood function($L(\\\\theta)$)을 최대화하는 학습 과정에 대해 살펴보았다. 요약하여 말하자면, log-likelihood function을 최대화함으로써 positive sample 단어쌍의 유사도는 높이고 negative sample 단어쌍의 유사도는 낮추는 방향으로 학습이 진행된다. 결론적으로 Skip-gram은 center word와 주변 단어 쌍이 positive sample에 속하는지 negative sample에 속하는지 이진분류(binary classification)하는 과정에서 학습되는 것이고 이 과정을 통해 도출된 word embedding vector는 유사성에 관한 정보를 지니게 되는 것이다. sampling train dataset',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': 'Illustrated by the author',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': '위 그림은 “Das Jahr ist irgendwie so schnell vergangen.”이라는 문장에서 window size(한번에 학습할 단어의 개수)가 2일 경우 문장의 시작부터 끝까지 window가 옮겨 다니며 학습이 진행되는 것을 표현한 그림이다. 우선 문장의 시작인 ‘Das’부터 시작하여 이를 center word로 설정한 후, 해당 단어를 기준으로 ‘Jahr’ 와 ‘ist’가 각각 (Das, Jahr), (Das, ist)와 같이 각각 따로 학습된다. 이 부분이 skip-gram과 CBOW의 차이점 중 하나이자 skip-gram이 CBOW보다 성능이 높은 이유 중 하나이다. CBOW는 ‘Jahr’와 ‘ist’가 한번에 학습되어 하나의 center word당 한번의 학습이 이루어지는 반면, skip-gram의 경우 하나의 center word마다 설정한 window size만큼 학습이 진행되기 때문에 계속 업데이트 되며 학습이 진행된다. 즉 이전에 학습한 것에 대한 개선이 이루어지기 때문에 CBOW보다 Skip-gram의 성능이 더 좋다. Tricks 이제 Skip-gram의 성능 향상을 위해 사용된 세가지 기법에 대해 소개하겠다. - Negative sampling',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': 'Word2vec모델은 출력층으로부터 나온 score vector를 softmax에 통과시켜 확률 값으로 만든 후 이를 각 위치의 정답 값과 비교하며 역전파(backpropagation)하는 구조를 지닌다. 해당 과정에서 softmax regression적용을 위해 center word와 문서 내의 다른 모든 단어들을 모두 내적한 후 다시 $exp$를 취하게 되는데 이때 계산량이 매우 커지게 된다. 즉, 정답 context word가 나올 확률은 올리고 나머지 단어들이 나올 확률은 낮추며 학습을 하는데 데이터가 매우 큰 경우 이 확률들을 모두 계산할 때 계산량이 매우 커지게 된다. 따라서 softmax를 적용할 때 전체 단어를 대상으로 확률 값을 구하지 않고 일부 단어만 뽑아서 계산을 하는 negative sampling기법을 사용하여 계산량을 줄였다. 해당 기법을 적용하기 위해서는 positive sample과 negative sample이 필요하다. Positive sample은 이전에 설명한 바와 같이 코퍼스 전체를 설정한 window size만큼 고려하여 슬라이딩 하며 학습 데이터를 만든다. 즉, center word 앞뒤로 설정한 window size만큼 보면서 전체 코퍼스를 훑는다는 것이다. 다음은 window size를 2로 설정하여 positive sample을 만든 것이다:',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': 'c (center word) o (output context words) 커피 카페 커피 에서 커피 와 커피 빵 아래는 Negative sample인데, Negative sample은 코퍼스 전체에서 랜덤 추출한 center word와 함께 등장하지 않은 단어의 데이터 쌍이다: c (center word) o (output context words) 커피 뭐 커피 컴퓨터 커피 종이 커피 자동차',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': '이렇게 positive sample과 negative sample을 생성한 후, 각 sample들을 합쳐 전체 단어처럼 사용하여 softmax 확률을 구한다. 이렇게 학습하면 1개의 positive sample과 $k$개의 negative sample만 계산하면 되기 때문에 매 step마다 전체 단어를 모두 계산해야 하는 방법 보다 계산 량이 훨씬 줄어든다. 위 예시에서는 $k$를 2로 설정한 것인데, 이 경우 모델은 한 스텝에서 1개의 positive sample(커피-카페)와 2개의 negative sample(커피-자동차, 커피-사람), 총 3개만 학습하면 된다. Negative sampling 확률은 아래와 같다.($f(w_i)$는 해당 단어가 코퍼스에 등장한 비율(해당 단어 빈도/전체 단어 수)이다.) \\\\begin{matrix} P({ w }_ { i })=\\\\frac { { f({ w }_ { i }) }^{ { 3 }/{ 4 } } }{ \\\\sum _ { j=0 }^{ n }{ { f({ w }_ { j }) }^{ { 3 }/{ 4 } } } } \\\\end{matrix} - Subsampling frequent words',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': 'Subsampling 기법은 위에서 설명한 negative sampling기법과 비슷해 보일 수 있으나 다른 기법이다. 이 기법 또한 계산량을 줄이기 위해 사용되는 방법인데, negative sampling과는 달리 등장 빈도가 높은 단어들에 대한 학습량을 줄여 계산량을 감소시킨 기법이다. 모든 단어 쌍에 대해 Training set을 만들지 않고 자주 등장하는 단어를 무작위로 training set에서 제외시켜 계산량을 줄였다. 즉, 고빈도의 단어에 대한 학습량만 줄이고 등장 빈도가 낮은 단어는 제외 시키지 않고 모두 학습한다 이 수식은 subsampling 확률이다. \\\\begin{matrix} { P }_ { subsampling }({ w }_ { i })=1-\\\\sqrt { \\\\frac { t }{ f({ w }_ { i }) } } \\\\end{matrix} - Hierarchical softmax',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': 'Illustrated by the author Hierarchical softmax도 계산량 줄이기 위해 사용하는 기법인데, 일반 softmax보다 계산량을 줄이는 기법 중 하나이다. Softmax를 모두 계산하지 않고 tree구조로 hierarchical하게 softmax를 계산하는 것이다. Word2Vec은 python의 gensim패키지에 구현되어 있다. 이전 연구들의 문제점을 많이 개선한Word2Vec에도 몇 가지 문제점이 존재한다. 우선 단어 집합 내에 적게 등장한 단어(infrequent words)에 대한 embedding 정확도가 높지 않고 모르는 단어(out of vocabulary/OOV)에 대해서는 embedding vector가 없으므로 유사도 계산이 불가능하다는 문제점이 존재한다. 따라서 Word2Vec의 이러한 단점을 보완하기 위해 후속 연구로 Sentence2Vec과 Paragraph2Vec이 연구되었다. Word2Vec 시각화 : https://ronxin.github.io/wevi/, https://projector.tensorflow.org/ Word2Vec 예시 : http://word2vec.kr/search/ Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Lee,Gichang.Sentence embeddings using korean corpora.seoul:acornpub,2019 Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean.”Efficient Estimation of Word Representations in Vector Space,”(2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean.”Distributed Representations of Words and Phrases and their Compositionality,”(2013) × Search',\n",
       "  'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/'},\n",
       " {'text': 'NLP Basics 실습 코드: https://github.com/finddme/Finddme_Blog_Code/blob/master/NLP_Code/Sentimental_Categorize1.ipynb 홍종선(2009)에서는 의미적 차원과 표현 구조를 검토하여 감정동사들을 총19유형(감동, 감사, 기쁨, 고뇌, 고독, 놀람, 동정, 두려움, 미안함, 분노, 상쾌함,수용, 수치심, 슬픔, 욕구, 이완, 허무, 혐오, 후회)으로 분류했다. 표현 구조를고려하여 분류된 감정동사들의 벡터 값이 유사할 것이라는 가정 하에 Word2Vec을 활용하여 이를 검토해보고자 한다. 데이터는 AI 허브에서 제공하는 감성분석 데이터에서 content만 따로 추출하여 데이터로 사용한다. import codecs from bs4 import BeautifulSoup from konlpy.tag import Okt from gensim.models import word2vec text = codecs.open(\"sent.content.txt\", \"r\", encoding = \"utf-8\") f = open(\"sent.content.txt\", \"r\", encoding=\"utf-8\") print(f.read(20)) 곧 만기 은퇴 앞두다 있다 노후 준비 word_dict = {} lines = f.read().split(\"\\\\n\") print(lines[10]) 친구 내 자다 되다 건 다 너 덕분 고맙다 문자 하다 통 넣다 morph_analyzer = Okt() morph_analyzer1 = Okt() for line in lines: morph_analysed = morph_analyzer.pos(line) for word in morph_analysed: if word[1] == \"Noun\": if not (word[0] in word_dict): word_dict[word[0]] = 0 word_dict[word[0]] += 1',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': 'C:\\\\Users\\\\yein4\\\\Anaconda3\\\\lib\\\\site-packages\\\\jpype\\\\_core.py:210: UserWarning: ------------------------------------------------------------------------------- Deprecated: convertStrings was not specified when starting the JVM. The default behavior in JPype will be False starting in JPype 0.8. The recommended setting for new code is convertStrings=False. The legacy value of True was assumed for this session. If you are a user of an application that reported this warning, please file a ticket with the developer. ------------------------------------------------------------------------------- \"\"\") keys = sorted(word_dict.items(), key=lambda x:x[1], reverse=True) print(keys[:10]) [(\\'것\\', 7900), (\\'수\\', 6570), (\\'친구\\', 5653), (\\'내\\', 5178), (\\'나\\', 3815), (\\'상황\\', 3586), (\\'기분\\', 3182), (\\'더\\', 3182), (\\'생각\\', 3003), (\\'말\\', 2977)] for word, count in keys[:20]: print(\"{0}{1}\".format(word, count)) f.close() 것7900 수6570 친구5653 내5178 나3815 상황3586 기분3182 더3182 생각3003 말2977 마음2867 지금2799 안2658 일2477 이2434 못2432 사람2352 게2351 거2128 정말2059 with open(\"sent.content.txt\", \"r\", encoding=\"utf-8\") as fr: results = [] lines = fr.read().split(\"\\\\n\") for line in lines: morph_analysed = morph_analyzer.pos(line, norm=True, stem=True) # stem=True로 어근 출력 r = [] for word in morph_analysed: if not word[1] in [\"Josa\", \"Punctuation\", \"Foreign\", \"Suffix\", \"Eomi\"]: r.append(word[0]) # 결과에서 제외 할 품사 입력하기 #print(r) rl = (\" \".join(r)).strip() #공백제거 results.append(rl) print(results[:4])',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '[\\'곧 만기 은퇴 앞두다 있다 노후 준비 미리 꼼꼼하다 해 두 어서 기쁘다\\', \\'노후 준비 미리 꼼꼼하다 해 두 어서 기쁘다\\', \\'이렇게 노후 준비 하다 수 있다 건 옆 도와주다 친구 덕분 이 고마움 뭐 갚다\\', \\'노후 준비 도운 친구 고마움 느끼다 있다 상대방 고마움 표현 하다 수 있다 좋다 방법 있다\\'] data_file = \"sent.content.txt\" with open(data_file, \"w\", encoding=\"utf-8\") as fw: fw.write(\"\\\\n\".join(results)) from gensim.models import word2vec data = word2vec.LineSentence(data_file) print(data) model = word2vec.Word2Vec(data, size=100, window=10, hs=1, min_count=2, sg=1) # CBOW, Skip-gram(0) model.init_sims(replace=True) #필요없는 메모리는 unload #model.save(\"senti_cat.model\") print(\"done\") <gensim.models.word2vec.LineSentence object at 0x000001E2CC7345C8> done ##model = word2vec.Word2Vec.load(\"senti_cat.model\") ##print(model.similarity(\"기쁘다\", \"슬프다\")) ##print(model.similarity(\"기쁘다\", \"행복하다\")) print(model.most_similar(\"기쁘다\")) [(\\'행복하다\\', 0.6797296404838562), (\\'뿌듯하다\\', 0.6624724864959717), (\\'자랑스럽다\\', 0.6465882062911987), (\\'값지다\\', 0.6333721280097961), (\\'다행\\', 0.6318965554237366), (\\'최고\\', 0.6219849586486816), (\\'센스\\', 0.6149460077285767), (\\'만족스럽다\\', 0.6145679950714111), (\\'예식\\', 0.6099244356155396), (\\'축하\\', 0.5977829098701477)] C:\\\\Users\\\\yein4\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead). \"\"\"Entry point for launching an IPython kernel. from gensim.models import KeyedVectors model.wv.save_word2vec_format(\"senti_cat_model\")',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': \"# PCA 돌리기 위해 모델 텐서를 저장한다 #!python -m gensim.scripts.word2vec2tensor --input gimal_model --output gimal_model 차원 축소한 벡터를 통해 감정동사에 해당하는 동사들의 cosine distance를확인하였다. cosine distance는 1-cosine similarity로, 값이 작을수록 유사도가 높은 것이다. 각 동사들의 cosine distance 값은 다음과 같다: import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline data=pd.read_excel('./senti_cat.xlsx') pd.set_option('display.max_rows',500) data 기준동사분류 기준동사 관련동사분류 관련동사 CD 0 감동 NaN NaN NaN NaN 1 감사 고맙다 기쁨 기쁘다 0.544 2 감사 고맙다 미안함 미안하다 0.518 3 고뇌 걱정스럽다 고뇌 불안하다 0.531 4 고뇌 곤란하다 고뇌 난감하다 0.469 5 고뇌 괴롭다 고뇌 불안하다 0.545 6 고뇌 괴롭다 고뇌 착잡하다 0.569 7 고뇌 괴롭다 슬픔 슬프다 0.464 8 고뇌 괴롭다 혐오 속상하다 0.505 9 고뇌 괴롭다 두려움 무섭다 0.521 10 고뇌 괴롭다 두려움 두렵다 0.523 11 고뇌 괴롭다 슬픔 우울하다 0.539 12 고뇌 괴롭다 슬픔 서럽다 0.541 13 고뇌 괴롭다 동정 안타깝다 0.591 14 고뇌 불안하다 고뇌 초조하다 0.343\",\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '15 고뇌 불안하다 고뇌 심란하다 0.584 16 고뇌 불안하다 고뇌 착잡하다 0.587 17 고뇌 불안하다 두려움 두렵다 0.470 18 고뇌 심란하다 슬픔 울적하다 0.549 19 고뇌 심란하다 혐오 속상하다 0.597 20 고뇌 싱숭생숭하다 놀람 얼떨떨하다 0.537 21 고뇌 암담하다 혐오 지긋지긋하다 0.526 22 고뇌 암담하다 두려움 무섭다 0.585 23 고뇌 초조하다 허무 허탈하다 0.602 24 고뇌 초조하다 슬픔 우울하다 0.609 25 고뇌 초조하다 슬픔 서글프다 0.614 26 고뇌 고통스럽다 동정 안쓰럽다 0.465 27 고뇌 고통스럽다 두려움 무섭다 0.590 28 고뇌 난감하다 고뇌 곤란하다 0.469 29 고뇌 난감하다 혐오 부담스럽다 0.620 30 고뇌 찝찝하다 혐오 언짢다 0.606 31 고독 쓸쓸하다 고독 외롭다 0.315 32 고독 쓸쓸하다 고독 허전하다 0.519 33 고독 쓸쓸하다 슬픔 우울하다 0.431 34 고독 쓸쓸하다 슬픔 서글프다 0.481 35 고독 쓸쓸하다 슬픔 울적하다 0.526 36 고독 쓸쓸하다 혐오 섭섭하다 0.555',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '37 고독 외롭다 고독 허전하다 0.456 38 고독 외롭다 슬픔 우울하다 0.440 39 고독 외롭다 슬픔 슬프다 0.449 40 고독 외롭다 슬픔 울적하다 0.486 41 고독 외롭다 슬픔 서글프다 0.549 42 고독 허전하다 슬픔 울적하다 0.473 43 고독 허전하다 슬픔 서글프다 0.558 44 고독 심심하다 두려움 무섭다 0.556 45 고독 심심하다 동정 딱하다 0.556 46 기쁨 기쁘다 기쁨 행복하다 0.342 47 기쁨 기쁘다 기쁨 뿌듯하다 0.368 48 기쁨 기쁘다 기쁨 자랑스럽다 0.421 49 기쁨 기쁘다 수용 만족스럽다 0.481 50 기쁨 기쁘다 감사 고맙다 0.544 51 기쁨 들뜨다 수용 설레다 0.539 52 기쁨 뿌듯하다 기쁨 기쁘다 0.368 53 기쁨 뿌듯하다 기쁨 자랑스럽다 0.409 54 기쁨 뿌듯하다 기쁨 행복하다 0.494 55 기쁨 뿌듯하다 수용 만족스럽다 0.482 56 기쁨 뿌듯하다 수용 대견하다 0.535 57 기쁨 신나다 기쁨 행복하다 0.603 58 기쁨 신나다 수용 설레다 0.462 59 기쁨 자랑스럽다 기쁨 뿌듯하다 0.409',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '60 기쁨 자랑스럽다 기쁨 기쁘다 0.421 61 기쁨 자랑스럽다 수용 대견하다 0.381 62 기쁨 즐겁다 기쁨 행복하다 0.359 63 기쁨 즐겁다 수용 설레다 0.501 64 기쁨 행복하다 기쁨 기쁘다 0.342 65 기쁨 행복하다 기쁨 즐겁다 0.359 66 기쁨 행복하다 기쁨 뿌듯하다 0.494 67 기쁨 행복하다 수용 설레다 0.499 68 기쁨 행복하다 수용 흐뭇하다 0.566 69 기쁨 유쾌하다 수용 흐뭇하다 0.523 70 놀람 당혹스럽다 놀람 황당하다 0.607 71 놀람 당혹스럽다 슬픔 슬프다 0.647 72 놀람 당혹스럽다 고뇌 고통스럽다 0.664 73 놀람 황당하다 놀람 당혹스럽다 0.607 74 놀람 황당하다 혐오 부담스럽다 0.638 75 동정 딱하다 고독 심심하다 0.556 76 동정 불쌍하다 슬픔 처량하다 0.543 77 동정 불쌍하다 혐오 밉다 0.552 78 동정 안쓰럽다 동정 안타깝다 0.580 79 동정 안쓰럽다 고뇌 고통스럽다 0.465 80 동정 안쓰럽다 슬픔 비통하다 0.516 81 동정 안쓰럽다 슬픔 처량하다 0.574',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '82 동정 안쓰럽다 슬픔 슬프다 0.579 83 동정 안타깝다 동정 안쓰럽다 0.580 84 동정 안타깝다 슬픔 슬프다 0.432 85 동정 안타깝다 슬픔 비통하다 0.469 86 동정 안타깝다 혐오 속상하다 0.534 87 동정 안타깝다 슬픔 우울하다 0.536 88 동정 안타깝다 허무 허무 0.538 89 동정 안타깝다 고독 허전하다 0.567 90 동정 안타깝다 슬픔 울적하다 0.590 91 동정 안타깝다 고뇌 괴롭다 0.591 92 동정 안타깝다 슬픔 서글프다 0.596 93 동정 안타깝다 슬픔 억울하다 0.613 94 두려움 겁나다 슬픔 비통하다 0.577 95 두려움 두렵다 두려움 무섭다 0.344 96 두려움 두렵다 고뇌 불안하다 0.470 97 두려움 두렵다 고뇌 괴롭다 0.523 98 두려움 두렵다 슬픔 슬프다 0.586 99 두려움 무섭다 두려움 두렵다 0.344 100 두려움 무섭다 고뇌 괴롭다 0.521 101 두려움 무섭다 고독 심심하다 0.556 102 두려움 무섭다 고뇌 암담하다 0.585 103 두려움 무섭다 고뇌 고통스럽다 0.590',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '104 두려움 무섭다 슬픔 비통하다 0.596 105 두려움 무섭다 미안함 죄송하다 0.596 106 미안함 미안하다 미안함 죄송하다 0.586 107 미안함 미안하다 감사 고맙다 0.518 108 미안함 미안하다 혐오 서운하다 0.582 109 미안함 미안하다 혐오 속상하다 0.633 110 미안함 죄송스럽다 미안함 죄송하다 0.440 111 미안함 죄송스럽다 혐오 속상하다 0.528 112 미안함 죄송스럽다 혐오 서운하다 0.563 113 미안함 죄송스럽다 혐오 괘씸하다 0.580 114 미안함 죄송스럽다 슬픔 슬프다 0.589 115 미안함 죄송하다 미안함 죄송스럽다 0.440 116 미안함 죄송하다 미안함 미안하다 0.586 117 미안함 죄송하다 두려움 무섭다 0.596 118 분노 화나다 혐오 속상하다 0.482 119 분노 화나다 슬픔 억울하다 0.544 120 분노 화나다 혐오 섭섭하다 0.593 121 분노 화나다 혐오 서운하다 0.608 122 상쾌함 개운하다 상쾌함 홀가분하다 0.513 123 상쾌함 개운하다 상쾌함 상쾌하다 0.556 124 상쾌함 개운하다 복합감정 시원섭섭하다 0.592',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '125 상쾌함 상쾌하다 상쾌함 개운하다 0.556 126 상쾌함 홀가분하다 상쾌함 개운하다 0.513 127 상쾌함 홀가분하다 슬픔 서럽다 0.579 128 상쾌함 홀가분하다 허무 허무하다 0.603 129 상쾌함 후련하다 상쾌함 홀가분하다 0.660 130 상쾌함 후련하다 고뇌 암담하다 0.610 131 수용 대견하다 기쁨 자랑스럽다 0.381 132 수용 대견하다 기쁨 뿌듯하다 0.535 133 수용 만족스럽다 기쁨 기쁘다 0.481 134 수용 만족스럽다 기쁨 뿌듯하다 0.482 135 수용 만족스럽다 기쁨 행복하다 0.542 136 수용 설레다 기쁨 신나다 0.462 137 수용 설레다 기쁨 행복하다 0.499 138 수용 흐뭇하다 기쁨 유쾌하다 0.523 139 수용 흐뭇하다 기쁨 행복하다 0.566 140 수치심 민망하다 수치심 창피하다 0.557 141 수치심 민망하다 수치심 부끄럽다 0.587 142 수치심 민망하다 혐오 싫다 0.581 143 수치심 부끄럽다 수치심 창피하다 0.488 144 수치심 부끄럽다 수치심 민망하다 0.587 145 수치심 부끄럽다 혐오 싫다 0.530',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '146 수치심 비참하다 슬픔 비통하다 0.464 147 수치심 비참하다 슬픔 슬프다 0.485 148 수치심 비참하다 혐오 속상하다 0.569 149 수치심 창피하다 수치심 민망하다 0.557 150 수치심 창피하다 슬픔 비통하다 0.472 151 수치심 창피하다 혐오 싫다 0.568 152 수치심 참담하다 허무 허무하다 0.566 153 수치심 참담하다 허무 허탈하다 0.598 154 슬픔 비통하다 슬픔 슬프다 0.438 155 슬픔 비통하다 슬픔 서럽다 0.531 156 슬픔 비통하다 수치심 비참하다 0.464 157 슬픔 비통하다 수치심 창피하다 0.472 158 슬픔 비통하다 동정 안쓰럽다 0.516 159 슬픔 비통하다 상쾌함 홀가분하다 0.549 160 슬픔 비통하다 혐오 속상하다 0.581 161 슬픔 서글프다 슬픔 울적하다 0.518 162 슬픔 서글프다 슬픔 슬프다 0.548 163 슬픔 서글프다 허무 허무하다 0.436 164 슬픔 서글프다 고독 쓸쓸하다 0.481 165 슬픔 서글프다 고독 외롭다 0.549 166 슬픔 서글프다 고독 허전하다 0.558 167 슬픔 서럽다 슬픔 슬프다 0.413',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '168 슬픔 서럽다 슬픔 서글프다 0.520 169 슬픔 서럽다 슬픔 우울하다 0.525 170 슬픔 서럽다 슬픔 비통하다 0.531 171 슬픔 서럽다 혐오 서운하다 0.481 172 슬픔 서럽다 혐오 속상하다 0.487 173 슬픔 서럽다 고뇌 괴롭다 0.541 174 슬픔 서럽다 고뇌 고통스럽다 0.562 175 슬픔 서럽다 상쾌함 홀가분하다 0.579 176 슬픔 슬프다 슬픔 서럽다 0.413 177 슬픔 슬프다 슬픔 우울하다 0.457 178 슬픔 슬프다 슬픔 울적하다 0.487 179 슬픔 슬프다 슬픔 서글프다 0.548 180 슬픔 슬프다 슬픔 억울하다 0.615 181 슬픔 슬프다 혐오 속상하다 0.320 182 슬픔 슬프다 동정 안쓰럽다 0.432 183 슬픔 슬프다 혐오 서운하다 0.459 184 슬픔 슬프다 수치심 비참하다 0.485 185 슬픔 슬프다 고독 쓸쓸하다 0.538 186 슬픔 슬프다 동정 안쓰럽다 0.579 187 슬픔 억울하다 분노 화나다 0.554 188 슬픔 억울하다 슬픔 비통하다 0.573 189 슬픔 억울하다 슬픔 슬프다 0.615 190 슬픔 억울하다 혐오 속상하다 0.569',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '191 슬픔 억울하다 수치심 비참하다 0.607 192 슬픔 억울하다 동정 안타깝다 0.613 193 슬픔 우울하다 슬픔 울적하다 0.428 194 슬픔 우울하다 슬픔 슬프다 0.457 195 슬픔 우울하다 슬픔 서럽다 0.525 196 슬픔 우울하다 고독 쓸쓸하다 0.431 197 슬픔 우울하다 고독 외롭다 0.440 198 슬픔 우울하다 혐오 속상하다 0.485 199 슬픔 우울하다 동정 안타깝다 0.536 200 슬픔 우울하다 고뇌 괴롭다 0.539 201 슬픔 울적하다 슬픔 우울하다 0.428 202 슬픔 울적하다 슬픔 슬프다 0.487 203 슬픔 울적하다 슬픔 서글프다 0.518 204 슬픔 울적하다 고독 허전하다 0.473 205 슬픔 울적하다 고독 외롭다 0.486 206 슬픔 울적하다 혐오 속상하다 0.499 207 슬픔 울적하다 고독 쓸쓸하다 0.526 208 슬픔 울적하다 혐오 섭섭하다 0.548 209 슬픔 울적하다 고뇌 심란하다 0.549 210 슬픔 처량하다 혐오 지긋지긋하다 0.494 211 욕구 궁금하다 혐오 섭섭하다 0.525 212 욕구 부럽다 혐오 야속하다 0.633 213 이완 NaN NaN NaN NaN',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '214 허무 허무하다 허무 허탈하다 0.540 215 허무 허무하다 슬픔 서글프다 0.436 216 허무 허무하다 동정 안타깝다 0.538 217 허무 허무하다 혐오 지긋지긋하다 0.540 218 허무 허무하다 수치심 참담하다 0.566 219 허무 허탈하다 허무 허무하다 0.540 220 허무 허탈하다 수치심 참담하다 0.598 221 허무 허탈하다 고뇌 초조하다 0.602 222 혐오 괘씸하다 혐오 섭섭하다 0.566 223 혐오 괘씸하다 혐오 서운하다 0.573 224 혐오 괘씸하다 슬픔 서럽다 0.520 225 혐오 물리다 고독 쓸쓸하다 0.505 226 혐오 밉다 동정 불쌍하다 0.552 227 혐오 부담스럽다 고뇌 난감하다 0.620 228 혐오 부담스럽다 놀람 황당하다 0.638 229 혐오 서운하다 혐오 섭섭하다 0.330 230 혐오 서운하다 혐오 속상하다 0.354 231 혐오 서운하다 혐오 괘씸하다 0.573 232 혐오 서운하다 슬픔 슬프다 0.459 233 혐오 서운하다 슬픔 서럽다 0.481 234 혐오 서운하다 미안함 죄송스럽다 0.563 235 혐오 서운하다 미안함 미안하다 0.582',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '236 혐오 서운하다 슬픔 울적하다 0.583 237 혐오 섭섭하다 혐오 서운하다 0.330 238 혐오 섭섭하다 혐오 속상하다 0.508 239 혐오 섭섭하다 혐오 괘씸하다 0.566 240 혐오 섭섭하다 분노 화나다 0.593 241 혐오 속상하다 혐오 서운하다 0.330 242 혐오 속상하다 슬픔 슬프다 0.320 243 혐오 속상하다 분노 화나다 0.482 244 혐오 속상하다 슬픔 우울하다 0.485 245 혐오 속상하다 슬픔 서럽다 0.487 246 혐오 속상하다 슬픔 울적하다 0.499 247 혐오 속상하다 미안함 죄송스럽다 0.528 248 혐오 속상하다 동정 안타깝다 0.534 249 혐오 속상하다 슬픔 억울하다 0.569 250 혐오 속상하다 수치심 비참하다 0.569 251 혐오 속상하다 슬픔 비통하다 0.581 252 혐오 속상하다 후회 아쉽다 0.586 253 혐오 싫다 혐오 짜증나다 0.543 254 혐오 싫다 혐오 지긋지긋하다 0.548 255 혐오 싫다 수치심 부끄럽다 0.530 256 혐오 싫다 수치심 창피하다 0.568 257 혐오 싫다 수치심 민망하다 0.581 258 혐오 싫다 고뇌 괴롭다 0.622',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': \"259 혐오 언짢다 고뇌 찝찝하다 0.606 260 혐오 지겹다 혐오 지긋지긋하다 0.597 261 혐오 지긋지긋하다 슬픔 처량하다 0.494 262 혐오 지긋지긋하다 고뇌 암담하다 0.526 263 혐오 지긋지긋하다 허무 허무하다 0.540 264 혐오 짜증나다 혐오 싫다 0.543 265 후회 아쉽다 혐오 속상하다 0.586 266 복합감정 시원섭섭하다 상쾌함 개운하다 0.592 v=data['기준동사분류'].value_counts() v2=v.to_frame() #v2.to_excel('senti_cat_v2.xlsx') v2 기준동사분류 슬픔 57 혐오 43 고뇌 28 기쁨 24 동정 19 고독 15 수치심 14 미안함 12 두려움 12 수용 9 상쾌함 9 허무 8 놀람 5 분노 4 감사 2 욕구 2 후회 1 감동 1 이완 1 복합감정 1 size1=data.groupby(['기준동사분류','관련동사분류']).size() size2=size1.to_frame() size2 0 기준동사분류 관련동사분류 감사 기쁨 1 미안함 1 고뇌 고뇌 8 놀람 1 동정 2 두려움 5 슬픔 6 허무 1 혐오 5 고독 고독 3 동정 1 두려움 1 슬픔 9 혐오 1 기쁨 감사 1 기쁨 13 수용 10\",\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '놀람 고뇌 1 놀람 2 슬픔 1 혐오 1 동정 고뇌 2 고독 2 동정 2 슬픔 10 허무 1 혐오 2 두려움 고뇌 5 고독 1 두려움 2 미안함 1 슬픔 3 미안함 감사 1 두려움 1 미안함 4 슬픔 1 혐오 5 복합감정 상쾌함 1 분노 슬픔 1 혐오 3 상쾌함 고뇌 1 복합감정 1 상쾌함 5 슬픔 1 허무 1 수용 기쁨 9 수치심 수치심 5 슬픔 3 허무 2 혐오 4 슬픔 고뇌 4 고독 9 동정 5 분노 1 상쾌함 2 수치심 4 슬픔 21 허무 1 혐오 10 욕구 혐오 2 허무 고뇌 1 동정 1 수치심 2 슬픔 1 허무 2 혐오 1 혐오 고뇌 4 고독 1 놀람 1 동정 2 미안함 3 분노 2 수치심 4 슬픔 11 허무 1 혐오 13 후회 1 후회 혐오 1',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': '18개이 범주 중 10개의 범주에서 기준동사범주와 관련동사범주의 일치율이 높은 것으로 확인되는데, 이를 통해 홍종선(2009)에서 제시된 감정동사의 분류 기재와 결과가 유의미하다는 것을 알 수 있다. 또한 각 기준동사범주에 대해 특정 관련동사범주의 비율이 높게 나옴을 확인할 수 있는데, 이는 감정동사범주들 간의 유사성도 존재한다는 것을 의미한다. 따라서 이후 규모가 더 큰 데이터를 사용하여 범주들 간의 유사성을 검증한 후 이를 다시 범주화할 수 있을 것으로 보인다. 이렇게 범주를 다시 범주화 하는 것은 세밀하게 나눠진 인간 감정의 범주를 덜 세분화하여 컴퓨터가 인간이 구사한 문장 속의 감정동사를 통해 인간이 전달하고자 한 감정과 근접한 감정을 인식할 수 있는또 다른 방법이 될 수도 있을 것이라 생각된다. × Search',\n",
       "  'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/'},\n",
       " {'text': 'Word2Vec에서 embedding공간 내에서 단어가 유사도를 나타내는 기준은 cosine similarity이다. 이는 두 vector사이의 각도를 이용하여 유사도를 구하는 방식이며 cosine similarity값이 1에 가까울 수록 유사도가 높은 것이다. CBOW(Continuous Bag Of Words Model) CBOW는 context word(주변 단어)를 통해 center word를 예측하는 모델이다. CBOW에서는 우선 하나의 center word를 두고 몇 개의 context word를 볼 것인지 window size를 정한다. 따라서 window의 크기가 $m$이라면 center word 예측을 위해 참고하는 주변 단어의 개수는 $2m$개가 되는 것이다. 그리고 window를 sliding하며 train dataset을 만든다: 해당 모델의 input(${ x }^{ (c) }$)은 one-hot-encoding된 context words이고 output($y$)은 one-hot-encoding된 center word이다: Architecture of CBOW 모델의 전체적인 구조가 진행되는 과정을 살펴보겠다.',\n",
       "  'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/'},\n",
       " {'text': '1. 우선 하나의 center word에 대한 context words의 one-hot-vectors를 만든다: \\\\begin{matrix} (x^{ (c-m) },x^{ (c-m+1) },…,x^{ (c-1) },x^{ (c+1) },…x^{ (c+m-1) },x^{ (c+m) })\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} 2. 해당 모델의 파라미터는 input layer에서 hidden layer로 넘어가는 matrix $W$와 hidden layer에서 output layer로 넘어가는 matrix $W’$가 있다: \\\\begin{matrix} \\\\mathbf{W}\\\\in\\\\mathbb{R}^{|V|\\\\times N},~\\\\mathbf{W}^{\\\\prime}\\\\in\\\\mathbb{R}^{N\\\\times |V|} \\\\end{matrix} 위에서 만든 one-hot-vectors와 파라미터를 내적하여 context에 대한 embedded word vectors를 얻는다($W$는 input words에 대한 $n$차원의 embedding된 단어 vector들의 집합이다.): \\\\begin{matrix} (v_{ (c-m) }=\\\\mathbf{W}x^{ (c-m) },v_{ (c-m+1) }=\\\\mathbf{W}x^{ (c-m+1) }…,v_{ (c+m) }=\\\\mathbf{W}x^{ (c+m) })\\\\in\\\\mathbb{R}^n \\\\end{matrix} Input의 형태가 one-hot-vector이니 input과 파라미터의 내적은 아래 그림과 같이 파라미터에서 각 단어에 대한 행(column)을 look up해 오는 것이다. 즉, 파라미터와 내적하여 embedding된 vector를 불러오는 것이다.',\n",
       "  'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/'},\n",
       " {'text': '3. 앞서 얻은 Embedded vector들의 평균을 구하여 Hidden layer값을 구한다: \\\\begin{matrix} \\\\hat{v}=\\\\frac{v_{c-m}+v_{c-m+1}+\\\\dotsm+v_{c+m}}{2m}\\\\in\\\\mathbb{R}^n \\\\end{matrix} 4. Hidden layer값에 $W’$를 내적하여 score값을 구한다: \\\\begin{matrix} z=\\\\mathbf{W’}\\\\hat{v}\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} 5. 마지막으로 score값을 확률값으로 변환하기 위해 softmax를 적용시킨다: \\\\begin{matrix} \\\\hat{y}=softmax(z)\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} Model Training 이제 $\\\\hat{y}$(예측값)과 $y$(정답값)이 일치하는 방향으로 학습을 진행한다. 이는 파라미터들($W$, $W’$)의 학습을 통해 가능한데 학습을 위한 목적함수(objective function)는 loss fuction으로 cross entropy를 사용하여 다음과 같이 정의된다: \\\\begin{matrix} H(\\\\hat{y},y)=-\\\\sum^{|V|}_ {j=1}y_j\\\\log(\\\\hat{y_j}) \\\\end{matrix} 위에서 언급했 듯이 $y$는 하나의 one-hot-vector이다. 위 목적함수에서 $y_j$를 발견할 수 있는데 이 또한 하나의 one-hot-vector이기 때문에 위 수식을 더 간단히 하면 다음과 같이 표현될 수 있다: \\\\begin{matrix} H(\\\\hat{y},y)=-y_i\\\\log(\\\\hat{y}_ {i}) \\\\end{matrix}',\n",
       "  'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/'},\n",
       " {'text': '학습은 위 식의 결과를 최소화하는 방향으로 진행되는데 $H$($loss$)가 0에 가까울수록 예측이 잘 된 것이다(위 수식에서 $i$가 예측하고자 하는 단어이다). 목적함수를 최소화 시키는 것을 표현한 수식은 다음과 같다:',\n",
       "  'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/'},\n",
       " {'text': '\\\\[\\\\begin{align*} minimize J &=-\\\\log P(w_c|w_{c-m},...,w_{c+m})\\\\\\\\ &=-\\\\log P(u_c|v^)\\\\\\\\ &=-\\\\log \\\\frac{exp(u_c^{\\\\intercal}\\\\hat{v})}{\\\\sum^{|V|}_{j=1}exp(u_j^{\\\\intercal}\\\\hat{v})}\\\\\\\\ &=-u_c^{intercal}\\\\hat{v}+\\\\log\\\\sum^{|V|}_{j=1}exp(u_j^{\\\\intercal}\\\\hat{v}) \\\\end{align*}\\\\] $u_c$와 $v$를 최적화(optimization)시키는 방법으로 SGD(stochastic gradient descent)가 사용된다. 지금까지 학습 진행과정을 살펴보았다. 학습이 완료된 후에는 지금까지 학습시킨 파라미터 중 W를 embedded vector matrix로 사용한다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean.”Efficient Estimation of Word Representations in Vector Space,”(2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean.”Distributed Representations of Words and Phrases and their Compositionality,”(2013) × Search',\n",
       "  'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/'},\n",
       " {'text': 'NPLM은 신경망을 이용하여 단어를 embedding하는 단어의 분산표상(distributed representation)방식 중 하나로 간단히 만하면 N-gram model이다. 즉 NPLM은 NPLM 모델은 앞서 나온 N개의 단어를 통해 다음 단어를 예측하는 방식으로 학습하는 모델이다. 아래 수식은 모델의 출력에 대한 수식인데 해당 식에 정의된 조건부 확률을 최대화하도록 학습하는 것이 NPLM모델이다. \\\\begin{matrix} P({ w }_ { t }|{ w }_ { t-1 },…,{ w }_ { t-n+1 })=\\\\frac { exp({ y }_ { { w }_ { t } }) }{ \\\\sum _ { i }^{ }{ exp({ y }_ { i }) } } \\\\end{matrix} 위 식에 정의된 조건부 확률을 높이기 위해서는우변의 분자 부분에 해당하는 벡터값은 높이고 분모의 벡터값은 낮춰야 한다. 벡터 $y_{w_t}$는 해당 단어$w_t$에 대한 score vector로, 코퍼스 전체 단어수($V$)에 따라 차원이 결정된다. 따라서 우변은$V$차원의 score vector $y_{w_t}$에 softmax함수를 적용한 확률 벡터이다. NPLM의 입력벡터($x_t$)는 다음 식을 통해 만들어진다. \\\\begin{matrix} x_t=C(w_t) \\\\end{matrix}',\n",
       "  'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'text': '위 식에서 $w_t$는 문장 내 $t$번째 단어에 대한 one-hot-vector이고 $C$는 전체단어 $V$개에 대한 embedding결과이다. 따라서 one-hot-vector와 행렬$C$의 내적은 아래 그림과 같이 행렬$C$에서 해당 단어에 해당하는 행(column)을 look up해 오는 것이라 말할 수 있다. 즉, 행렬 $C$는 embedding된 단어 vector들의 집합이며 one-hot-vector($w_t$)는 embedding된 벡터를 불러오는 역할을 하는 것이다.($C$는 초기에 랜덤으로 만든다.)',\n",
       "  'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'text': 'Architecture of NPLM',\n",
       "  'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'text': 'NPLM의 구조를 보면 크게 input layer와 hidden layer 그리고 output layer가 있다. 설명을 위해 간단한 예를 들겠다. “퇴근 시간 서울역은 최악이다”라는 문장에서 [퇴근], [시간], [서울역은] 이렇게 세 개의 단어가 주어진 경우에 [최악이다]라는 단어를 예측한다고 가정하겠다. 이때 Input layer에서는 우선 주어진 세 개의 벡터에 대해 행렬$C$와 내적한다. 이후 내적하여 나온 값들을 concatenate하면 입력 벡터 $x$가 만들어진다. 입력 벡터 $x$에 대한 정의는 다음과 같다: \\\\begin{matrix} x=[{ { x }_ { t-1 } },{ { x }_ { t-2 } },… ,{ { x }_ { t-n+1 } }] \\\\end{matrix} 위 식에서 $n$은 n-gram의 개수이다. 즉 예측 대상 단어를 포함한 단어의 개수이고 $t$는 문장 내에서 해당 단어가 등장한 순서를 나타낸다. 이렇게 구한 $x$를 $H$와 내적한 뒤 bias term ’$d$’를 더한 후 tanh를 적용시켜 hidden layer를 만들고, 거기에 $U$를 내적한 후, bias term ’$b$’를 더해주면 $y_{w_t}$가 구해진다: \\\\begin{matrix} { y }_ { { w }_ { t } }=b+U\\\\cdot tanh(d+H_x) \\\\end{matrix}',\n",
       "  'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'text': '마지막으로 $y_{w_t}$($N$개의 단어가 나왔을 때 예측한 다음 단어)에 softmax함수를 적용한 뒤 정답 단어의 index와 비교하여 역전파(backpropagation)하는 방식으로 학습이 이루어진다. 여기에서 loss를 줄이는 loss-function으로는 cross-entropy를 사용한다. 최종 목표는 $C$를 학습시키는 것이라고 말할 수 있다. Parameter 학습 파라미터를 정리하면 아래와 같다:',\n",
       "  'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'text': '여기에서 사실 $U$가 뭔지, $H$가 뭔지 $b$가 뭔지 $d$가 뭔지 모르겠지만, 일단 학습 파라미터가 엄청 많다는 것은 확실히 알 수 있다. 학습 파라미터를 보면 행렬C이외에도 $H$, $U$, $b$, $d$등도 학습해야 하기에 계산의 복잡성이 높다. 이는 학습량을 늘려 학습을 느리게하는 원인이 된다. 따라서 이후 제안된 Word2vec같은 단어 임베딩 기법에서는 학습 파라미터를 줄이고 그의 품질은 높이는 방향으로 발전한다. Reference Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin.”A Neural Probabilistic Language Model,”(2003) Lee,Gichang.Sentence embeddings using korean corpora.seoul:acornpub,2019 × Search',\n",
       "  'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/'},\n",
       " {'text': 'Word embedding은 단어의 유사도를 표현하지 못하는 one-hot-encoding이 지닌 몇 가지 문제점를 해결하기 위해 고안된 방법이다. one-hot-encoding은 해당 단어를 제외한 모든 단어를 0으로 표현하는 binary한 구조를 가져 데이터가 많아질 수록 차원이 매우 커지며 벡터가 굉장히 sparse해진다는 문제와 두 벡터의 내적이 0이 되어 단어들 간의 유사도 표현이 불가능 하다는 문제가 있다.',\n",
       "  'source_title': 'Word embedding(Distributed Representation)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/Word-embedding(Distributed-Representation)/'},\n",
       " {'text': '이러한 문제를 해결하기 위해 등장한 개념이 Word embedding인데 이는 분산 표상(distributed representation/Distributional similarity based representations)이라고도 불린다. 분산표상은 Distributional Hypothesis라는 언어학 개념으로부터 비롯되었다. 이 개념은 비슷한 분포를 가진 단어들은 의미 또한 비슷하다는 가정이다. 이러한 가정으로부터 착안된 분산 표상은 말 그대로 단어들의 유사도(similarity)를 기반으로 그들의 분포(Distributional)를 벡터 공간 안에 표현(representation)한 것이다. 간단히 말하자면 분산표상은 단어를 연속형의 실수값으로 표현함으로써 위에서 언급한 one-hot-encoding의 여러 문제점를 해결하였다. 앞서 언급했 듯이 분산표상은 단어 간의 유사도 정보를 포함하고 있기 때문에 다음 그림과 같이 유사한 의미를 지닌 단어들은 가깝게, 그렇지 않은 경우에는 멀게 배치 된다.',\n",
       "  'source_title': 'Word embedding(Distributed Representation)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/Word-embedding(Distributed-Representation)/'},\n",
       " {'text': 'Embedding Vector word embedding의 결과는 위 그림과 같이 similarity를 기반으로 embedded된 값이 나온다. embedding된 벡터는 해당 단어와 함께 사용된 주변 단어들(context words)를 기반으로 나오기 때문에 해당 단어는 문법적, 개념적 의미(semantics of word meaning) 정보를 지닌 하나의 벡터가 된다. 예를 들어 “나는 카페에서 커피를 마시고 있다.”라는 문장에서 “커피”자리에는 예를 들어 “주스”나 “스무디”와 같이 그와 비슷한 품사와 의미를 지닌 단어가 들어갈 수 있을 것이다. Word embedding 학습 논리 Word embedding 학습 논리는 다음 수식을 통해 설명될 수 있다: \\\\begin{matrix} p(context | w_t)=… \\\\end{matrix} \\\\begin{matrix} J = 1- p({ { w }_ { -t } } | w_t) \\\\end{matrix} 이는 아래 수식과 같이 곧 Loss값으로, J가 0에 가까울수록 잘 예측한 것이고 1에 가까울수록 잘못 예측한 것이다. 따라서 loss값이 클수록 system내의 parameter값을 크게 조정하여 이후에 더 좋은 예측 결과를 내도록 하는 방식으로 진행된다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) × Search',\n",
       "  'source_title': 'Word embedding(Distributed Representation)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/Word-embedding(Distributed-Representation)/'},\n",
       " {'text': '자연어처리 시 단어-문서 행렬(Word-Document Matrix), TF-IDF(Term Frequency-Inverse Document Frequency) 행렬, 그리고 단어-문맥(window based con-occurrence matrix) 행렬과 같이 문서를 벡터화하여 행렬로 표현한 데이터가 사용된다. 잠재 의미 분석(Latent Semantic Analysis) 은 이러한 행렬의 차원을 줄여 계산의 복잡성을 낮추고 학습량을 줄임과 동시에 행간에 숨어있는(latent)의미를 이끌어내기 위한 방법론이다. 해당 방법론에서는 입력으로 사용되는 행렬 데이터에 특이 값 분해(Singular Value Decomposition, SVD)를 수행하여 데이터의 차원을 축소한다. 따라서 잠시 SVD에 대해 설명하도록 하겠다. SVD는 다량의 전체 데이터(massive data)를 한 바퀴 탐색하며 여러 번 등장하는 단어(co-occurrence)들의 등장 빈도를 계산하여 이를 기반으로 matrix $A\\u200b$ 형태로 표현한 후 SVD를 수행하여 $U\\\\Sigma { V }^{ T }\\u200b$ decomposition을 얻는다. 간단히 말하자면 빈도를 계산하여 표현된 matrix $A\\u200b$를 $U\\\\Sigma { V }^{ T }\\u200b$로 분해하는 것이다. 특이 값 분해는 다음과 같이 정의된다: \\\\begin{matrix} A=U\\\\Sigma { V }^{ T }\\\\ \\\\end{matrix}',\n",
       "  'source_title': 'LSA(Latent Sematic Analysis)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/'},\n",
       " {'text': '$U$는 $A { A }^{ T }$를 고유값 분해(eigenvalue decomposition)하여 얻은 직교행렬(orthogonal matrix)이고, $V$는 ${ A }^{ T } A$를 고유값 분해해서 얻어진 직교행렬(orthogonal matrix)이다. 그리고 $\\\\Sigma$는 $A { A }^{ T }$, ${ A }^{ T } A$를 고유 값 분해하여 얻은 고유값들의 제곱근을 대각원소로 갖는 행렬인데 해당 행렬의 대각 원소들이 바로 $A$의 특이값(singular value)들을 제곱한 값이다. 다시 LSA로 돌아와서, LSA기법에서는 위에서 설명한 SVD가 수행되는데 일반 SVD대신 절단된 SVD(truncated SVD)가 사용된다. 절단된 SVD를 그림으로 나타내면 다음과 같다:',\n",
       "  'source_title': 'LSA(Latent Sematic Analysis)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/'},\n",
       " {'text': '절단된 SVD(truncated SVD)수행을 위해 우선 $A$의 고유 값 개수($r$)보다 작은 임의의 $k$를 설정한 후, 대각 행렬 $\\\\Sigma$에서 가장 큰 $k$개의 값을 제외한 나머지를 제거하여 $Σ_k$를 만든다. 그리고 행렬$U$와 $V$에서 이와 대응하는 부분만 남겨 $U_k $ 와 $V_k\\u200b$를 만들어 모두 곱하면 행렬$A$와 근사하지만 차원이 줄어든 $A_k$가 만들어진다. 아래 그림을 참고하면 이해가 빠를 것이다. truncated SVD 결과가 갖는 의미에 대해 살펴보도록 하겠다. $U_kΣ_k{ V_k }^{ T }$로 도출된 행렬은 단어와 문서 간의 유사도를 나타내고, $U_kΣ_k{ V_k }^{ T }$에 ${ U_k }^{ T }$를 곱해 $Σ_k{ V_k }^{ T }$를 구하면 결과로 도출된 행렬의 행(colunm)은 문서 간의 유사도를 의미하게 된다. 그리고 $U_kΣ_k{ V_k }^{ T }$에 $V_k$를 곱하여 $U_kΣ_k$의 결과를 구하게 되면 도출된 행렬의 열(row)는 단어 간의 유사도를 의미한다.',\n",
       "  'source_title': 'LSA(Latent Sematic Analysis)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/'},\n",
       " {'text': '지금까지 설명한 LSA기법은 데이터의 차원을 축소하여 학습량을 줄여 성능을 높일 수 있다는 장점도 있지만 SVD의 특성상 새로운 데이터가 추가될 경우 계산을 모두 다시 해야 해 정보 업데이트에 대해 취약하다는 단점도 존재한다. 따라서 이러한 문제점으로 인해 새로운 데이터 업데이트가 용이한 인공 신경망 기반의 word embedding방법이 연구되어 계속 발전되고 있다. × Search',\n",
       "  'source_title': 'LSA(Latent Sematic Analysis)\\n      03 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/03/LSA(Latent-Sematic-Analysis)/'},\n",
       " {'text': '컴퓨터는 숫자 연산으로 모든 것들을 이해하기 때문에 인간의 언어를 컴퓨터가 받아들이기 위해서는 컴퓨터가 연산할 수 있도록 숫자로 바꿔 주어야한다. 즉 단어를 벡터(vector)화 시켜야한다. One-Hot Vector One-hot vector는 가장 간단하고 단순한 word vector이다. 전통적인 NLP에서 단어는 각각 개별적, 독립적인 entity으로 다뤄졌다. 따라서 각 단어에 유일한 벡터를 부여했는데 이 방식을 one-hot-encoding이라고 한다. 이는 해당 단어에는 (1)bit를, 나머지에는 모두 (0)bit를 할당하여 ${ R }^{ v×M }$벡터로 표현하는 방식이다($v=$ 해당 언어의 어휘사전 크기). 즉, vector공간 안에 한 단어를 독립적이고 유일한 존재로 하나의 1과 여러 0들로([0 0 0 0 0 0 1 0 0 0 0 0 0]) 표현하는 방식이다. 이는 rule-based/count-based NLP라고도 불린다. 이러한 encoding방식으로 나타낸 word vector는 아래 그림과 같은 형태를 띄게 된다:',\n",
       "  'source_title': 'How to represent words?( Word Vectors)\\n      01 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/01/Word-Vectors/'},\n",
       " {'text': 'one-hot-vector로 단어를 표현하게 되면 다음과 같이 여러 문제점들이 발생한다: 1. 차원(dimension/space)이 단어 활용 상황에 따라 기하급수적으로 늘어난다. 단어 하나를 표현한 벡터의 크기가 크면 연산량이 늘어나기 때문에 문장 하나를 처리하는 것도 어려운 과제가 될 것이다. 2. 두인간은 언어를 구사할 때 미세한 뉘앙스도 함께 전달하는데 one-hot-vector로 단어를 표현하면 단어들 간의 미세한 뉘앙스 차이를 놓치게 된다. 3. 언어는 매일매일 변화하며 무수히 많은 새로운 단어가 생겨나는데 위와 같이 만들어진 단어 사전에 매번 새로운 단어를 업데이트할 수 없다는 문제가 있다. 지속적으로 업데이트를 하더라도 사람이 직접 기록하는 것이기 때문에 기록자의 주관적인 생각이 개입될 수 있다는 것도 또다른 문제 중 하나이다.',\n",
       "  'source_title': 'How to represent words?( Word Vectors)\\n      01 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/01/Word-Vectors/'},\n",
       " {'text': '4. 마지막으로 인간의 언어에서 단어 간의 관계는 중요한 정보 중 하나인데 one-hot-vector는 단어 간 유사성에 대한 정보를 담아내지 못한다. 즉, 단어가 지닌 고유의 개념과 내재적 개념(=의미, inherent notion)를 담아내지 못한다. 위에서 언급된 마지막 문제점이 one-hot vector가 지닌 가장 큰 한계라 말할 수 있다. 연관 검색어를 생각해 보면 이것이 왜 문제인지 이해될 것이다. 예를 들어 사용자가 ‘신촌 카페’를 검색했을 때 신촌에 있는 카페 뿐만 아니라 찻집까지 함께 검색해 준다면 사용자가 앉아서 쉬며 음료를 마실 수 있는 장소를 더 많이 검색해 볼 수 있게 될 것이다. 하지만 one-hot-vector를 사용한다면 ‘카페’와 ‘찻집’간의 유사도 정보를 담지 못하기 때문에 이것이 불가능하다.',\n",
       "  'source_title': 'How to represent words?( Word Vectors)\\n      01 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/01/Word-Vectors/'},\n",
       " {'text': 'one-hot-encoding은 이렇게 단어 벡터가 굉장히 크며 sparse하고 단어 간의 유사도도 나타낼 수 없다는 단점을 지니고 있다. 따라서 지금까지 설명한 문제점들을 해결하기 위해 우선 차원의 크기를 ${ R }^{ v×M }$보다 작게 만들어야 하고 단어들 간의 연관성을 나타내는 subspace를 찾아서 encode하는 방식을 찾아야한다. 이후 게시물에서 차원문제와 유사도문제를 해결한 방안들을 소개하도록 하겠다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) × Search',\n",
       "  'source_title': 'How to represent words?( Word Vectors)\\n      01 Nov 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/01/Word-Vectors/'},\n",
       " {'text': 'Tokenization(토큰화) 토큰화는 말 그대로 문자열을 토큰들로 분리하는 작업이다. 자연어처리에서 토큰화는 중요한 과정이다. 예를 들어 ‘United Kingdom’을 분리하면 전혀 다른 의미가 되기 때문에 두 단어는 붙여서 처리해야 한다. 즉, 토큰화 대상에서 제외해야 한다. 단어들의 의미를 고려한 토큰화를 지향해야 해당 데이터를 이용한 학습의 결과가 좋아진다. 토큰화 | 단어 | 영어 # sentence1 = \\'I can always predict the fortune cookies\\' sentence1 = \"I play the violin when I\\'m thinking and sometimes I don\\'t talk for days on end.\" tokens = sentence1.split(\\' \\') print(tokens) print(type(tokens)) # sentence1을 공백을 기준으로 split한 결과를 리스트에 담아준다. 이렇게도 가능 tokens1 = [x for x in sentence1.split(\\' \\')] print(tokens1) print(type(tokens1)) [\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \"I\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \"don\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end.\\'] <class \\'list\\'> [\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \"I\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \"don\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end.\\'] <class \\'list\\'>',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '토큰화 | 단어 | Mecab 한국어는 띄어쓰기를 준수하지 않아도 의미가 전달되기 때문에 띄어쓰기가 지켜지지 않는 경우가 많다. 그래서 공백을 기준으로 토큰화를 하면 데이터에 문제가 생길 수 있다. 그리고 한국어의 형태소는 영어의 형태소와 개념이 다르기 때문에 추가적으로 고려할 사항들이 있다. 한국어의 특성을 고려하여 잘 구축된 konlpy라는 라이브러리를 이용하면 처리가 간편하다.형태소분석기는 Mecab을 추천한다. 빠르고 정확하다. 토큰화는 Mecab이라는 형태소분석기를 사용한 결과에서 토큰 결과만 반환받는 설정을 해주면 된다. 윈도우 Mecab설치 방법: https://cleancode-ws.tistory.com/97 https://hong-yp-ml-records.tistory.com/91 from konlpy.tag import Mecab import MeCab kor_sentence = \\'이게 무슨 일이냐고 여름에는 호떡을 안 파냐고\\' kor_tagger = Mecab(dicpath=r\"C:\\\\mecab\\\\mecab-ko-dic\") # Mecab을 사용할 객체를 만든다 kor_tagger.pos(kor_sentence) # tagger를 통해 pos tagging을 해보자',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '[(\\'이게\\', \\'NP+JKS\\'), (\\'무슨\\', \\'MM\\'), (\\'일\\', \\'NNG\\'), (\\'이\\', \\'VCP\\'), (\\'냐고\\', \\'EC\\'), (\\'여름\\', \\'NNG\\'), (\\'에\\', \\'JKB\\'), (\\'는\\', \\'JX\\'), (\\'호떡\\', \\'NNG\\'), (\\'을\\', \\'JKO\\'), (\\'안\\', \\'MAG\\'), (\\'파\\', \\'VV\\'), (\\'냐고\\', \\'EC\\')] # 태거를 통해 morph만 뽑아보자(형태소 분석 결과에서 토큰화 결과만 출력.) kor_tagger.morphs(kor_sentence) [\\'이게\\', \\'무슨\\', \\'일\\', \\'이\\', \\'냐고\\', \\'여름\\', \\'에\\', \\'는\\', \\'호떡\\', \\'을\\', \\'안\\', \\'파\\', \\'냐고\\'] # 명사만 추출해보자 kor_tagger.nouns(kor_sentence) [\\'이게\\', \\'일\\', \\'여름\\', \\'호떡\\'] 토큰화 | 단어 | NLTK NLTK(Natural Language Tool Kit)이라는 패키지의 tokenizer모듈을 활용하여 tokenize해줄 수 있다. 단어 토큰화는 word_tokenize()함수를 이용한다. import nltk # nltk를 불러오고 nltk.download(\\'punkt\\') # nltk에서 토큰화를 위해 punkt를 다운받는다. from nltk.tokenize import word_tokenize [nltk_data] Downloading package punkt to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [nltk_data] Package punkt is already up-to-date! # word_tokenize에 문장을 넣어주면 토큰화를 해준다. # 앞서 split을 통해 공백을 기준으로 split한 결과보다 낫다. tokenizer1 = word_tokenize(sentence1) print(tokenizer1) [\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \\'I\\', \"\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \\'do\\', \"n\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end\\', \\'.\\']',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '토큰화 | 문장 여러 문장들이 있을 때 문장들을 나눠주는 토큰화 방식도 있다. sentences1 = \"Quiet. \\\\n Calm. \\\\n Peaceful. \\\\n Isn\\'t it hateful.\" sentence_tokens = [x for x in sentences1.split(\\'\\\\n\\')] # 개행을 기준으로 문장을 split print(sentence_tokens) [\\'Quiet. \\', \\' Calm. \\', \\' Peaceful. \\', \" Isn\\'t it hateful.\"] 토큰화 | 문장 | NLTK from nltk.tokenize import sent_tokenize sent_tokenize1 = sent_tokenize(sentences1) print(\"개행 표시 있는 문장: \", sent_tokenize1) sentences2 = \"Quiet. Calm. Peaceful. Isn\\'t it hateful.\" # 문장 사이 개행표시 삭제 sent_tokenize1 = sent_tokenize(sentences2) print(\"개행 표시 없는 문장:\", sent_tokenize1) 개행 표시 있는 문장: [\\'Quiet.\\', \\'Calm.\\', \\'Peaceful.\\', \"Isn\\'t it hateful.\"] 개행 표시 없는 문장: [\\'Quiet.\\', \\'Calm.\\', \\'Peaceful.\\', \"Isn\\'t it hateful.\"] 토큰화 | 문장 | kss(Korean sentence spliter) 한국어 문장 토큰화는 kss라는 라이브러리를 사용하여 수행한다.(이걸 사용해도 한국어 자연어 처리에는 고려할 사항이 많아서 토큰화가 잘 안된다.) # !pip install kss Collecting kss Downloading kss-2.6.0-py3-none-any.whl (67 kB) Installing collected packages: kss Successfully installed kss-2.6.0 import kss print(kss.split_sentences(kor_sentence)) # 잘 안 된다. [\\'이게 무슨 일이냐고 여름에는 호떡을 안 파냐고\\'] 토큰화 | NLTK | RegexpTokenizer | 영어 nltk의 tokenize의 RegexpTokenizer(Regular Expression Tokenizer)를 사용하여 토큰화할 수 있다. 역슬래시(\\\\)를 이용한 정규 표현식 문자 규칙',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '문자 설명 \\\\\\\\ 역슬래시 그자체 \\\\d digit. 모든 숫자(= [0-9]) \\\\D 숫자를 제외한 모든 문자(= [^0-9]) \\\\s space. 공백(= [ \\\\t\\\\n\\\\r\\\\f\\\\v]) \\\\S 공백을 제외한 모든 문자(= [^ \\\\t\\\\n\\\\r\\\\f\\\\v]) \\\\w word. 문자와 숫자(= [a-zA-Z0-9]) \\\\W 문자와 숫자를 제외한 다른 문자(= [^a-zA-Z0-9]) from nltk.tokenize import RegexpTokenizer sentence1 = \"I play the violin when I\\'m thinking and sometimes I don\\'t talk for days on end.\" # \\'문자와 숫자가 최소 한개 이상\\'이라는 패턴에 해당하는 것들만 토큰화한다. # 즉, 문자나 숫자로 된 것만 tokenize하는 tokenizer를 객체에 담아준다. reg_tokenizer1 = RegexpTokenizer(\"[\\\\w]+\") # 객체를 이용하여 문장을 토큰화한다. tokens1 = reg_tokenizer1.tokenize(sentence1) print(tokens1) # 특수문자를 빼고 처리되었다. [\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \\'I\\', \\'m\\', \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \\'don\\', \\'t\\', \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end\\'] from nltk.tokenize import regexp_tokenize print(regexp_tokenize(sentence1, \"[\\\\w]+\")) [\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \\'I\\', \\'m\\', \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \\'don\\', \\'t\\', \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end\\']',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '# gaps=True를 이용하여 해당 정규표현식을 토큰화 기준으로 설정할 수 있다. # 공백을 기준으로 토큰화를 한다. reg_tokenizer2 = RegexpTokenizer(\"[\\\\s]+\", gaps= True) tokens2 = reg_tokenizer2.tokenize(sentence1) print(tokens2) # 위 코드에서 gaps=True를 없애면 공백만 나온다. 공백만 토큰화하라는 말이 되니까. [\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \"I\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \"don\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end.\\'] 토큰화 | NLTK | RegexpTokenizer | 한국어 # 해당 정규표현식에 걸린 것들만 토큰화한다 kor_tokenizer = RegexpTokenizer(\"[가-힣]+\") kor_tokens = kor_tokenizer.tokenize(kor_sentence) kor_tokens [\\'이게\\', \\'무슨\\', \\'일이냐고\\', \\'여름에는\\', \\'호떡을\\', \\'안\\', \\'파냐고\\'] # 마찬가지로 gaps=True를 이용하여 해당 정규표현식을 토큰화 기준으로 설정할 수 있다. # 공백을 기준으로 토큰화를 한다. kor_tokenizer2 = RegexpTokenizer(\"[\\\\s]+\", gaps= True) kor_tokens2 = kor_tokenizer2.tokenize(kor_sentence) kor_tokens2 [\\'이게\\', \\'무슨\\', \\'일이냐고\\', \\'여름에는\\', \\'호떡을\\', \\'안\\', \\'파냐고\\'] 토큰화 | Keras | 영어 딥러닝 프레임워크인 keras를 이용하여 토큰화를 하는 방법도 있다. from tensorflow import keras # text_to_word_sequence라는 모듈에서 tokenizer를 제공한다. from tensorflow.keras.preprocessing.text import text_to_word_sequence',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '# 공백을 기준으로 토큰화한다. text_to_word_sequence(sentence1) [\\'i\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \"i\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'i\\', \"don\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end\\'] ### 토큰화 Keras 한국어 text_to_word_sequence(kor_sentence) [\\'이게\\', \\'무슨\\', \\'일이냐고\\', \\'여름에는\\', \\'호떡을\\', \\'안\\', \\'파냐고\\'] 토큰화 | TextBlob | 영어 # !pip install textblob from textblob import TextBlob blob_tokenizer = TextBlob(sentence1) blob_tokenizer.words WordList([\\'I\\', \\'play\\', \\'the\\', \\'violin\\', \\'when\\', \\'I\\', \"\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \\'do\\', \"n\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'on\\', \\'end\\']) 토큰화 | TextBlob | 한국어 kor_blob_tokenizer = TextBlob(kor_sentence) kor_blob_tokenizer.words WordList([\\'이게\\', \\'무슨\\', \\'일이냐고\\', \\'여름에는\\', \\'호떡을\\', \\'안\\', \\'파냐고\\']) × Search',\n",
       "  'source_title': 'Tokenization;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/Tokenization/'},\n",
       " {'text': '\\'\\'\\' spilt 함수 를 사용하여 공백을 기준으로 나누기 \\'\\'\\' string1.split() [\"It\\'s\", \\'good\\', \\'news\\', \\'for\\', \\'breathing.\\'] \\'\\'\\' split한 결과에서 특정 단어가 어느 인덱스에 있는지 찾기 \\'\\'\\' string1.split().index(\\'good\\') 1 \\'\\'\\' slicing으로 특정 인덱스의 문자에 접근 \\'\\'\\' string1[-7:] \\'athing.\\' \\'\\'\\' split한 결과에서의 특정 인덱스에 있는 단어가 뭐냐 \\'\\'\\' string1.split()[4] \\'breathing.\\' string1.split()[3][::-1] # split한 결과의 3번째 인덱스에 있는 단어를 reverse해라 \\'rof\\' String 다루기 | 한국어 \\'\\'\\' 파이썬은 유니코드를 지원하기 때문에 별도의 처리 없이 한글을 다룰 수 있다. \\'\\'\\' string2 = \\'호떡 먹고 싶다 호떡 호떡\\' \\'호떡\\' in string2 True string2.split() [\\'호떡\\', \\'먹고\\', \\'싶다\\', \\'호떡\\', \\'호떡\\'] string2.split()[0] # split한 결과의 0번째 인덱스에 있는 원소가 뭐냐 \\'호떡\\' 정규화 \\'\\'\\' 텍스트에 존재하는 단어들의 통일성에 신경을 써야 처리가 편하다. 코퍼스의 전반적인 정규화/변환과정은 자연어를 처리할 때 꼭 필요하다. \\'\\'\\' # s1 = \\'I can always predict the fortune cookies 12 34 56\\' s1 = \\'The UK leant towards the US proposal.\\' # UK와 US를 풀어쓰는 방식으로 바꿔주자. s1_1 = s1.replace(\"UK\", \"United Kingdom\").replace(\"US\", \"Untitled.ipynbnited States\") print(s1_1) The United Kingdom leant towards the Untitled.ipynbnited States proposal.',\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': '정규표현식(Regular Expression) 정규표현식을 통해 특정 문자드을 편하게 지정할 수 있고, 지정한 것을 통해 텍스트에 추가 혹은 삭제 처리를 할 수 있다.코퍼스 전처리 단계에서 정규표현식이 많이 사용된다. 예를 들어 텍스트에 번호가 포함되어 있는지 알아보는 등에 쓰인다. 정규 표현식 문법 | 특수문자 | 설명 | | - | - | | . | . 앞의 문자 1개가 있냐 | | ? | ? 앞의 문자 1개가 있냐(하지만 그 문자가 존재할 수도, 존재하지 않을 수도 있음) | | * | * 앞의 문자가 0개 이상 존재하냐 | | + | + 앞의 문자가 최소 1개 이상 있냐 | | ^ | ^ 뒤의 문자로 문자열이 시작하냐 | | \\\\$ | ‘$ 앞의 문자로 문자열이 끝나냐 | | {n} | n번만큼 반복 | | {n1, n2} | n1 이상, n2 이하만큼 반복하냐 (n2를 지정하지 않을 경우, n1 이상만 반복하냐) | | [ abc ] | 대괄호 안의 문자 중 한 개의 문자와 매치되냐(a-z처럼 범위도 지정 가능) | | [ ^a ] | 대괄호 안의 문자를 제외하고 매치 | | a|b | a 또는 b`(파이프라인 (|)은 일반적으로 ‘또는’의 의미로 사용된다) |',\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': '역슬래시(\\\\)를 이용한 정규 표현식 문자 규칙 문자 설명 \\\\\\\\ 역슬래시 그자체 \\\\d digit. 모든 숫자(= [0-9]) \\\\D 숫자를 제외한 모든 문자(= [^0-9]) \\\\s space. 공백(= [ \\\\t\\\\n\\\\r\\\\f\\\\v]) \\\\S 공백을 제외한 모든 문자(= [^ \\\\t\\\\n\\\\r\\\\f\\\\v]) \\\\w word. 문자와 숫자(= [a-zA-Z0-9]) \\\\W 문자와 숫자를 제외한 다른 문자(= [^a-zA-Z0-9]) 정규표현식 | match | 영어 import re # 정규표현식ㅇ르 사용할 때 re 라이브러리를 불러와야 한다. check1 = \\'ab.\\' # ab. -> ab하고 \\'.\\'부분에 문자가 하나라도 오냐 print(\"about \\'abc\\': \", re.match(check1, \\'abc\\')) print(\"about \\'abc\\': \", re.match(check1, \\'abcd\\')) print(\"about \\'c\\': \", re.match(check1, \\'c\\')) print(\"about \\'ab\\': \", re.match(check1, \\'ab\\')) about \\'abc\\': <re.Match object; span=(0, 3), match=\\'abc\\'> about \\'abc\\': <re.Match object; span=(0, 3), match=\\'abc\\'> about \\'c\\': None about \\'ab\\': None 정규표현식 | match | 한국어 import re check2 = \\'[ㄱ-ㅎ]+\\' # ㄱ부터 ㅎ까지의 한글 자음이 나오고 그 뒤로 문자가 최소 하나 이상 오냐 # check2에 등록한 정규표현식에 걸리는 애를 찾아라 print(re.match(check2, \\'ㅎ안녕\\')) print(re.match(check2, \\'안녕ㅎ\\')) # ㄱ-ㅎ에 걸리는 \\'ㅎ 안녕\\'에서 ㅎ이 걸렸다. # 걸린게 없다 <re.Match object; span=(0, 1), match=\\'ㅎ\\'> None',\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': '정규표현식 | compile | 영어 re 라이브러리의 compile을 사용하면 정규표현식을 사용할 때마다 정의하지 않고 한번 정의한 후에 불러오는 방식으로 쓸 수 있어서 빠르고 편리하다. \\'\\'\\' compile을 쓴 버전과 안 쓴 버전의 속도 차를 확인해보자 \\'\\'\\' import re import time # 시간을 체크하기 위해 time을 불러온다. text1 = \\'abc def abg\\' normal_start = time.time() # 시작 r1 = \\'ab.\\' for i in range(1001): # i를 1001번 돌린다. re.match(r1, text1) # text1이 ab어쩌고인지 확인해라 print(\"compile 안 쓴 버전: \", time.time() - normal_start) # 끝시간 - 시작시간 compile_start = time.time() # 시작 r2 = re.compile(\\'ab.\\') # compile for i in range(1001): # i를 1001번 돌린다. result = r2.match(text1) # re가 아니라 re.compile한 객체를 불러온다 print(\\'compile 쓴 버전: \\', time.time() - compile_start) compile 안 쓴 버전: 0.0010042190551757812 compile 쓴 버전: 0.0009706020355224609 정규표현식 | search | 영어 match는 문자열 처음에 해당 표현이 없으면 바로 끝나고 search는 문자열 전체를 검사해서 해당 표현에 매칭되는게 있는지 찾아낸다. r1 = re.compile(\\'ab.\\') print(\"a에 대해 search: \",r1.search(\\'a euosblkg abc\\')) print(\"a에 대해 match: \",r1.match(\\'a euosblkg abc\\')) print(\"abhey에 대해 search: \",r1.search(\\'jigdas abhey\\')) print(\"abhey에 대해 match: \",r1.match(\\'jigdas abhey\\'))',\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': 'print(\"heyab에 대해 search: \",r1.search(\\'det sryh heyab\\')) print(\"heyab에 대해 match: \",r1.match(\\'det sryh heyab\\')) a에 대해 search: <re.Match object; span=(11, 14), match=\\'abc\\'> a에 대해 match: None abhey에 대해 search: <re.Match object; span=(7, 10), match=\\'abh\\'> abhey에 대해 match: None heyab에 대해 search: None heyab에 대해 match: None 정규표현식 | search | 한국어 check2 = \\'[ㄱ-ㅎ | ㅏ-ㅣ]+\\' print(re.search(check2, \\'ㅎ안녕\\')) print(re.match(check2, \\'ㅎ안녕\\')) print(re.search(check2, \\'안녕ㅎ\\')) # match는 앞에서부터 차례로 체크하는데 처음부터 걸린게 없어서 None이 반환된다. print(re.match(check2, \\'안녕ㅎ\\')) <re.Match object; span=(0, 1), match=\\'ㅎ\\'> <re.Match object; span=(0, 1), match=\\'ㅎ\\'> <re.Match object; span=(2, 3), match=\\'ㅎ\\'> None 정규표현식 | split | 영어 split에 정규표현식을 사용하여 정규표현식에 해당하는 문자열을 기준으로 문자열을 나눌 수 있다. s1 = \\'abc 1 d5ef 3 ccb 2 ca2d\\' r1 = re.compile(\\' \\') # compile을 사용하여 공백을 표현 print(r1.split(s1)) # 문자열 s1을 compile한 결과가 담긴 객채를 통해 split해준다. # 공백을 기준으로 split될 결과가 반환된다. [\\'abc\\', \\'1\\', \\'d5ef\\', \\'3\\', \\'ccb\\', \\'2\\', \\'ca2d\\'] r2 = re.compile(\\'c\\') print(r2.split(s1)) # \\'c\\'를 기준으로 나눈다 [\\'ab\\', \\' 1 d5ef 3 \\', \\'\\', \\'b 2 \\', \\'a2d\\'] r3 = re.compile(\\'[0-9]\\') print(r3.split(s1)) # 0-9 사이의 숫자를 기준으로 나눈다 [\\'abc \\', \\' d\\', \\'ef \\', \\' ccb \\', \\' ca\\', \\'d\\']',\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': \"sub | 영어 정규표현식으로 걸린 부분을 다른 문자열로 대체하는 함수이다. replace라고 생각하면 된다. s2 = '457758' print(re.sub('[a-z]', '1', s1)) # s1에서 a-z에 포함된 것들을 모두 1로 바꿔라 print(re.sub('[a-z]', '1', s2)) # s2에서 a-z에 포함된 것들을 모두 1로 바꿔라 111 1 1511 3 111 2 1121 457758 print(re.sub('[^a-z]', '1', s1)) # s1에서 a-z에 포함되지 않은 것들을 모두 1로 바꿔라 print(re.sub('[^a-z]', '1', s2)) # s2에서 a-z에 포함되지 않은 것들을 모두 1로 바꿔라 abc111d1ef111ccb111ca1d 111111 sub | 한국어 # [가-힣]는 모든 한글 자모 조합을 표현하는 정규표현이다. # '안녕안녕'에서 해당 정규 표현식에 걸리는 게 있으면 다 1로 바꿔라 print(re.sub('[가-힣]', '1', '안녕안녕')) # '안녕안녕'에서 [가-힣]에 포함되지 않는 것만 1로 바꿔라 print(re.sub('[^가-힣]', '1', '안녕안녕')) 1111 안녕안녕\",\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': 'findall 정규표현식에 해당하는 모든 문자열을 찾아서 리스트에 담아 반환한다. print(re.findall(\\'[\\\\d]\\',s1)) # s1에서 숫자를 다 뽑아내라 # 해당 문자열에서 문자나 숫자가 아닌 걸 다 뽑아내라 print(re.findall(\\'[\\\\W]\\',\\'1#sjspoen@%$^%&95736\\')) [\\'1\\', \\'5\\', \\'3\\', \\'2\\', \\'2\\'] [\\'#\\', \\'@\\', \\'%\\', \\'$\\', \\'^\\', \\'%\\', \\'&\\'] finditer find한 결과를 iterator객체로 반환하는 함수이다. iterator객체에 담겨있기 때문에 하나씩 빼주는 작업을 통해 결과를 하나씩 확인할 수 있다. iter1 = re.finditer(\\'[\\\\d]\\',s1) # s1에서 숫자를 다 뽑아내라 print(\"iter1: \", iter1) # iter1을 출력해보면 iterator객체에 담겨있다고 나온다 for i in iter1: # iter1에 한번씩 방문하여 들어있는 걸 꺼내서 출력하자 print(\"iter1에서 찾은 결과: \",i) iter1: <callable_iterator object at 0x0000021C924383C8> iter1에서 찾은 결과: <re.Match object; span=(4, 5), match=\\'1\\'> iter1에서 찾은 결과: <re.Match object; span=(7, 8), match=\\'5\\'> iter1에서 찾은 결과: <re.Match object; span=(11, 12), match=\\'3\\'> iter1에서 찾은 결과: <re.Match object; span=(17, 18), match=\\'2\\'> iter1에서 찾은 결과: <re.Match object; span=(21, 22), match=\\'2\\'> iter2 = re.finditer(\\'[\\\\W]\\', \\'nkisdb3845$$^%&#\\') print(\"iter2: \", iter2) for i in iter2: print(i)',\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': \"iter2: <callable_iterator object at 0x0000021C92438688> <re.Match object; span=(10, 11), match='$'> <re.Match object; span=(11, 12), match='$'> <re.Match object; span=(12, 13), match='^'> <re.Match object; span=(13, 14), match='%'> <re.Match object; span=(14, 15), match='&'> <re.Match object; span=(15, 16), match='#'> from konlpy.tag import Mecab × Search\",\n",
       "  'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/'},\n",
       " {'text': \"N-Gram 토큰을 연속하는 n개로 분류하여 출연 횟수를 추출하는데 쓰인다. n = 1일 때는 unigram, n = 2일 때는 bigram, n = 3일 때는 trigram… bigram이 가장 많이 사용된다. N-Gram | NLTK from nltk import ngrams sentence1 = 'I can always predict the fortune cookies' # n-gram모듈에 sentence1을 split한 것을 넣고, 2개씩 묶어서 나눈다. bigram1 = list(ngrams(sentence1.split(), 2)) print(bigram1) [('I', 'can'), ('can', 'always'), ('always', 'predict'), ('predict', 'the'), ('the', 'fortune'), ('fortune', 'cookies')] N-Gram | TextBlob from textblob import TextBlob ngram_blob = TextBlob(sentence1) # TextBlob에 있는 ngrams함수를 사용한다. # n=2로 넣어서 bigram으로 만든다.(default는 trigram으로 설정되어 있다) ngram_blob.ngrams(n=2) [WordList(['I', 'can']), WordList(['can', 'always']), WordList(['always', 'predict']), WordList(['predict', 'the']), WordList(['the', 'fortune']), WordList(['fortune', 'cookies'])] ngram_blob.ngrams() [WordList(['I', 'can', 'always']), WordList(['can', 'always', 'predict']), WordList(['always', 'predict', 'the']), WordList(['predict', 'the', 'fortune']), WordList(['the', 'fortune', 'cookies'])] POS(Part of Speech) tagging POS tagging은 품사태깅이다. 문장 내의 단어들에 대해 품사를 부착해주는 것이다. POS | NLTK import nltk nltk.download('punkt') from nltk import word_tokenize [nltk_data] Downloading package punkt to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [nltk_data] Package punkt is already up-to-date! # 먼저 tokenize해준다. tokenize_result = word_tokenize(sentence1) tokenize_result ['I', 'can', 'always', 'predict', 'the', 'fortune', 'cookies'] nltk.download('averaged_perceptron_tagger') # tagger를 다운로드한다.\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': \"[nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [nltk_data] Package averaged_perceptron_tagger is already up-to- [nltk_data] date! True nltk.pos_tag(tokenize_result) # nltk의 pos_tag이라는 함수를 사용하여 태깅해준다. [('I', 'PRP'), ('can', 'MD'), ('always', 'RB'), ('predict', 'VB'), ('the', 'DT'), ('fortune', 'NN'), ('cookies', 'NNS')] # tokenize와 pos tagging을 한번에 nltk.pos_tag(word_tokenize(sentence1)) [('I', 'PRP'), ('can', 'MD'), ('always', 'RB'), ('predict', 'VB'), ('the', 'DT'), ('fortune', 'NN'), ('cookies', 'NNS')] POS Tag List(영어기준. 한국어와 맞지 않는 품사도 있음) Number Tag Description 설명 1 CC Coordinating conjunction 2 CD Cardinal number 3 DT Determiner 한정사 4 EX Existential there 5 FW Foreign word 외래어 6 IN Preposition or subordinating conjunction 전치사 또는 종속 접속사 7 JJ Adjective 형용사 8 JJR Adjective, comparative 헝용사, 비교급 9 JJS Adjective, superlative 형용사, 최상급 10 LS List item marker 11 MD Modal 12 NN Noun, singular or mass 명사, 단수형 13 NNS Noun, plural 명사, 복수형 14 NNP Proper noun, singular 고유명사, 단수형 15 NNPS Proper noun, plural 고유명사, 복수형 16 PDT Predeterminer 전치한정사 17 POS Possessive ending 소유형용사 18 PRP Personal pronoun 인칭 대명사 19 PRP$ Possessive pronoun 소유 대명사 20 RB Adverb 부사 21 RBR Adverb, comparative 부사, 비교급 22 RBS Adverb, superlative 부사, 최상급 23 RP Particle 24 SYM Symbol 기호 25 TO to 26 UH Interjection 감탄사 27 VB Verb, base form 동사, 원형 28 VBD Verb, past tense 동사, 과거형\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': '29 VBG Verb, gerund or present participle 동사, 현재분사 30 VBN Verb, past participle 동사, 과거분사 31 VBP Verb, non-3rd person singular present 동사, 비3인칭 단수 32 VBZ Verb, 3rd person singular present 동사, 3인칭 단수 33 WDT Wh-determiner 34 WP Wh-pronoun 35 WP$ Possessive wh-pronoun 36 WRB Wh-adverb 불용어(Stop words) 제거 분석에 불필요한 전치사, 접속사, 저빈도 단어 등을 제거해 주면 정확한 분석 결화를 얻을 수 있다. 불용어를 제거해주는 도구들이 있지만 데이터에 따라 불용어로 취급되는 것들이 다를 수 있기 때문에 불용어 사전을 직접 만들어 제거해주는 것이 좋다. sentence2 = \"I play the violin when I\\'m thinking and sometimes I don\\'t talk for days on end.\" stop_words = \"on the\" # \"on 과 the\"를 불용어로 취급한다 # 불용어 사전에 등록한 단어들을 공백을 기준으로 나눠 리스트에 담는다 stop_words = stop_words.split(\\' \\') sentence2 = sentence2.split(\\' \\') # 문장을 토큰화한다. except_stop_words = [] # 불용어를 제거한 문장을 담을 리스트를 준비한다. for word in sentence2: # 토큰화된 문장의 단어들을 하나씩 방문해서 if word not in stop_words: # 해당 단어가 불용어 사전에 없으면 except_stop_words.append(word) # 미리 만든 리스트에 넣어라 except_stop_words # on과 the가 빠진 것을 확인할 수 있다.',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': '[\\'I\\', \\'play\\', \\'violin\\', \\'when\\', \"I\\'m\", \\'thinking\\', \\'and\\', \\'sometimes\\', \\'I\\', \"don\\'t\", \\'talk\\', \\'for\\', \\'days\\', \\'end.\\'] 불용어 제거 | NLTK import nltk nltk.download(\\'stopwords\\') from nltk import word_tokenize from nltk.corpus import stopwords # nltk에서 불러온 stopwords라는 불용어 사전의 words를 가져온다. 언어는 영어. stop_words1 = stopwords.words(\\'english\\') print(stop_words1) [nltk_data] Downloading package stopwords to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [\\'i\\', \\'me\\', \\'my\\', \\'myself\\', \\'we\\', \\'our\\', \\'ours\\', \\'ourselves\\', \\'you\\', \"you\\'re\", \"you\\'ve\", \"you\\'ll\", \"you\\'d\", \\'your\\', \\'yours\\', \\'yourself\\', \\'yourselves\\', \\'he\\', \\'him\\', \\'his\\', \\'himself\\', \\'she\\', \"she\\'s\", \\'her\\', \\'hers\\', \\'herself\\', \\'it\\', \"it\\'s\", \\'its\\', \\'itself\\', \\'they\\', \\'them\\', \\'their\\', \\'theirs\\', \\'themselves\\', \\'what\\', \\'which\\', \\'who\\', \\'whom\\', \\'this\\', \\'that\\', \"that\\'ll\", \\'these\\', \\'those\\', \\'am\\', \\'is\\', \\'are\\', \\'was\\', \\'were\\', \\'be\\', \\'been\\', \\'being\\', \\'have\\', \\'has\\', \\'had\\', \\'having\\', \\'do\\', \\'does\\', \\'did\\', \\'doing\\', \\'a\\', \\'an\\', \\'the\\', \\'and\\', \\'but\\', \\'if\\', \\'or\\', \\'because\\', \\'as\\', \\'until\\', \\'while\\', \\'of\\', \\'at\\', \\'by\\', \\'for\\', \\'with\\', \\'about\\', \\'against\\', \\'between\\', \\'into\\', \\'through\\', \\'during\\', \\'before\\', \\'after\\', \\'above\\', \\'below\\', \\'to\\', \\'from\\', \\'up\\', \\'down\\', \\'in\\', \\'out\\', \\'on\\', \\'off\\', \\'over\\', \\'under\\', \\'again\\', \\'further\\', \\'then\\', \\'once\\', \\'here\\', \\'there\\', \\'when\\', \\'where\\', \\'why\\', \\'how\\', \\'all\\', \\'any\\', \\'both\\', \\'each\\', \\'few\\', \\'more\\', \\'most\\', \\'other\\', \\'some\\', \\'such\\', \\'no\\', \\'nor\\', \\'not\\', \\'only\\', \\'own\\', \\'same\\', \\'so\\', \\'than\\', \\'too\\', \\'very\\', \\'s\\', \\'t\\', \\'can\\', \\'will\\', \\'just\\', \\'don\\', \"don\\'t\", \\'should\\', \"should\\'ve\", \\'now\\', \\'d\\', \\'ll\\', \\'m\\', \\'o\\', \\'re\\', \\'ve\\', \\'y\\', \\'ain\\', \\'aren\\', \"aren\\'t\", \\'couldn\\', \"couldn\\'t\", \\'didn\\', \"didn\\'t\", \\'doesn\\', \"doesn\\'t\", \\'hadn\\', \"hadn\\'t\", \\'hasn\\', \"hasn\\'t\", \\'haven\\', \"haven\\'t\", \\'isn\\', \"isn\\'t\", \\'ma\\', \\'mightn\\', \"mightn\\'t\", \\'mustn\\', \"mustn\\'t\", \\'needn\\', \"needn\\'t\", \\'shan\\', \"shan\\'t\", \\'shouldn\\', \"shouldn\\'t\", \\'wasn\\', \"wasn\\'t\", \\'weren\\', \"weren\\'t\", \\'won\\', \"won\\'t\", \\'wouldn\\', \"wouldn\\'t\"] [nltk_data] Unzipping corpora\\\\stopwords.zip.',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': 'sentence2 = \"I play the violin when I\\'m thinking and sometimes I don\\'t talk for days on end.\" s2_tokenize = word_tokenize(sentence2) except_stop_words1 = [] for w in s2_tokenize: if w not in stop_words1: except_stop_words1.append(w) except_stop_words1 # the, when, and, do, for, on이 불용어로 처리되어 빠졌다. [\\'I\\', \\'play\\', \\'violin\\', \\'I\\', \"\\'m\", \\'thinking\\', \\'sometimes\\', \\'I\\', \"n\\'t\", \\'talk\\', \\'days\\', \\'end\\', \\'.\\'] 철자교정 텍스트의 오탈자를 고쳐준다 # !pip install autocorrect from autocorrect import Speller # Speller 모듈을 불러온다 spell1 = Speller(\\'en\\') # 영어 철자를 검사할 수 있게 객체를 생성한다. print(spell1(\\'spelleerr\\')) # 철자가 틀린 단어를 넣어본다 speller wrong_sentence1 = \\'I caan alwayss predicta the fortunie cookiess\\' s1_tokenize = word_tokenize(wrong_sentence1) # s1_tokenize 리스트 안에 있는 단어를 하나씩 돌면서 spell1객체에 통과시켜 철자를 확인하고 # 단어 사이에 공백을 넣어 join한다. check_spell = \\' \\'.join([spell1(s) for s in s1_tokenize]) print(check_spell) # caan, alwayss, predicta, fortunie, cookiess 다 잘 고쳐졌다 I can always predict the fortune cookies 단수화 복수화 from textblob import TextBlob sentence1 = \\'I can always predict the fortune cookies\\' textblob1 = TextBlob(sentence1) # 문장을 토큰화한다. print(textblob1.words) # 토큰화 결과 print(textblob1.words.singularize()) # 토큰화한 것을 단수화한 결과 # always에서 s를 빼버렸다... 복수로 인식했나보다... print(textblob1.words.pluralize()) # 그냥 다 s를 붙여버리네 [\\'I\\', \\'can\\', \\'always\\', \\'predict\\', \\'the\\', \\'fortune\\', \\'cookies\\'] [\\'I\\', \\'can\\', \\'alway\\', \\'predict\\', \\'the\\', \\'fortune\\', \\'cookie\\'] [\\'we\\', \\'cans\\', \\'alwayss\\', \\'predicts\\', \\'thes\\', \\'fortunes\\', \\'cookiess\\']',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': \"Stemming(어간추출) import nltk # nltk에서 제공하는 어간추출 라이브러리 중 가장 많이 사용된다. stemmer = nltk.stem.PorterStemmer() stemmer.stem('stemming') # 어간을 추출해보자 'stem' Lemmatization(표제어 추출) import nltk nltk.download('wordnet') from nltk.stem.wordnet import WordNetLemmatizer lemmatizer1 = WordNetLemmatizer() lemmatizer1.lemmatize('holds') [nltk_data] Downloading package wordnet to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [nltk_data] Package wordnet is already up-to-date! 'hold' NER(Named Entity Recognition, 개채명 인식) 고유명사와 같은 개체의 이름을 인식한다. import nltk from nltk import word_tokenize nltk.download('maxent_ne_chunker') # max named entity chunker를 다운받는다 nltk.download('words') sentence1 = 'The name\\\\'s Sherlock Holmes and the address is 221B Baker Street.' s1_pos_tok = nltk.pos_tag(word_tokenize(sentence1)) # tokenize, pos tagging을 해준다 s1_named = nltk.ne_chunk(s1_pos_tok, binary=True) print(s1_named) (S The/DT name/NN 's/POS (NE Sherlock/NNP Holmes/NNP) and/CC the/DT address/NN is/VBZ 221B/CD Baker/NNP Street/NNP ./.) [nltk_data] Downloading package maxent_ne_chunker to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [nltk_data] Package maxent_ne_chunker is already up-to-date! [nltk_data] Downloading package words to [nltk_data] C:\\\\Users\\\\yein4\\\\AppData\\\\Roaming\\\\nltk_data... [nltk_data] Package words is already up-to-date! Lexical Ambiguity(단어 중의성 해소) import nltk from nltk.wsd import lesk ambiguous_sentence1 = 'I went to the bank of the river' ambiguous_sentence2 = 'I went to the bank to deposit some money' print(lesk(word_tokenize(ambiguous_sentence1), 'bank')) # 이건 잘 파악하지 못했지만 print(lesk(word_tokenize(ambiguous_sentence2), 'bank')) # 얘는 잘 파악했다 Synset('bank.v.07') Synset('savings_bank.n.02')\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': \"BOS(Bag of Words) # 벡터 개수 세는 라이브러리를 불러온다. from sklearn.feature_extraction.text import CountVectorizer bos_sentence = ['betty bought a bit of better butter to make her bitter butter better'] count_words1 = CountVectorizer() # vector 세어줄 라이브러리를 담은 객체를 생성한다 bow1 = count_words1.fit_transform(bos_sentence) # 문장에 fit_transform해준다 print(bow1.toarray()) # 벡터 센 결과를 리스트 형태로 출력한다. print(count_words1.vocabulary_) # 결과에서 어떤 단어에 대한 카운트가 어느 인덱스에 있는지 출력한다. [[2 1 1 1 1 2 1 1 1 1]] {'betty': 1, 'bought': 4, 'bit': 2, 'of': 8, 'better': 0, 'butter': 5, 'to': 9, 'make': 7, 'her': 6, 'bitter': 3} # 불용어는 제거하고 카운트를 셀 수 있다. count_words2 = CountVectorizer(stop_words='english') bow2 = count_words2.fit_transform(bos_sentence) print(bow2.toarray()) print(count_words2.vocabulary_) # of, to, her이 불용어로 처리되어 제거됐다. [[2 1 1 1 1 2 1]] {'betty': 1, 'bought': 4, 'bit': 2, 'better': 0, 'butter': 5, 'make': 6, 'bitter': 3} # 보기 편하게 DataFrame으로 바꾸자 import pandas as pd from pandas import DataFrame bos_index = count_words2.vocabulary_ bos_result = bow2.toarray() bos_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.)\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': '# 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, # 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다. # 이렇게하면 index 순으로 정렬된다. for k, v in sorted(bos_index.items(), key= lambda i : i[1]): bos_list.append(k) # 리스트에 순서대로 담는다 # 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트 bos_df = DataFrame(bos_result, columns= bos_list) bos_df better betty bit bitter bought butter make 0 2 1 1 1 1 2 1 BOS | 한국어 kor_bos_sentence = [\\'내가 어제 호떡을 먹고 싶어서 사 온 건데 나를 빼고 다 먹었더라고\\'] count_words1 = CountVectorizer() kor_bow = count_words1.fit_transform(kor_bos_sentence) print(kor_bow.toarray()) print(count_words1.vocabulary_) # 한국어는 띄어쓰기를 기준으로 나뉜 벡터를 세면 부정확하다. [[1 1 1 1 1 1 1 1 1]] {\\'내가\\': 2, \\'어제\\': 7, \\'호떡을\\': 8, \\'먹고\\': 3, \\'싶어서\\': 6, \\'건데\\': 0, \\'나를\\': 1, \\'빼고\\': 5, \\'먹었더라고\\': 4} from konlpy.tag import Mecab import re import MeCab kor_bos_sentence2 = \\'내가 어제 호떡을 먹고 싶어서 사 온 건데 나를 빼고 다 먹었더라고.\\' kor_tagger = Mecab(dicpath=r\"C:\\\\mecab\\\\mecab-ko-dic\") # 온점은 없애고 형태소 분석한 결과를 kor_tokens에 넣는다 kor_tokens = kor_tagger.morphs(re.sub(\\'(\\\\.)\\', \"\", kor_bos_sentence2)) print(kor_tokens)',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': \"['내', '가', '어제', '호떡', '을', '먹', '고', '싶', '어서', '사', '온', '건데', '나', '를', '빼', '고', '다', '먹', '었', '더라고'] kor_vocab = {} # 토큰화된 단어들을 {토큰:인덱스}형태로 저장할 딕셔너리를 만든다 kor_bow2 = [] # bow결과를 저장할 리스트를 만든다 for tok in kor_tokens: # 토큰화된 것을 하나씩 돌면서 if tok not in kor_vocab.keys(): # kor_vocab이라는 딕셔너리의 key값에 tok이 없으면 # 딕셔너리 key값으로 tok을 하나 넣고 그의 value값은 딕셔너리의 길이, 즉 원소 개수 kor_vocab[tok] = len(kor_vocab) # 그리고 리스트 kor_bow에는 '딕셔너리 길이 -1'인덱스 자리에 1을 넣는다 kor_bow2.insert(len(kor_vocab)-1, 1) else: # kor_vocab이라는 딕셔너리의 key값에 tok이 있으면 index1 = kor_vocab.get(tok) # 딕셔너리에서 key값으로 tok을 가지는 것의 value값을 index1에 넣는다. # 리스트에서 가져온 인덱스 값에 해당하는 것에 1을 더한다. kor_bow2[index1] = kor_bow2[index1] + 1 print(kor_bow2) print(kor_vocab)\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': '[1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] {\\'내\\': 0, \\'가\\': 1, \\'어제\\': 2, \\'호떡\\': 3, \\'을\\': 4, \\'먹\\': 5, \\'고\\': 6, \\'싶\\': 7, \\'어서\\': 8, \\'사\\': 9, \\'온\\': 10, \\'건데\\': 11, \\'나\\': 12, \\'를\\': 13, \\'빼\\': 14, \\'다\\': 15, \\'었\\': 16, \\'더라고\\': 17} import pandas as pd from pandas import DataFrame kor_bow2_1 = [kor_bow2] kor_bow_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.) # 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, # 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다. # 이렇게하면 index 순으로 정렬된다. for k, v in sorted(kor_vocab.items(), key= lambda i : i[1]): kor_bow_list.append(k) kor_dtm_df = DataFrame(kor_bow2_1, columns= kor_bow_list) #kor_dtm_df = DataFrame(kor_bow2, index= kor_bow_list) kor_dtm_df 내 가 어제 호떡 을 먹 고 싶 어서 사 온 건데 나 를 빼 다 었 더라고 0 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 DTM(Document Term Matrix, 문서 단어 행렬) 문서에 등장하는 단어들의 빈도를 기반으로 행렬을 표현하는 것이다. 즉, 문서에 대한 BOW를 행렬로 표현하는 것이다. from sklearn.feature_extraction.text import CountVectorizer doc1 = [\"Crime is common.\", \" Logic is rare.\", \"Therefore it is upon the logic rather than upon the crime that you should dwell.\"] count_words1 = CountVectorizer() doc_bow = count_words1.fit_transform(doc1)',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': \"print(doc_bow.toarray()) # 각 행은 문서 하나, 열은 해당 문서의 단어를 나타낸다. print(count_words1.vocabulary_) # 인덱스 [[1 1 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 1 0 1 1 0 0 0 0 0 0 0 0] [0 1 1 1 1 1 0 1 1 1 1 2 1 2 1]] {'crime': 1, 'is': 3, 'common': 0, 'logic': 5, 'rare': 6, 'therefore': 12, 'it': 4, 'upon': 13, 'the': 11, 'rather': 7, 'than': 9, 'that': 10, 'you': 14, 'should': 8, 'dwell': 2} # 보기 편하게 DataFrame으로 바꾸자 import pandas as pd from pandas import DataFrame index2 = count_words1.vocabulary_ bow_result = doc_bow.toarray() dtm_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.) # 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, # 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다. # 이렇게하면 index 순으로 정렬된다. for k, v in sorted(index2.items(), key= lambda i : i[1]): dtm_list.append(k) # 리스트에 순서대로 담는다 # 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트 dtm_df = DataFrame(bow_result, columns= dtm_list) dtm_df common crime dwell is it logic rare rather should than that the therefore upon you 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 2 0 1 1 1 1 1 0 1 1 1 1 2 1 2 1\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': 'TF-IDF(Term Frequency - Inverse Document Frequency) 문서에 등장한 단어를 단순히 count하는 방법이 아니라 다른 문서에 비해 더 많이 나온 단어를 알아보기 위한 방법이다. 다른 문서보다 특정 문서에서 더 많이 등장한 단어가 해당 문서의 핵심어일 가능성이 높다는 가정 하에 만들어진 방법이다. TF-IDF는 Term Frequency * Inverse Document Frequency로 계산된다. Term Frequency (앞서 다룬 DTM이랑 같다. 특정 문서에 나온 단어들의 빈도수), Inverse Document Frequency({전체문서 수}/{특정 단어가 들어있는 문서의 개수 + 1}에 로그를 씌워준 것) scikit-learn에서 tfidfvectorizer를 사용하여 구현할 수 있다. from sklearn.feature_extraction.text import TfidfVectorizer tfidf1 = TfidfVectorizer().fit(doc1) # doc1에 대해 tf-idf를 fit시킨다. print(tfidf1.transform(doc1).toarray()) print(tfidf1.vocabulary_)',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': \"[[0.72033345 0.54783215 0. 0.42544054 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0. 0. 0.42544054 0. 0.54783215 0.72033345 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0.18177122 0.2390073 0.14116156 0.2390073 0.18177122 0. 0.2390073 0.2390073 0.2390073 0.2390073 0.47801461 0.2390073 0.47801461 0.2390073 ]] {'crime': 1, 'is': 3, 'common': 0, 'logic': 5, 'rare': 6, 'therefore': 12, 'it': 4, 'upon': 13, 'the': 11, 'rather': 7, 'than': 9, 'that': 10, 'you': 14, 'should': 8, 'dwell': 2} import pandas as pd tfidf_index = tfidf1.vocabulary_ tfidf_result = tfidf1.transform(doc1).toarray() tfidf_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.) # 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, # 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다. # 이렇게하면 index 순으로 정렬된다. for k, v in sorted(tfidf_index.items(), key= lambda i : i[1]): tfidf_list.append(k) # 리스트에 순서대로 담는다 # 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트 tfidf_df = DataFrame(tfidf_result, columns= tfidf_list) tfidf_df common crime dwell is it logic rare rather should than that the therefore upon you\",\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': '0 0.720333 0.547832 0.000000 0.425441 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1 0.000000 0.000000 0.000000 0.425441 0.000000 0.547832 0.720333 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2 0.000000 0.181771 0.239007 0.141162 0.239007 0.181771 0.000000 0.239007 0.239007 0.239007 0.239007 0.478015 0.239007 0.478015 0.239007 × Search',\n",
       "  'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/'},\n",
       " {'text': 'NLP란 컴퓨터가 인간의 언어를 이해할 수 있게 처리하는 것이 NLP이다. 인간은 언어라는 도구로 구어와 문어를 구사한다. 컴퓨터는 인간이 둘 중 어떤 방식으로 언어를 전달하더라도 그의 의미를 이해할 수 있어야한다. 컴퓨터가 인간의 언어를 이해하기 위해서 우선 언어의 형태를 분석하고(형태소 분석) 그를 기반으로 각각의 의미를 분석한 후 전체 의미를 해석할 수 있어야한다. 예를 들어 누군가 완벽히 조립해 놓은 레고 모형이 있다고 생각해보자. 이 모형이 어떻게 만들어진 것인지 알기 위해 우선 레고를 하나하나 분해 한 후 각각의 레고 조각들이 어느 부위에서 어떻게 쓰였던 것인지 그의 역할에 대해 생각해 보고 다시 그 레고를 스스로 조립해보면 해당 모형에 대해 이해할 수 있을 것이다. 이와 같은 흐름으로 컴퓨터는 인간의 언어를 이해한다. NLP의 최종 목표는 다양한 task(질의 응답, 번역, 다양한 업무 수행, 예약하기, 물건구매 등)를 처리하기 위해 컴퓨터가 자연어를',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '완벽히 이해할 수 있도록 하는 것이다.',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '현재 NLP 응용분야로는 맞춤법 검사, 키워드 검색, 동의어 탐색, 정보 추출, 문서 분류, 기계번역, 질의응답 등이 있다. What’s Natural Language? 자연어란 인간이 일상 생활에서 사용하는 언어를 가리킨다. 인간의 언어는 인간이 의사소통에서 사용하는 도구로 표현될 수 있는데 언어라는 도구를 사용하여 인간은 서로 신호를 보내고 받는 행위를 한다. 일반적으로 언어라 통칭되는 것은 linguistic entity가 아니라 단순 symbol이다. 즉, 하나의 단어는 기의(시니피에)과 기표(시니피앙)을 mapping한 것이다. 예를 들어 “책상”이라는 단어는 해당 물체를 가리키는 단어이다. Symbol은 여러 방식으로 표현될 수 있는데 어떠한 방식으로든 언어는 연속적인 신호들(continuous signals)로 뇌에 전달된다. (기호학)',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '이러한 행위는 위에서 언급한 바와 같이 구어 혹은 문어로 표현될 수 있다. 어떠한 방식으로 표현이 되어도 본질적으로 전달하려는 의미는 변하지 않는다. 따라서 컴퓨터가 자연어를 완벽히 처리하기 위해서는 자연어가 어떤 방식으로 들어가도 모두 이해할 수 있어야 한다. What’s Deep Learninig?',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '딥러닝은 머신러닝의 하위 분야이다. 머신러닝은 human-designed representations과 input features로 기계를 학습시킨다. 뒤에서 에서 더 자세히 다룰 것이지만 간단히 말하자면 기계학습은 위에서 언급한 human-designed representations과 input features와 같이 정해진 condition 즉, 인간이 선택한 Feature로 최적의 결과를 예측할 수 있는 “weight(parameter)”(변수값)를 찾는 optimization 역할을 한다. 여기에서 과거 머신러닝을 통해 이루어지던 자연어처리가 큰 성과를 거두지 못한 이유가 드러난다. 인간이 사용하는 언어를 기계에 학습시키기 위해 인간이 학습시킬 Feautre들을 선택하는 것은 너무 인간 중심적 생각이다. 물론 인간이 중심이 되어야 하지만, 선생님이 학생들의 눈높이에 맞춰 강의를 진행하지 않고 본인 지식을 중심으로 강의를 진행한다면 학생들은 강의 내용을 제대로 이해하지 못 할 것이다. 가장 좋은 방법은 학생들에게 날것의 지식을 던져주고 자유롭게 창의성을 키우도록 길을 잡아주는 것이다. 자연어처리도 마찬가지이다. 인간은 선생이고 기계가 학생인 입장에서 가장 좋은',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '방법은 기계에게 raw-data를 던져주고 기계가 자동으로 그의 Features를 찾아 그를 기반으로 optimizing하는 것이다. 이는 딥러닝(Deep Neural Network)을 통해 가능하기 때문에 딥러닝이 도입된 후 자연어처리가 빛을 보기 시작한 것이다. (딥러닝에서는 지도학습(supervised)과 비지도학습(unsupervised)이 모두 가능하다.)',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '딥러닝 알고리즘은 위에서 언급한 바와 같이 머신러닝 알고리즘보다 뛰어난 성능을 자랑하며 많은 관심을 받고 있다. 딥러닝이 가능해진 이유는 크게 컴퓨터 성능의 발전과 새로운 모델, 알고리즘 그리고 아이디어들의 등장 덕분이다. 컴퓨터 성능의 발전으로 많은 양의 데이터를 빠르게 처리할 수 있게 되었고 이를 기반으로 새로운 알고리즘과 모델들을 구현할 수 있게 되었다. 딥러닝은 세상에 대한, 즉 시각적으로 존재하는 것들 혹은 인간이 구사하는 언어와 같은 세상에 대한 정보를 flexible하고 universal하며 learnable한 framework를 제공한다. 딥러닝이 raw-data의 Features를 찾아내는 방식은 이후 더 자세히 다룰 것이지만 여기에서 한번개괄적으로 설명하고 지나가겠다.',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '이 그림에서 보이는 동그라미들을 Neuron이라고 부른다. 딥러닝에서는 그림과 같은 구조 통해 Neuron들이 input data(x)의 features를 찾아낸다. 그림을 예로 들자면 위와 같은 구조를 통해 기계는 고양이에 대한 정보를 고양이의 얼굴, 꼬리, 발, 털 등으로 분해하여 각각의 Neuron에 담아 고양이에 대한 정보를 학습한다. 이처럼 딥러닝은 raw data(e.g., sound, characters of words)인 input data로부터 좋은 features를 자동으로 추출하여 학습하는데 이를 다른 말로 Representation learning이라고도 한다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) × Search',\n",
       "  'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/'},\n",
       " {'text': '1. Architectures for Coreference Algorithms coreference에 대한 세 가지 일반적인 아키텍쳐는 mention-pair, mention-rank, 그리고 entity-based로 각각 feature-based 혹은 neural classifier를 사용할 수 있다. 우선 feature-based 알고리즘에 대해서 설명하겠다. categorization을 사용한 시스템의 다양한 architecture를 개괄적으로 살펴보자. 알고리즘은 크게 entity-based(담화모델에서 각 개체를 나타내는 방식으로 coreference를 결정)인지, mention-based(각 mention을 독립적으로 고려하는 방식으로 coreference를 결정)인지 혹은 잠재적 선행사를 직접적으로 비교하기 위해서 ranking model을 사용하는지의 여부에 따라 구분된다. 1.1 The Mention-Pair Architecture 먼저 mention-based algorithm인 mention-pair architecture에 대해서 알아보겠다. mention-pair architecture는 이름에서도 알 수 있듯이 한 쌍의 mention, candidate anaphor그리고 candidate antecedent(후보선행사)가 주어지는 classifier를 기반으로 하며, binary classification(이항분류; corefering하는지 안 하는지)를 통해 결론을 도출한다. Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3 million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '위 예문에 이 classifier의 task를 대입해보면 그림 22.2와 같이 된다. 이것은 예문에 있는 대명사 she와 그에 대한 잠재적 선행사 mention이 이루는 쌍에 대해 coreference link의 확률을 할당하는 것을 보여주는 그림이다. 여기에서 각각의 Victoria Chen, Megabucks Banking, her과 같은 prior mention에 대해 binary classifier는 she의 선행사가 그 mention인지 아닌지 확률을 계산한다. 여기에서 우리는 이 확률을 실제 선행사(Victoria Chen, her, the 38-year-old)에 대해서는 높이고 선행사가 아닌 것(Megabucks Banking, her pay)에 대해서는 낮춰야 한다. 1.1.1 Training 훈련을 위해서는 training samples를 선택해야 하는데 이때 문서의 대부분 mention쌍은 corefernent가 아닌데 이것을 다 훈련 샘플로 선택하면 너무 많은 negative sample이 생기기 때문에 heuristic method가 필요하다. 가장 일반적인 heuristic은 positive example로 가장 가까운 선행사를 택하고, 그 사이에 있는 모든 쌍들은 negative example로 선택하는 것이다. 이것을 조금 formal하게 표현하자면, anaphor mention $m_i$에 대해서 $m_j$는 $m_i$의 가장 가까운 선행사인 positive instance($m_i$, $m_j$)를 만들고, $m_j$와 $m_i$사이에 있는 $m_k$에 대한 negative instance ($m_i$, $m_k$)를 만든다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '1.1.2 Clustering Classifier가 훈련되고 나면 clustering단계에서 test sentence에 적용된다. clustering에는 두 가지 과정이 있다: closest-first clustering과 best-first clustering. - closest-first clustering closest-first clustering에서 classifier는 오른쪽에서 왼쪽으로, 즉 mention $i-1$에서 mention 1까지로 실행된다. 그리고 확률 0.5가 넘는 first antecedent는 $i$와 연결된다. 선행사가 확률 0.5를 넘지 못하는 경우에는 $i$에 대한 선행사가 선택되지 않는다. - best-first clustering best-first clustering에서 classifier는 모든 선행사들에 대해서 실행되는데, 가장 유력한 antecedent mention이 $i$의 선행사로 선택된다. 1.1.3 Problems mention-pair model은 단순하다는 장점이 있지만 두 가지 주요한 문제가 있다. 1) classifier가 직접적으로 후보 선행사들을 비교하지 못한다. 따라서 두 선행사 사이에서 실제로 더 나은 것이 무엇인지 결정하는 훈련이 이루어지지 못한다. 2) 담화 모델을 무시하며, entity가 아닌 단지 mention만을 살핀다. 각 classifier decistion은 동일한 entity의 다른 mention들을 고려하지 않고 한 쌍에 대해서만 완전히 국부적으로 이루어진다. 1.2 The Mention-Rank Architecture',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'Mention-Rank Architecture는 mention-pair model의 결점을 보완한 model이다. mention-pair model의 단점 중 하나는 선행사 후보를 직접적으로 비교하지 않는다는 것이다. mention-ranking model은 이를 보완하여 선행사 후보를 직접적으로 비교하며 각 anaphor에 대해서 높은 점수의 선행사를 선택한다. mention-ranking systems에서는 mention(anaphor)에 $i-1$에서 1까지, 그리고 여기에 추가적으로 선행사가 없다는 것까지 고려하기 위한 dummy mention $\\\\epsilon$까지의 범위에 걸쳐 관련된 랜덤 변수 $y_1$을 가진다. dummy mention $\\\\epsilon$은 discourse-new나 새로운 coref chain의 시작 혹은 non-anaphoric을 나타낸다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '그림 22.3은 single candidate anaphor she에 대한 계산 예를 보여준다. mention-ranking system은 후보 anaphoric mention she에 대해 이전의 모든 mention들, 그리고 추가적으로 특수한 dummy mention $\\\\epsilon$에 대해 소프트맥스를 계산하여 확률분포를 할당한다(각 후보 선행사들에 대한 확률을 구한다). 이전의 mention-pair model의 훈련은 단순했는데, mention-ranking model의 훈련은 비교적 까다롭다. 왜냐하면 mention-pair model에서는 positive-sample / negative-sample을 만들어서 훈련에 사용되는 sample pair를 다 알았는데, mention-ranking model은 각 anaphor에 대해서 훈련을 위해 사용할 수 있는 모든 gold antecedent들을 알 수 없기 때문이다. 대신 각 mention에 대한 best antecedent는 잠재되어 있다; 즉 각 mention에 대해서 선택할 수 있는 legal gold antecednet의 전체 클러스터를 가지고 있다. 이런 잠재된 선행사를 모델링하는 다양한 방법들이 존재하는데, 이는 3장에서 상세히 다룰 것이다. Mention-ranking model 은 hand-build feature와 neural representation learning 둘 다를 통해 구현될 수 있느데, 전자는 2장에서, 후자는 3장에서 자세히 다룰 것이다. 1.3 Entity-based Models 앞서 소개한 mention-pair model과 mention-ranking model 둘 다 mention-based algorithm이었고, 이번에 소개할 모델은 entity-based model이다. - entity-based model',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'entity-based model은 각 mention을 이전 mention이 아닌 이전 담화 개체(mentions들의 cluster)에 연결한다. - entity-ranking model entity-ranking model은 단순히 mention-ranking model에서 classifier가 개별 mention들이 아니라 mention들의 cluster들에 대해 결정을 내리게 한 것이다. entity-based model도 mention-ranking model처럼 feature-based model과 neural model 둘 다를 통해 구현될 수 있는데, 이전의 모델들보다 표현력이 더 뛰어나지만 cluster-level 정보를 이용하는 것이 실제로 큰 성능 향상으로 이어지지 않았기 때문에 mention-ranking model이 여전히 더 많이 사용된다. 그래서 entity-based model은 간단하게만 소개하고 넘어가도록 하겠다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'feature-based model의 경우 클러스터에서 feature를 추출하여 이를 수행할 수 있다. Neural model은 cluster의 representation(cluster를 vector로 표현하는 것)을 자동으로 학습할 수 있다. 예를 들어 cluster representation에 해당하는 상태를 인코딩하기 위해 일련의 cluster mention에 대해 RNN을 이용하거나, mention pair의 학습된 representation에 대해 pooling하여 cluster쌍에 대한 distiributed(dense) representation을 학습한다. ‘Distiributed’라는 말이 붙는 이유는 하나의 정보가 여러 차원에 분산되어 표현되기 때문이다. 하나의 차원이 여러 속성들이 버무려진 정보를 들고 있다. 즉, 하나의 차원이 하나의 속성을 명시적으로 표현하는 것이 아니라 여러 차원들이 조합되어 나타내고자 하는 속성들을 표현하는 것이다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '2. Classifiers using hand-built features 이번 장에서는 coreference resolution을 위해 logistic regression, SVM 또는 random forest classifier에서 일반적으로 사용되는 feature들에 대해 기술한다. anaphor mention과 potential antecedent mention이 있을 때 대부분의 feature based classifier는 세 가지 유형의 feature를 사용한다: 1) anaphor에 대한 feature, 2) 선행사 후보에 대한 feature, 3) 쌍 사이의 관계에 대한 feature. Entity-based model은 추가적으로 두 개의 클레스를 더 사용할 수 있다: 4) 선행사의 개체 클러스터로부터 온 모든 mention들에 대한 feature, 5) 선행사 entity cluster의 mention과 anaphor간의 관계에 대한 feature. 그림 22.4는 일반적으로 사용되는 feature들을 보여준다. 앞서 사용한 예문에서 잠재적 anaphor she와 잠재적 선행사 Victoria Chen에 대한 것이다. 노란색 박스로 표시된 것은 이전 연구에서 특히 유용하다고 밝혀진 feature이고 파란색 박스로 표시된 것은 neural model에 사용했을 때 도움이 될 수 있는 feature이다. Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3 million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks. 위는 Aanphor나 선행 mention에 대한 feature들이다. 위는 선행사 entity에 대한 feature(여기에서는 she에 대한 선행사니까 Victoria Chen, her, the 38-year-old에 대한 것; 그래서 Antecedent cluster size가 3)이다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '위는 mention쌍에 대한 feature이다. Longer anaphor는 anaphor가 선행사보다 기냐 Paris of any features는 선행사 + anaphor 유형쌍에 대한 각 feature. 어떤 feature든 선행사 + anaphor쌍으로. Sentence distance는 선행사와 anaphor 사이에 있는 문장 수 Mention distance는 선행사와 anaphor 사이에 있는 mention 수 i-within-i는 어떠한 mention이 다른 mention에 포함되어 있으며 두 mention의 reference가 같은 것. Appositive 동격(앞의 내용을 명사구나 다른 명사를 사용해 정의하거나 설명하는 것). anaphor가 선행사와 동격관계에 있는 경우. 이것은 mention들이 동격어가 아닌 경우에도 유용하다. 동격어를 선행 헤드와 연결하는 것을 알기 위해. 위는 entity쌍에 대한 feature이다. Exact String Match는 determiner와 modifier를 모두 포함하여 정확히 동일한 텍스트를 포함하는 경우에만 두 mention을 연관시키는 것이다. 즉 선행사와 anaphor cluster에 있는 아무 두 mention의 string(문자열)이 같으냐 Head Word Match는 선행사 cluster에 있는 mention과 anaphor cluster의 mention이 같은 headword를 가지냐 Word Inclusion는 선행사 cluster에 있는 단어가 anaphor cluster에 있는 모든 단어를 포함하냐 그 아래에 문서에 대한 feature는 장르가 뭔지에 대한 것이다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'feature-based system에서 feature들의 결합을 사용하는 것은 중요하다고 한다. 한 실험에서는 classifier의 개별 feature들을 사용한 것과 다중 feature의 결합을 사용했을 때를 비교해 봤을 때 결합한 것의 F1 score가 4점 높았다고 한다. 3. A neural mention-ranking algorithm 초기 classifier들은 2장에서 소개한 hand-built feature를 사용했고, 보다 최근의 classifier들은 지금 소개할 neural representation learning을 사용한다. 이번 장에서는 Lee et al. (2017b)의 neural mention-ranking system에 대해 기술하겠다. 이 end-to-end system은 별도의 mention-detection단계를 가지고 있지 않다. 대신 가능한 모든 텍스트 범위를 설정된 length(예를 들어 길이 1, 2, 3, … N의 모든 n-gram)까지를 가능한 mention으로 간주한다(Lee의 논문에서는 length를 10으로 설정했다). 그림 22.5는 span representation과 mention score에 대한 계산을 보여준다. 그림 22.6은 그림 22.5의 예시 문장에서 the company의 가능한 세 가지 선행사에 대한 score $s$를 계산하는 것을 보여준다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '위 부분을 살펴보기 위해 나중에 계산될 span representations $g_i$에 대해서 잠시 얘기하도록 하겠다. span representations $g_i$는 스팬에서의 첫 번째 단어와 마지막 단어의 contextual representation 그리고 스팬에 있는 headword에 대한 representation 그리고 feature 하나로 구성된다. 여기에서 스팬의 첫 번째 단어와 마지막 단어의 contextual representation이 standard biLSTM으로 계산된다. 그리고 biLSTM은 ELMo와 같은 contextual word embedding을 기반으로 해서 각 단어에 대한 representation $w_t$를 input으로 받는다 (ELMo대신 BERT를 사용하면 성능이 훨씬 향상된다). 그러니까 biLSTM은 $w_t$를 input으로 받아서 output으로 $h_t$를 내는 것이다. 위 부분은 스팬의 head를 나타내기 위한 부분이다. 시스템은 스팬의 head를 나타내기 위해 스팬의 단어들에 대해 attention을 사용하였다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'attention에서와 마찬가지로 시스템은 가중치 벡터 $w_{\\\\alpha}$를 학습하고 FFNN(Feed Forward Neural Networks)에 의해 변화된 hidden state $h_t$를 내적(dot product)해서 attention score를 구한다. 그리고 그 다음에 어텐션 과정에서 attention score를 softmax에 통과시켜서 attention weight을 구한 것 처럼 여기에서도 attention score를 softmax에 통과시켜 분포를 정규화한다. attention distribution은 스팬 $i$에 있는 단어들을 attention-weighted sum(attention weight값들을 각각 모두 곱하고 최종적으로 모두 더한 값)을 한 vector $h_{ATT(i)}$를 생성하는데 사용된다. span representation $g_i$는 이전에 말한 것처럼 스팬의 시작과 끝의 hidden representation(hidden state 값), head 그리고 스팬의 $i$의 length같은 feature vector(이전 장에서 neural model에서 유용할 수 있는 feature 중 하나)를 모두 concatenate한 것이다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '위 그림은 the company의 가능한 세 가지 선행사에 대한 score $s$를 계싼하는 것을 보여주는 그림이다. antecedent score부분을 보면 the company에 대해서 General Electric이 선행사인 경우와 the Postal Service가 선행사인 경우 각각이 들어가는 것을 볼 수 있고, 이들의 요소별 곱도 들어가며, 추가적으로 다른 feature들도 들어간다. $m(i)$와 $c(i,j)$는 scoring 함수로, 각각 mention score, antecedent score를 나타내며 둘 다 스팬 $i$를 나타내는 백테 $g_i$를 기반으로 한다. FFNN(Feed Forward Neural Network). 가중치의 반복적인 업데이트. 1) 인풋 $x$를 받아서 2) 이것의 $y=Wx+b$를 계산하고 3) 여기에 activation function(sigmoid, tanh, ReLU, etc.)를 적용 선행사 score $c(i, j)$는 input으로 스팬 $i$와 $j$의 representation을 취하며, $g_i \\\\circ g_j$($g_i$와 $g_j$의 요소별 곱)은 두 스팬의 요소별 유사도이다. 그리고 마지막에는 mention distances 그리고 화자와 장르에 대한 정보와 같은 유용한 feature들을 인코딩한 feature vector $\\\\varphi(i, j)$가 들어간다. 이제 coreference score $s$를 보겠다. score $s(i, j)$는 세 가지 요소를 포함한다:',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '$m(i)$: span $i$가 mention인지에 대한 여부; $m(j)$: span $j$가 mention인지에 대한 여부; $c(i, j): $j$가 $i$의 선행사인지에 대한 여부를 포함한다. dummy 선행사의 경우에, score $s(i, \\\\epsilon )$은 0으로 고정된다. 이 방법은 nondummy score가 양수면 모델이 가장 높은 점수의 선행사를 예측하지만 만약 모든 점수가 다 음수라면 제외하는 방법이다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'task는 각 스팬 $i$에 선행사 $y_i$를 할당하고, 이전 스팬 그리고 special dummy token $\\\\epsilon$에 랜덤 변수, 즉 확률 변수를 할당하는 것이다. 여기 식을 보면 exponential score $i$, $y_i$의 summation 분의 exponential score $i$, $y_i$해서 선행사에 대한 확률값 $P(y_i)$가 나오는 것을 볼 수 있다. 만약 dummy token의 확률이 제일 높게 나온다면 $i$가 discourse-new이고 새로운 coreference chain을 시작하거나 nonanaphoric이기 때문에 선행사를 갖지 않는다는 것을 의미한다. \\\\begin{matrix} \\\\sum_{\\\\hat{y}\\\\in Y(i)\\\\cap GOLD(i)}P(\\\\hat{y}) \\\\end{matrix} 학습 시에 legal antecedent 의 coreference 확률의합을 최대화하는 손실함수를 사용한다. 가능한 선행사 $Y(i)$를 가진 특정 mention $i$에 대해, $GOLD(i)$는 $i$를 포함하는 gold cluster의 mention 집합이라고 가정한다. $i$이전에 발생한 mention 집합은 $Y(i)$이기 때문에 $i$이전에 발생하는 gold cluster의 mention의 집합은 $Y(i)$와 $GOLD(i)$의 교집합이다. 따라서 이 확률을 최대화하는 방향으로 학습되어야 한다. mention $i$가 gold cluster에 없을 경우 $GOLD(i) = \\\\epsilon$이다. \\\\begin{matrix} L=\\\\sum_{i-2}^{N} - log \\\\sum_{\\\\hat{y} \\\\in Y(i) \\\\cap GOLD(i)}P(\\\\hat{y}) \\\\end{matrix}',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '방금 언급한 그 확률을 손실함수로 바꾸기 위해 위 수식처럼 확률에 $-log$를 취하고 그 값을 모두 더해서 corss-entropy loss function을 사용한다. 이 손실함수 값을 최소화 하는 방향으로 학습이 진행되어야 한다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '손실함수: 모델의 출력값과 사용자가 원하는 출력값의 차이, 즉 오차를 말한다. 이 손실함수 값이 최소화 되도록 하는 가중치와 편향을 찾는 것이 학습이다. 4. Evaluation of Coreference Resolution 우리는 시스템이 생산한 일련의 체인이나 클러스터를 human labeling이나 gold나 reference chain 혹은 클러스터 집합과 비교하고, precisiton과 recall을 보고하며 이론적으로 evaluate coreference algorithm을 평가한다. 이 비교를 수행하기 위한 다양한 방법이 있다. coreference algorithm을 평가하는데 사용되는 5가지 일반적인 metric(측정방법)은 다음과 같다: 1) MUC metric(link based) 2) BLANC metric(link based) 3) $B^3$ metric (mention based) 4) CEAF metric (entity based) 5) LEA metric (link based entity aware) 5. Entity Linking coreference와 밀접한 관련이 있는 entity linking task는 텍스트의 mention을 world에 있는 entity의 목록인 ontology에서 어떤 real-world entity의 representation과 연관시키는 것이다. 이 task에 사용되는 가장 일반적인 온톨로지는 위키피디아이다. 위키피디아의 각 페이지는 특정 entity에 대한 고유한 id역할을 한다. 따라서 wikification의 entity linking task는 어떤 한 individual에 해당하는 Wikipedia page가 mention에 의해 지시되는지를 결정하는 작업이다.',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'entity linking은 두 단계로 이루어진다: mention detection과 mention disambiguation. Coreference가 올바른 위키피디아 페이지에 연결하기 위해 더 많은 간으한 surface form들을 제공하여 entity linking에 도움을 주기도 하지만 entity linking이 coreference resolution을 개선하기 위해 다른 방향으로도 사용될 수 있다. entity linking을 coreference에 통합하면 백과사전적 지식(Donald Tsang이 대통령이라는 사실처럼)을 이끌어내서 President에 대한 언급을 명확하게 하는 데에 도움이 될 수 있다. 6. Winograd Schema problems',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'Winograd는 위 예시를 제시하며 coreference의 일부 사례가 상당히 어려운 것으로 나타나 세계 지식이나 복잡한 추론을 요구하는 것으로 보인다고 지적했다. Winogard는 대부분의 독자들이 대명사 뒤에 이어지는 것에 대해서 선호하는 선행사는 (a)에서는 the city council이지만 (b)에서는 the demonstrators라는 것을 알아챘다. 그는 이것이 두 번째 절은 첫 번째 절의 설명으로 의도된 것임을 이해해야 하고, 또한 우리의 문화 프레임은 시의회가 아마도 시위대보다 폭력을 두려워할 가능성이 있고, 시위대는 폭력을 옹호할 가능성이 더 높다는 것을 시사한다. 이와 관련된 challenge task도 있고 Winograd와 coreference resolution problem이 있는 dataset들도 있다고 한다. 7. Gender Bias in Coreference Language processing의 다른 측면과 마찬가지로 coreference model도 성별과 기타 다른 편향들을 나타낸다. embedding은 그들의 training test에서 사회적 편향을 복제한다. 남성들은 의사와 같이 역사적으로 전형적인 남성 직업과 연관시키고, 여성은 비서와 같이 전형적인 여성 직업과 연관시킨다. 전형적인 남성직업 여성직업…?',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': 'WinoBias dataset은 Winograd Schema 패러다임의 변형을 사용하며 coreference algorithn이 문화적 고정 관념과 일치하는 선행사와 성별 대명사를 연결하는 방향으로 편향된 정도를 테스트하는데, 이 데이터셋은 전형적인 남성 그리고 전형적인 여성 직업에 해당하는 두 mention과 그 중 하나에 연결되어야 하는 성별 대명사를 포함한다. 위 예시에서 22.66은 pro-stereotypical이고 22.67은 anti-stereotypical하다. 이런 편향의 원인 중 하나는 데이터셋에 female entity가 매우 적다는 것인데, 이것을 해소하기 위해 남성과 여성 entity를 바꿔서 데이터셋을 바꾸고 기존 데이터셋과 합쳐서 비율을 맞추는 등의 시도가 있었다고 한다. Reference Daniel Jurafsky and James H. Martin. 2019. Speech and Language Processing, 3rd Edition. https://web.stanford.edu/~jurafsky/slp3/ × Search',\n",
       "  'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/'},\n",
       " {'text': '1. Introduction 많은 논문들에서 BERT가 어떻게 언어를 이해하는지에 대해 다루는데 본 논문에서는 BERT가 통사적 지식을 어떻게 가지는지를 평가해보고자 한다. BERT의 통사적 지식은 한가지 방법으로만 평가될 수는 없다. 따라서 크게 두 가지 방법을 통해 실험을 진행한다. 1) negative polarity item(NPI) : NPI는 부정적인 언어환경에서 나오는 단어이다. 2) five approaches: Boolean acceptability : 문장의 참과 거짓을 판단. Absolute Minimal Pair : 두 개의 문장이 있을 때 그 문장들이 단어나 미묘하게 다른 표현들을 포착할 수 있는지 실험 Gradient Minimal Pair : Absolute Minimal Pair에 대해 확률적으로 rough하게 봐서 일정 수준을 넘어가면 참. Cloze Test : 특정 단어를 가리고 테스트. Feature Probing : NPI, licensor, scope을 기준으로 문장을 학습하고 inference하는 과정',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': '2. Related Work language representation모델이 문법적 지식을 평가하지 못하고 있다. 최근 연구들에서도 probing task, Minimal pair, Boolean acceptability judgment 등으로 평가를 시도하고 있지만 모델들의 직접적인 비교는 하지 목하고 있다. 따라서 본 논문에서는 NPI를 통해 BERT와 같은 language representation모델을 평가해보고자 한다. 2.1 Evaluating Sentence Encoders Boolean classification task, Minimal pair, probing task는 BERT의 encoder 부분을 평가하는 방법들이다. Boolean classification task를 통해 모델의 grammatical knowledge encoding 능력을 평가할 수 있다. 이 task의 목적은 입력된 단일 문장의 acceptability 예측하는 것이다. Minimal pair를 통해 언어학적 acceptability와 하나의 요소가 다른 두 문장쌍을 비교하여 모델이 single grammatical feature를 포착하는지 판단한다. Probing task는 모델의 embedding이 tense, voice, sentence length, morphological number와 같은 syntactic, surface feature들을 encoding하는지를 확인하기 위해 사용된다. 2.2 Negative Polarity Items NPI는 any처럼 부정적인 문장에서 받아들일 수 있는 단어이다. NPI는 licensor라는 환경이 구축되어야 사용될 수 있다. licensor는 일정의 언어적 환경으로, NPI를 쓰는 부정적인 환경이다. 그리고 Scope는 licensor가 미치는 영향의 범위이다.',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': '예문 (1)에서 hasn’t라는 licensor가 존재하기 때문에 부정적인 단어 any가 NPI로 나올 수 있다. 그리고 이러한 licensor의 영향 하에 NPI가 나왔기 때문에 scope가 맞게 되는 것이다. 예문 (2)에는 licensor가 없기 때문에 any라는 NPI가 나왔지만 이는 허용되지 않는 쓰임으로, 비문이 된다. 예문 (3)에서는 licensor가 있지만 any라는 NLP와 scope가 맞지 않아 비문이 된다. 즉, 부정적인 환경에서 NPI가 나와야 하는데 환경 영향 범위가 시작되기 이전에 나왔기 때문에 비문이 되는 것이다. 예문 (4)는 DE(down entailing)이라는 환경과 관련된 예시이다. (4)-a에서 I haven’t been to France와 I have been to Paris에서 파리는 프랑스에 속해 있기 때문에 후자가 DE환경이다. 이처럼 어떠한 범위 안에 포함된 관계일 경우, 이를 DE환경이라고 부른다. 하지만 (4)-b에서는 뒷 문장에 부정적이 표현이 들어가지 않아 DE문장이 되지 않는다.',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': 'DE가 부정적인 것과 어떤 관련이 있나? 최근 semantic에서 단조성(monotonicity: 한쪽 방향으로 쭉 진행되는 것)이 많이 연구되고 있다(downstream이랑 비슷한 개념이라고 생각하면 된다). 물 흘러가 듯이 cascade처럼 위에 나온 것이 참이라면 그 다음과 그 다음 다음 것도 참인 것이 되는, 단조적인 방법으로 진행되는 것을 말한다. 위 예시에서도 마찬가지로 프랑스를 간 적이 없는 것이 참이라면 파리를 간 적이 없는 것도 참인 것이다. 하지만 프랑스에 간 적이 있다해도 파리에 가지 않았을 수 있다. 따라서 a는 monotonic구조이고 b는 non-monotonic구조이다. NPI는 다음과 같은 문맥에서 허용된다: Direct negation (I haven’t seen anybody.) Indirect negation (I don’t think that anybody will like this.) Non-affirmative predicates (I doubt that he has done anything.) Negative prepositions (They hired someone without any experience.) Adversative predicates (I am surprised that anything so absurd could have been circulated.) Restrictor of universal quantifier (Everyone who has any interest in linguistics…) Restrictor of superlative (This is the most stupid question any person has ever asked.) Comparative sentences (He was more exhausted than anyone else.) Predications of ‘excess’ with too (He was too exhausted to understand anything.) The protasis of a conditional clause (If you need anything, let me know.)',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': 'NPI는 자연언어에서 개념적으로 정의할 수는 있지만 나타나는 수많은 구성양상을 모두 설명하는 것은 불가능하다. 따라서 NPI는 언어학에서 매우 tricky한 주제이기 때문에 본 논문의 주제로 선정되었을 것이다. 2.3 CoLA(Corpus of Linguistic Acceptability) CoLA는 데이터셋이다. 본 논문은 CoLA를 기반으로 생성한 데이터를 직접 구현하여 사용한다. CoLA는 1만개 이상의 example sentence를 지니는 데이터셋이며, supervised acceptability classifier를 수행하기 위해 사용된다. 한국어 NPI는 영어보다 복잡하다. 왜냐하면 한국어의 NPI가 영어보다 많기 때문이다. 영어에서는 NPI가 명확한데 한국어에서는 이게 NPI인가 싶은 것들도 NPI인 경우가 많다. 그리고 한국어 NPI는 문맥상의 의미에 따라 여러 제약이 존재한다. 한국어 NPI를 연구하여 딥러닝 모델에 적용시킬 수 있다면 좋을텐데…',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': '3. Methods 본 연구는 다섯 가지 방식(Boolean acceptability, Minimal pair(Absolut, Gradient), Cloze Test, Feature probing을 통해 BERT가 NPI라는 환경에서 문법적인 지식을 이해할 수 있는지 평가한다. 앞서 본 연구에서 사용된 데이터는 CoLA를 기반으로 생성하였다는 점을 언급한 바 있다. 데이터는 한 문장과 0과 1로 구성된 Boolean label로 구성되어 있으며, Boolean label에는 크게 세 가지의 meta-data(licensor, NPI, scope) 변수들이 존재한다. meta-data의 유무에 따라 0 혹은 1의 값이 주어진다. > 위 표에 나타난 Licensor, NPI, Scope가 meta-data 변수이다. Licensor Table 2가 Questions environment에 대한 것이니까 Table 1에서 Question부분을 보면 whether가 licensor로 주어진 것을 확인할 수 있다. 다시 Table 2를 보면 licensor whether가 온 경우에는 1이 표시되어 있고 licensor가 존재하지 않을 경우에는 that이라는 licensor replacement가 오고 0으로 표시된다. NPI Questions environment sample에서 NPI는 never이다. Table 2를 보면 NPI가 없을 경우에는 NPI replacement인 often이 오고 0으로 표시된 것을 확인할 수 있다.',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': 'Scope Scope는 Licensor가 미치는 영향이다. Table 2를 보면 Licensor whether 뒤에 대괄호가 있는데 이것이 Licensor가 미치는 영향의 범위, 즉 Scope이다. 따라서 이 범위 안에 NPI가 존재할 경우에는 Scope가 1이되고 그렇지 않을 경우에는 0이 된다.',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': '3.1 Boolean Acceptability Boolean Acceptability는 언어 표준에 따라 문장이 만족스러운지를 평가하는 방식이다. 해당 모델이 특정 문장에 대해 acceptable하다 혹은 unacceptable하다를 잘 판단했는지 평가하는 것이다. Boolean Acceptability 판단을 위해 fine-tuning을 진행하게 되는데, BERT같은 경우에는 마지막 layer의 [CLS] embedding 상단에 classifier를 추가시킨다. Glove Bow의 경우에는 MLP classifier에 max pooling layer를 추가하여 classifier를 생성했다. 모델의 성능은 예측된 label과 정답 값의 label을 비교하는 MCC로 측정되었다. 3.2 Minimal Pair Minimal Pair(최소대립쌍)은 두 개의 문장이 있을 때 두 문장에서 딱 하나의 token만 다르게 하여 해당 쌍을 비교할 수 있도록 하는 것이다. 여기에서는 Absolute와 Gradient Minimal pair에 대해 실험이 진행된다. 3.2.1 Absolute Minimal Pair 본 실험에서는 두 문장이 paradigm(세 가지 meta-data 변수의 상황) 환경 내에서 NPI와 관련된 Boolean meta-data가 하나만 다를 경우에 최소 대립쌍을 형성하게 되는데, 이때 모델이 한 가지만 다른 것을 제대로 분류할 수 있는지를 확인한다.',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': '위 예시에서 (1)은 NPI licensor가 존재하고, (2)는 존재하지 않는다. 여기에서 not이라는 것이 NPI의 Licensor가 된다. 3.2.2 Gradient Minimal Pair Gradient Minimal Pair는 Absolute Minimal Pair보다 rough한 버전의 방식이다. Absolute Minimal Pair는 NPI의 위치를 통해 모델이 정확한 분류를 해내야 하지만 Gradient Minimal Pair에서는 해당 문장이 acceptable할 확률이 unacceptable할 확률보다 높으면 올바르다고 간주한다. 4. Cloze Test Cloze Test는 어떤 문장이 있을 때 중간에 한 단어를 비워 놓고 어떤 단어가 오는지 예측하는 것이다. 하지만 본 실험에서는 BERT의 Masked token에 해당하는 단어를 예측하는 방식이 아닌 해당 문장에서 masking된 부분의 위치가 어디인지 알아낼 수 있는지를 실험하였다. 5. Feature Probing Feature Probing은 meta-data를 더 세분화한 방식이다. fine-tuning 유무와 관계없이 문장의 encoder부분을 freezing시키고 그 위에 lightweight classifier를 학습시켰다. lightweight classifier란 meta-data label을 예측하기 위해 Reference Warstadt et al.”Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs,”Association for Computational Linguistics.2019 × Search',\n",
       "  'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/'},\n",
       " {'text': '1. Introduction 본 연구에서는 영어의 장거리 의존문장을 여러 학습 모델에서 어떻게 처리하는지에 대해 실험한다. 장거리 의존 구문이란 자연어가 가지고 있는 sequential 한 속성과 더불어 hierarchical한 속성을 가지고 있다는 대표적인 예시이다. 영어에서 나타나는 대표적인 장거리 의존 구문은 WH-question, Relative clause, Topicalization이다. 장거리 의존 구문에 대한 자세한 설명은 Long-Distance Dependency 여기에 있다. 장거리 의존 문장과 같이 복잡한 구조를 가진 문장을 잘 파악하지 못하면 아래와 같은 오류를 낼 수 있기 때문에 자연어처리 분야에서의 장거리 의존 구문 처리는 매우 중요하다. It was the lawyer(s) who I think you said (GAP) was/were upset. -> It was the lawyer who I think you said (GAP) was/*were upset. -> It was the lawyer(s) who I think you said (GAP) *was/were upset. 따라서 본 연구에서는 딥러닝 모델들이 다양한 형태의 장거리 의존 구문을 잘 처리하는지 알아보고자 한다. 2. LSTM RNNs',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '여기에서 사용되는 모델은 Gulordava model과 Google model이다. Gulordava model은 English Wikipedia corpus 9000만 token으로 학습되었고 650 unit hidden layer 두 개를 가진다. Google model은 10억개의 Word Benchmark(Chelba et al., 2013)로 학습되었고 8196 unit을 가진 두 개의 hidden layer로 이루어져 있다. 그리고 character level CNN의 output을 LSTM의 input으로 사용한다. 이 두 모델은 아래와 같은 두 가지 조건 하에 사용된다. 기본적인 실험 방식은 Wilcox et al.(2019b)를 기반으로 한다. 2.1 Experiment 1: agreement in clefts 첫 번째 조건은 cleft sentence(분열문)이다. 분열문은 한 문장이 두 부분으로 구성되어 있는 문장으로, it-that 강조구문을 생각하면 쉽다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 예문 a는 wh-관계절로 이루어진 문장으로, it was the lawyer부분과 삽입절 I think로 나뉠 수 있다. 위 예문 a, b, c, d는 순서대로 삽입절의 개수가 점점 추가되는 것을 확인할 수 있다. 이에 따라 내포 수준(LEVEL)도 증가하는 것도 볼 수 있다. 본 실험에서는 2x2x4 factorial design(삽입절을 통해 강조되는 the lawyer(s)는 단수일 경우와 복수일 경우가 존재하며 이에 따라 동사의 경우도 2가지(단수/복수), 위 예문처럼 삽입절이 추가되는 4가지 문장)을 이용한 20 cleft item을 만들어 총 320문장을 가지고 실험을 진행한다. 2.2 Experiment 2: agreement in indirect interrogatives 두 번째 조건은 indirect interrogative(간접의문문)이다. 이는 embedded question이라고도 하는데, 종속절로 기능하는 의문문이다. 여기에서도 마찬가지로 2x2x4 factorial design, 20 cleft items, 320 sentences로 실험이 진행된다. 2.3 Result 두 조건에 대한 결과는 Surprisal로 제시된다. 본 실험에서는 수일치를 확인하고자 하기 때문에 RNN이 수일치에 성공했다면 Surprisal 수치가 낮을 것이고 틀렸다면 높을 것이다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': 'Surprisal(의외성/놀람) Wilcox et al. (2018)에서는 log inverse probability라고 하며 아래와 같은 공식을 제시하였다. 위 수식에서 x_i는 예측하고자 하는 단어이고 h_{i-1}은 x_i 이전의 hidden state 값이다. 따라서 맢선 맥락을 바탕으로 해당 단어에 대해 기대하는 정도를 surprisal로 나타낸다. 값이 낮을수록 의외가 아닌 것이고 높을수록 의외인 것이다. (공식) CoNLL에서 주로 사용되는 개념으로, Surprisal(의외성)은 사건이 일어나지 말아야하는 환경에서 나타날 경우에 나타나는 놀라움의 정도를 나타낸다. 예를 들어 초등학교 동창을 같은 동네에서 마주친 것보다 휴가지로 떠난 하와이에서 마주친 경우가 의외성의 수준이 더 높은 것이다. 2.3.1 Experiment 1 result: agreement in clefts',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 표에서 npl(빨간 박스)은 복수명사, nsg(초록 박스)는 단수명사를 나타낸다. 그리고 표의 x축에는 vpl(복수 동사)와 vsg(단수동사)가 나와있고 위에 제시된 숫자는 삽입절 내포 수준을 나타낸다. RNN이 제대로 학습했다면 명사가 복수일 때 동사가 복수여야 하기 때문에 초록색 박스보다 빨간색 박스의 수치가 낮게 나타나야 한다. 하지만 표를 보면 두 박스가 나타내는 수치의 차이가 거의 없다. google model에서는 삽입절이 많을수록 surprisal값이 높아지는 것을 볼 수 있다(예측을 잘 하지 못한다). 2.3.2 Experiment 2 result: agreement in indirect interrogatives',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '두 번째 실험의 결과도 첫 번째 실험처럼 전반적으로 겹침이 많은 것을 볼 수 있다. 따라서 Gulordava와 Google model은 장거리 의존 구문의 형태 통사적인 특징을 잘 학습하지 못한다는 것을 알 수 있다. 3. Transformer-XL Transformer-XL은 기존 Transformer의 self-attention network에 ‘recurrence’개념을 도입한 모델이다. 기존 Transformer는 현재의 정보를 처리할 때 이전 정보가 단절되는데 Transformer-XL은 recurrence를 도입하여 이전 정보가 현재 정보에도 전파되도록 한다. Dai et al.(2019)에 따르면 RNN보다 80%더 긴 dependency를 학습할 수 있다고 한다. 이를 증명하기 위해 본 논문에서는 RNN으로 진행한 실험을 동일하게 진행하였다. 3.1 Experiment 1, 2 result: agreement in clefts, agreement in indirect interrogatives 위 그래프는 실험 1의 결과를 시각화한 것이다. 내포수준 1과 3에서만 주-동 수 일치 surprisal이 불일치 surprisal보다 낮게 나왔다. 즉 내포수준 1과 3에서만 제대로 구분을 하고 나머지는 못하는 것이다. 따라서 Transformer-XL의 성능이 RNN보다 아주 조금 더 좋다는 것을 알 수 있다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '실험 2에서도 결과가 좋지 않았다. 3.2 Experiment 3: Filler-gap surprisal in subject-inverted interrogatives 실험3은 주어가 도치된 의문문의 dependency에 대한 실험이다. 아래 예시 문장의 경우, talk about의 목적어가 what으로 대체되고 what이 절의 맨앞으로 이동한 것이다. 이때 what이 원래 있던 자리에 GAP이 생기는데 이 GAP과 what 사이에 dependency가 발생한다. GAP자리에 목적어 it을 삽입하면 talk about의 목적어가 두개(what과 it)가 되어버리기 때문에 비문이 된다. 따라서 실험3은 what과 GAP사이의 dependency를 모델이 잘 처리할 수 있는지 알아보는 것이다. 20개의 item 대해 WH-question이 있는 문장과 없는 문장 그리고 GAP이 있는 것과 없는 것 그리고 내포수준 1-4까지 해서 20x2x2x4, 총 320문장으로 실험을 진행한다. 그리고 GAP뒤에 나오는 item, 위 예시에서는 at뒤에 나오는 item의 softmax값(확률값)을 추출한다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '본 논문의 저자인 Rui P. Chaves는 island effect를 전공한 사람이다. 그래서 experiment syntax 연구 모델을 활용하여 본 실험을 설계한 것이다. a, b, c, d에서 b와 c는 정문이고 a와 d는 비문이다. 이와 같은 2x2 design은 a-b, a-c, c-d 각각이 syntactic constraint를 기준으로 하나의 minimal pair가 된다. 이런 식으로 2x2 conditional design으로 정석적인 experiment syntactic method를 사용하였다. 20개의 item은 각 항목마다 lexicalization을 해서 20개씩 분포했다는 말이다. 이러한 metric은 human evaluation에서도 매우 좋은 설계이다. 3.2.1 Experiment 3 result: Filler-gap surprisal in subject-inverted interrogatives',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 boxplot은 내포수준 1에 대한 결과를 시각화 한 것이다. 그래프 상에서 두 번째와 세 번째 박스가 정문이고 첫 번째와 네 번째 박스가 비문에 해당한다(wh가 없을 때는 gap도 없어야 정문이고 wh가 있을 때는 gap도 있어야 정문이 된다.) 따라서 모델이 예측을 잘 했다면 두 번째와 세 번째 surprisal이 첫 번째와 네 번째보다 낮아야 한다. 하지만 실험 결과, 정문에 해당하는 surprisal이 비문에 해당하는 surprisal보다 높게 나타났다. 나머지 내포수준에서도 전반적으로 결과가 좋지 않아 Transformer-XL이 주어 도치 의문문 처리 성능이 좋지 않음이 확인되었다. 최근 정략적 기반의 empirical linguistic study에서 boxplot을 이용한 시각화가 많이 보이고 있다. 3.3 Experiment 4: Filler-gap surprisal in uninverted indirect interrogatives',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '실험 4에서는 모델이 도치가 안 된 간접 의문문에서의 dependency를 파악할 수 있는지를 확인한다. 일반적으로 의문문은 주어와 동사가 도치되지만 의문문이 모절 안에 내포될 경우에는 도치 없이 실현된다. What과 GAP사이에 dependency가 발생한다는 점은 실험 3과 동일하지만 We talked about과 같이 주어와 동사가 도치되지 않는 점이 차이이다. 3.3.1 Experiment 4 result: Filler-gap surprisal in uninverted indirect interrogatives 실험 4의 결과는 그냥 처참하다. 모든 embedding level에서 올바른 surprisal pattern이 관찰되지 않았다. Transformer-XL은 RNN보다 dependency처리 능력이 나쁘다고 볼 수 있다. 4. BERT BERT에 대한 자세한 설명은 BERT여기에 있다. 4.1 Experiment 1 result: agreement in clefts BERT에는 masked token이 사용되는데 본 실험에서는 be동사에 masking을 했다. 위 예문처럼 was나 were부분을 가려놓고 그 곳의 surprisal값을 구하는 방식으로 실험을 진행하였다. 정문이 비문보다 surprisal이 낮아야 한다. 위 boxplot을 보면 한 박스 내에서 1번과 4번이 낮고 2번과 3번이 높게 나타나 BERT 실험 결과가 통계적으로 유효한 차이를 보이고 있음을 알 수 있다. 4.2 Experiment 2 result: agreement in indirect interrogatives 실험 2는 간접 의문문과 관련된 실험이다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '여기에서도 실험 1과 마찬가지로 한 박스 내에서 1번과 4번이 나머지보다 surprisal이 낮아야 한다. 내포수준 1, 2, 3에서는 잘 처리가 되고 있고 level4에서는 조금 결과가 안 좋다. 조금 아쉬운 결과지만 다른 모델들보다 BERT의 결과가 가장 잘 나왔다. 4.3 Additional Experiment 1 result 위 실험 결과와 같이 BERT가 언어를 잘 처리한다는 결과가 나왔는데 정말로 잘 처리하는 것인가를 알아보기 위해 추가적인 실험을 진행하였다. long-distance dependency처럼 생겼지만 long-distance dependency를 처리하는 것처럼 처리하면 안 되는 경우에 BERT가 어떤 반응을 보이는지 살펴보았다. 예를 들어 앞에 나온 NP와 뒤에 나오는 동사의 수가 다른 조건을 주어 헷갈리게 하는 것이다. 위 예문을 보면 동사는 the boy/the boys가 아닌 she/he와 we/they에 걸려야 정문이 되는 문장인데 동사가 the boy/the boys에 걸려서 비문이 되었다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 그래프는 이 실험에 대한 결과이다. 본 실험은 앞에 있는 NP와 동사의 단복수가 다른 조건에서 surprisal이 낮아야 하는 상황이기 때문에 다른 실험들의 결과와는 반대로 나타나야 한다. 위 그래프를 보면 BERT가 문장을 잘 파악해서 주-동 수일치를 잘 식별한 것을 알 수 있다. 4.4 Experiment 3, 4 result: Filler-gap surprisal in subject-inverted interrogatives, Filler-gap surprisal in uninverted indirect interrogatives 이 실험은 앞선 실험과는 달리 long distance dependency를 도치와 비도치 의문문 조건에서 살펴본다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '10번과 11번 예문을 보면 뒤에 있는 at the party에서 at을 masking 해놓고 이것의 softmax값을 도출하면서 surprisal값을 낸다. 정문인 경우 at이 있어야 하기 때문에 surprisal이 낮게 나타날 것이고 비문의 경우에는 at이 있으면 이상하니까 surprisal이 높을 것이다. 정문인 b와 c는 boxplot에서 2번과 3번에 대응되기 때문에 2번과 3번은 낮아야 하고 1번과 4번이 높게 나와야 하는데 잘 처리되고 있음을 볼 수 있다. 4.5 Additional Experiment 2 result BERT가 우수한 성과를 내는 이유 중 하나는 양방향 학습이라고 볼 수 있다. 양방향으로 학습하며 후행 성분을 고려하는 특성으로, 앞선 다른 모델들에 대한 실험과 골자는 동일하지만 critical item에 yesterday repeatedly, again, then과 같은 부사를 넣어 실험을 진행한다. (12)b,c는 정문이고 a, d는 비문이다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 boxplot에서 두 번째와 세 번째가 b와 c에 대응하고 첫 번째와 네 번째는 a와 d에 대응한다. 위 표를 보면 정문이 비문보다 낮게 나타나야 하는데 지금까지 진행한 BERT실험들과는 좋지 않은 결과가 나왔다. 이는 BERT의 처리 능력에도 한계가 있다는 것을 시사한다. 5. XLNet XLNet은 ELMo와 GPT의 AR(AuotRegressive) model과 BERT의 AE(Auto Encoding)model의 장점을 합쳐 놓은 것이다. AR, AE AE모델은 문장을 bidirectional하게 한꺼번에 받고 masked token을 복원하는 과정으로 학습이 진행된다. 하지만 masked token은 문장에서 독립적으로 존재하기 때문에 문장 내 의존관계(dependency)를 따질 수 없다. 반면 AR모델은 문장을 sequential하게 순차적으로 읽어 나간다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '하지만 AR의 경우 문장을 단방향으로만 보기 때문에 앞뒤 문맥을 파악하지 못한다는 단점이 있다. ELMo는 마지막 layer에서 forward LSTM과 backward LSTM의 결과값을 모두 사용하기는 하지만 각각 따로 학습하기 때문에 완전한 양방향 모델이라고 할 수는 없다. 하지만 XLNet을 개발한 Yang et al.(2019)는 두 방식에 대한 문제점을 해결하기 위해 두 모델이 가지는 장점만 합쳐보았다. 이를 Permutation language model라고 부른다. 이 모델은 token을 랜덤으로 셔플한 후 셔플된 순서를 학습한다. 아래 예시처럼 ‘거칠어진다, 칠수록, 말은, 고와지고, 가루는’을 받아서 ‘할수록’을 예측하는 것이다. 이러한 방식으로 학습을 하면 token들을 순차적으로 학습하는 AR model과 학습 방향을 같지만 결과적으로 양방향으로 문맥을 파악할 수 있다. 5.1 Experiment 1, 2 result: agreement in clefts, agreement in indirect interrogatives LSTM RNNs로 진행한 동일한 실험(cleft sentence, embedded question)을 XLNet으로 진행했을 때 아래와 같은 결과가 나왔다.',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 표를 보면 XLNet도 대부분 겹침이 많은 것을 볼 수 있다. BERT의 단점을 보완한 모델이라고는 하나, BERT보다도 성능이 좋지 않았다. 6. GPT-2 GPT-2는 Radford et al.(2019)논문에서 제안된 모델로, Transformer를 기반으로 하는 unsupervised model인 GPT의 후속 모델이다. 해당 모델은 다양한 task를 수행할 수 있도록 domain specific하지 않은 다양한 raw data를 학습한다. 본 논문에서는 GPT-2 medium model(3억 4천만개 parameter version)을 사용한다. GPT-2에 대해 진행한 실험 1-4의 결과는 아래와 같다. 6.1 Experiment 1 result: agreement in clefts 실험1의 결과, 모든 embedding level에서 정문(주어-동사 일치)의 surprisal이 유의미하게 나타났다. 그리고 위 표를 보면 embedding level이 깊어짐에 따라 각 condition간의 차이가 줄어드는 것을 확인할 수 있는데, 본 논문에서는 이러한 현상에 대해 embedding level이 깊어질수록 dependency가 줄어드는 결과라고 해석했다. 6.2 Experiment 2 result: agreement in indirect interrogatives',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '실험 2도 1과 같이 모든 embedding level에서 주어 동사 일치와 불일치 간의 surprisal이 뚜렷하게 구분된다. 잘 파악하고 있다. 6.3 Experiment 3 result: Filler-gap surprisal in subject-inverted interrogatives 앞서 말했듯이 첫 번째와 네 번째 박스가 비문이고 두 번째와 세 번째가 정문이다. 비문에 해당하는 surprisal이 정문보다 높게 나와 모델이 학습을 잘 하는 것처럼 보이지만 본 논문에 따르면 wh-GAP과 nwh-nGAP이 둘 다 정문이기 때문에 surprisal이 비슷하게 나와야 하지만 wh-gap의 surprisal이 더 높게 나타나고 있다. 본 논문에서는 이와 같은 점이 조금 아쉽지만 전반적으로 주어-동사 도치 의문문의 dependency를 어느정도 잘 처리하고 있다고 볼 수 있다고 해석하였다. !!!!!!!! wh-GAP과 nwh-nGAP의 surprisal이 비슷해야 한다고…? 글쎄… 이는 유표성과 관련된다. wh-GAP과 nwh-nGAP 중 nwh-nGAP이 더 unmarked form이다. 자연어에서는 wh-question보다 non wh-question이 더 많이 쓰이기 때문에 사용 빈도 측면에서도, syntactic complexity 측면에서도 nwh-nGAP의 surprisal이 낮은 것이 당연하다. 오히려 모델이 이를 잘 포착한 것으로 보인다. 별로 아쉬운 결과가 아니라고 생각한다. 6.4 Experiment 4 result: Filler-gap surprisal in uninverted indirect interrogatives',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '위 표에서 세 번째와 네 번째 박스, 즉 wh-GAP(정문)과 wh-nGAP(비문)의 surprisal차이가 거의 나타나지 않는다는 점은 아쉽지만 전반적으로 나쁘지 않은 결과를 보이고 있다. !!!!!!!! 이 논문에서는 nwh-nGAP과 wh-GAP의 차이가 없다고 하는데 잘못된 설명 같다. Figure 17은 실험 4에 대한 그림인데 아래 설명은 inverted interrogative라고 되어있고 Figure 16에는 uninverted라고 되어 있다. 둘이 설명이 바뀐게 아닌가요…? 7. Discussion 실험 결과를 바탕으로 모델들의 성능은 XLNet, Transformer-XL < LSTM < GPT-2, BERT이다. GPT-2와 BERT가 다른 모델들보다 좋은 성과를 보이긴 했지만 주어진 dependency condition에 따라 성능이 크게 달라지는 양상을 보임으로써 많은 양의 데이터로 학습한 결과물이어도 아직은 불안정하다는 것을 확인할 수 있었다. Reference Da Costa.J et al.”Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies,”Association for Computational Linguistics.2020 Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860. https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/ https://openai.com/blog/language-unsupervised/ https://openai.com/blog/better-language-models/ https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': 'Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances inneural information processing systems (pp. 5998-6008). Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog,1(8), 9. Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu, Y. (2016). Exploring the limits of language modeling. arXiv preprintarXiv:1602.02410. Maria M. Piñango, Emily Finn, Cheryl Lacadie, and R. Todd Constable (2016). The Localization of Long-Distance Dependency Components:Integrating the Focal-lesion and Neuroimaging Record. Front Psychol. 2016; 7: 143 × Search',\n",
       "  'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/'},\n",
       " {'text': '1. Introduction pre-trained된 BERT모델을 downstream NLP task에 사용된다. 일반적으로 BERT는 unsupervised learning으로 학습되지만 다른 task에 이용하기 위해 supervised learning으로도 학습하여 사용될 수도 있다. supervised learning으로 학습된 BERT가 GLUE benchmark의 11개 task에서 기존의 state-of-the-art를 뛰어넘었다고 한다. 그런데 BERT가 왜 잘 되는지 아직 잘 모른다고 한다. research question: BERT의 각 layer에서 학습되는 representation이 무엇인지 밝힐 수 있을까?(각 layer에서 자연어의 어떤 특성이 학습되는지 밝힐 수 있을까?) 위와 같은 질문에 대한 답을 찾기 위해 본 논문에서는 아래와 같은 네 가지 실험을 한다. 이 실험들은 Goldberg(2019)연구를 기반으로 하여 각 layer에서 학습한 representation에 대한 평가를 진행한다. 1) 각 layer가 phrase정보를 얼마나 잘 포착했는지 실험 2) BERT가 자연어의 hierarchical한 속성을 잘 포착했는지 실험. Conneau(2018)연구를 기반으로 각 layer에서 surface, syntactics, semantics feature를 얼마나 잘 캡쳐하는지 실험. 3) 주어-술어 수일치와 같은 task가 있을 때 long-distance dependency에 대해 BERT가 task를 얼마나 잘 수행하는지 실험. 4) compositional structure를 test하여 BERT가 tree구조를 얼마나 잘 포착하는지 실험. 2. BERT',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': 'BERT는 transformer를 기반으로 한 pre-trained 모델로, 각 layer에서 bidirectional하게 학습한다. BERT는 두 가지 방법으로 training이 된다: 1) Masked Language Model: input 문장 일부에 mask token을 넣어 해당 token이 원래 어떤 단어였는지 알아내는 학습 방법. 2) Next sentence prediction: input sequence에 대해 절반은 pair가 되는 문장으로, 나머지는 corpus에서 랜덤으로 추출한 문장을 놓고 두 문장이 연결되는 문장인지 아닌지 판별하는 학습 방법.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '문장 처음에는 [CLS]token을, 마지막에는 [SEP]token을 놓아 모델이 각 문장을 구분할 수 있게 한다. 입력 token의 15%은 [Mask]token으로 들어가 embedding된다. 2.1 Setup PyTorch 12 layer, 768 hidden layer, 12 attention head BERT representation을 compute하기 위해 [CLS]token을 붙인다. ([CLS]token은 transformer를 모두 거치고 나면 token sequence의 결합된 의미를 가지게 된다. 그래서 여기에 classifier를 붙이면 classification task를 수행할 수 있게 되는 것인데 classification task가 아니라면 [CLS]token은 그냥 무시해도 된다.) 3. Related Work 1) Peters(2018) 이 연구에서는 contextual word embedding을 사용한 모델이 Glove와 같은 기존의 word embedding방식을 사용한 모델보다 훨씬 뛰어나고 언어의 구조 정보까지도 각 layer에서 포착한다는 것을 밝힌 논문이다. 2) Tenney(2019) contextual word representation이 구조 정보를 잘 encode하지만 syntactic 정보에 대해서는 잘 파악하지 못한다는 것을 밝힌 논문이다. 3) Liu(2019) 이 논문에서는 아래와 같은 세 가지 결과를 밝혔다: contextual word embedding이 fine-grained된 언어정보는 잘 포착하지 못한다. RNN의 경우, attention model과는 달리 higher layer에서 task specific한 정보를 잘 파악한다. 특정한 task에 대해 미리 훈련한 모델이 일반적으로 훈련한 모델보다 성능이 좋다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '이 논문에서는 RNN(BiLSTM)과 Transformer의 몇 번째 layer가 어떠한 정보를 학습하는지 실험하기 위해 17가지 task를 수행하였다. RNN의 경우, 일반적인 정보는 중간 층위에서 잘 캡쳐되고 task specific한 것에 대해서는 높은 층위에서 더 잘 캡쳐되었다(예를 들어 4개 layer를 쌓았을 때 4번 layer에서 task specific한 정보가 잘 캡쳐되고 2-3번째 layer에서는 일반적인 정보가 잘 캡쳐된다). Transformer의 경우에는 그냥 주로 중간 층위에서만 많이 된다는 것을 확인하였다. 이를 통해 이 논문은 pre-training모델도 task specific하게 학습하는 것이 의미 있을 수 있다는 결론을 도출하였다. 4) Hewitt(2019) input에 contextual base가 있는 경우가 그렇지 않은 경우보다 Parse tree가 더 잘 반영된다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '이 논문에서는 text의 hierarchical한 정보가 embedding에 잘 반영되는지 확인하기 위해 parse tree를 복원해낼 수 있는 선형변환을 찾는 연구를 다룬다. ELMo와 BERT트를 대상으로 실험이 진행되는데, ELMo에서도 충분히 복원이 되긴 했지만 BERT가 더 정확했다. parse tree는 tree가 진짜로 존재 한다기보다 vector norm(vector distance)로 저장되어 있었다. 따라서 이를 바탕으로 vector공간 내에서 통하는 metric을 기준으로 parse tree를 제고해낼 수 있는 정보를 얻을 수 있다는 것을 확인하였다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '위 네 연구는 모두 contextual word embedding의 중요성을 시사한다. 본 논문에서는 ability of BERT in learning syntactic structure를 증명하고자 한다. 4. Phrasal Syntax BERT가 구조적인 정보들을 얼마나 잘 처리하는지를 알아보기 위해 본 논문에서 진행된 4가지 실험 중 첫 번째이다. 여기에서는 BERT가 phrase단위의 언어정보를 잘 포착하는지 살펴본다. 이 실험과 관련된 선행연구는 Peters(2018)로, 여기에서는 LSTM기반 모델이 Phrase-level information을 잘 캡쳐한다고 보고했다. 이를 이어 받아 본 연구에서는 traditional language modeling을 목적으로 훈련하지 않은 BERT모델도 phrase-level의 정보들을 잘 처리할 수 있는지를 확인한다. 또한 BERT가 처리하는 phrase level information이 multiple한 layer(12개의 layer)에서 어떻게 나타나는지 확인하고자 한다. 4.1 Experiment procedure 실험 절차는 BERT의 각 layer에서 span representation을 뽑아내고 각 layer에서 첫 번째 hidden vector와 마지막 hidden vector를 concatenate해서 span representation을 산출하는 식으로 이루어진다. 4.2 Dataset CoNLL200 chunk dataset에서 3000개의 labeled chunk와 500개의 unlabeled chunk의 span을 무작위로 골라 data로 사용하여 주석된 카테고리대로 잘 분류하는지 실험하였다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': 'CoNLL은 아래 예시처럼 문장 내에서 NP나 VP같은 phrase단위의 카테고리들이 chunk단위로 주석처리 되어 있는 데이터이다. 4.3 Result 분류 결과는 위와 같다. Figure1은 앞에서 2개 layer, 뒤에서 2개 layer의 span representation을 시각화한 것이다. 이는 chunk type을 10가지로 정해 놓고 K-means clustering으로 분류한 것으로, 하위 layer에서 phrase level information을 잘 포착하는 반면 higher layer의 경우 카테고리들이 뭉개져서 구분이 힘든 것을 볼 수 있다. 이 cluster결과를 NMI metric을 사용하여 evaluate를 해봤는데 위 표처럼 하위 layer에서의 성능이 상위 layer의 성능보다 좋았다고 한다. 5. Probing Tasks probing task는 classification task라고도 한다. 신경망 모델이 주어진 문장에서 특정 linguistic feature들을 잘 probe할 수 있는지를 test하는 것이다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '두 번째 실험이다. 이 실험에서는 BERT가 문장의 surface, syntactic, semantic feature들을 잘 포착하는지 확인한다. 이는 auxiliary classification task를 setting함으로서 달성된다. 이 task에서는 모델의 final output이 특정 언어 현상을 예측할 수 있도록 해주는 feature로 사용된다. auxiliary classifier가 특정 linguistic property를 잘 예측하면 original model도 해당 property를 잘 encoding한다는 것이다. 이 실험에서는 각 모델의 layer가 다른 종류의 linguistic feature를 잘 encoding하는지 확인한다. layer를 평가하기 위해 10개의 probing task를 수행하는데, task의 dataset에 따라 세 그룹으로 나뉠 수 있다. 5.1 Surface tasks Sentence Length, Word Content Surface task는 비교적 단순한 작업으로, 1) 문장의 길이나 2) 문장에 특정 단어가 존재하는지에 대한 feature를 찾는 task이다. 5.2 Syntactic tasks Bigram shift, Tree depth, Top constituent 이 task에서는 1)어순에 대한 sensitivity(문장 구성성분들의 순서가 어떤지, 그 배열이 제대로 된 것인지), 2) syntactic tree의 깊이가 어느정도인지(내포절이 많고 수식구가 많으면 tree의 깊이가 깊어진다), 3) tree안에서 top level에 있는 constituent들의 순서를 확인한다. 5.3 Semantic task',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': 'Tense, subject Number, Object Number, Coordination Inversion and Semantic Odd Man Out 이 task에서는 1) 시제 2) 주어 3) 목적어의 수 그리고 4) 명사와 동사의 무작위 배치에 대한 sensitivity(이건 parsing과정이 좀 복잡하다)를 확인하는 작업이 이루어진다. 5.4 Result 위 표를 보면 BERT가 linguistic feature들의 hierarchical한 특성을 잘 embedding하는 것을 확인할 수 있다. Surface information(feature)의 경우 low layer에서 잘 포착된다. Syntactic information(feature)은 middle layer에서 잘 포착된다. 비교적 복잡한 Semantic information(feature)은 higher layer에서 잘 포착된다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': 'task가 복잡해질수록 상위 layer에서 좋은 결과를 얻는데, 이를 통해 training이 더 진행될수록 기본적인 문장길이와 같은 feature보다는 조금 더 복잡한 feature들을 더 잘 처리하는 것이 확인된다. 괄호 안의 숫자는 훈련된 BERT와 훈련되지 않은 BERT간의 차이를 나타낸다. 괄호 안의 수가 마이너스 값으로 나타나는 경우는 훈련되지 않은 BERT의 성능이 더 뛰어나다는 것이다. 위 표에서 문장 길이를 파악하는 task의 중간 layer부터 마이너스 값이 확인되는데, 이는 훈련되지 않은 BERT도 기본적인 문장 길이정도는 충분히 예측할 수 있다는 것을 나타낸다. 이 실험을 통해 결론적으로 BERT가 linguistic feature들의 hierarchy를 잘 embedding하는 것을 확인할 수 있다. 6. Subject-verb agreement 세 번째 실험이다. 이 실험에서는 주어 동사 수일치(주어의 수와 동사의 수가 단수인지 복수인지분류)를 통해 둘 사이의 dependency를 잘 파악하는지 확인하고, 주어와 동사 사이에 attractor가 있는 경우 long-distance dependency를 얼마나 잘 파악하는지 확인한다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '여기에서 dependency란 문장 내에서 특정 요소들이 가지는 syntactic한 관계를 의미한다. a) The keys are on the table. b) The keys to the cabinet are on the table. c) The keys to the cabinet whose color is blue are on the table. 예문 b에서 복수 명사인 The keys와 복수 동사 are사이에 단수 명사 the cabinet이 끼어 있는데, 이런 것을 attractor라고 한다. 이렇게 attractor가 주어와 동사 사이에 존재하게 되는 경우, 기본적인 문장인 예문 a보다 더 복잡한 처리 과정을 요하게 된다. 이러한 개념을 바탕으로 주어와 동사 사이에 존재하는 attractor의 개수를 조절해가며 BERT의 각 layer에 대해 실험을 진행하였다. 6.1 Result 위 표의 가로는 attractor의 수, 즉 distance이고 세로는 layer number이다. 대부분 layer 5에서 9사이의 결과가 좋은 것을 확인할 수 있다. 이는 앞서 진행한 probing task 중 syntactic feature가 중간 layer에서 잘 캡쳐되는 것과 비슷한 양상이다. 그리고 attractor 증가에 대한 전반적인 성능 하락이 관찰된다. attractor가 증가해도 상위 layer에서는 비교적 좋은 성능이 나오는 것을 확인할 수 있다. 이는 복잡한 처리과정을 요하는 문장을 다루기 위해서는 deep한 layer가 필요하다는 것을 시사하는 것으로 볼 수 있다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '7. Compositional structure 네 번째 실험이다. 이 실험에서는 BERT의 representation에 compositional한 특성이 나타나는지를 확인하고, 주어-동사 수일치에 대한 dependency외에도 문장 내 단어들 간의 다른 dependency를 포착할 수 있는지를 확인한다. 언어의 compositional한 특성이란, 예를 들어 black + board = blackboard처럼 단어들 간의 결합을 통해 새로운 의미 혹은 확장된 의미의 단어가 만들어지는 것을 포함하여 문장 내에서 the + cabinet이 the cabinet이라는 NP가 되는 것을 의미한다. 7.1 Setup 언어의 compositional한 특성을 보기 위해 Tensor Product Decomposition Network(TPDN)을 사용했다. 이 실험에서는 문장의 compositional structure를 알기 위해 6가지 role scheme이 사용된다. left-to-right : 문장을 순차적으로 쭉 읽는 것(방향: ->) right-to-left : 문장을 거꾸로 읽는 것(방향: <-) bag-of-words : 문장 순서를 고려하지 않고 특정 단어의 등장 횟수를 세는 것 bidirectional : 문장을 정방향, 역방향으로 보는 것(양방향) syntactic tree : tree 구조 상 해당 단어의 위치를 확인하는 것 random binary tree : tree 구조 상 해당 단어의 위치를 확인하는 것',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '사용할 role scheme을 미리 선택하고 선택한 role scheme에 따라 input token을 구성한다. TPDN의 representation이 신경망 모델(BERT)에 의해 학습된 representation에 근사하게 훈련이 되면 해당 role scheme이 compositionality를 조금 더 잘 명시할 수 있다는 개념을 바탕으로 실험이 진행된다. 7.2 Result 위 표는 사전 학습된 BERT의 representation과 TPDN의 representation에 대한 Mean squared error(평균제곱오차값)을 구한 것이다. 대부분의 layer에서 TPDN이 BERT에 근사한 결과값을 내는 것을 볼 수 있다. left-to-right이나 right-to-left와 같이 단순한 정보처리 방식은 하위 layer에서 좋은 성능을 보이는 반면 tree structure와 같이 syntactic하고 복잡한 정보 처리 에 있어서는 상위 layer의 성능이 좋은 것을 확인할 수 있다. 7.3 Additional experiment 위와 같은 연구결과를 바탕으로 Raganato and Tiedemann(2018)이 진행한 self-attention weight로부터 제고된 dependency tree에 대한 연구를 추가적으로 진행한다. 이 실험은 모델이 문장 내 단어 간의 dependency관계를 파악할 수 있는지를 확인한다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '위 그림은 모델이 예시 문장의 dependency를 파악한 것을 나타낸다. 1) key, cabinet, table. 이 세개의 명사와 해당 명사들의 관사에 대한 dependency가 잘 파악되었다. 2) key와 are의 주어-동사 dependency가 잘 파악되었다. 3) 단어들 간의 dependency뿐만 아니라 문장 내의 논항 구조도 잘 파악했다. 논항 구조란 동사를 중심으로 술어가 어떤 요소들을 필수적으로 요하는지에 대한 정보를 담고 있는 구조이다. 위 예시에서 논항 key, cabinet, table이 dependency chain인 것을 파악하고 있다. 8. Conclusion 지금까지 진행한 실험 결과에 대해 정리를 해보도록 하겠다. 첫 번째 실험을 통해 BERT가 phrasal information(문장 내에서 NP와 VP찾기)을 잘 encoding하는 것을 확인하였다. 두 번째 실험을 통해 BERT가 linguistic feature(surface, syntactic, semantic feature)들을 각 layer에서 잘 포착함을 확인함으로써 BERT가 hierarchical한 정보를 잘 파악한다는 것을 확인하였다. 세 번째 실험을 통해 subject-verb agreement의 long dependency에 대한 실험을 진행하여 deep한 layer의 필요성을 확인하였다. 네 번째 실험을 통해 BERT가 언어의 compositional한 특징, 단어들 간의 dependency 그리고 논항 구조를 모두 잘 파악한다는 것을 확인했다.',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '결론적으로 BERT가 영어의 구조적 특성을 잘 파악한다는 것을 증명하였다. 한국어나 독일어에 적용한 사례가 있는지는 모르겠다… 찾아보자… Reference Jawahar.G et al.”What does BERT learn about the structure of language?,” Association for Computational Linguistics.2019 Sang et al. “Introduction to the CoNLL-2000 Shared Task Chunking,” CONLL/WS. 2000 × Search',\n",
       "  'source_title': 'What does BERT learn about the structure of language?(Jawahar.G(2019))\\n      01 Apr 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/01/Jawahar/'},\n",
       " {'text': '1. Contextual representations of language language model들의 vector representation방식 변화는 아래와 같다. word type들에 대해 pre-trained vector embedding된 word2vec과 Glove가 있다. 아래 수식에서$f_{vocab} : v\\\\rightarrow h$에서 $f_{vocab}$은 vocabulary가 embedding된다는 것을 나타낸다. \\\\begin{matrix} f_{vocab}: v\\\\rightarrow h \\\\end{matrix} FastText와 같은 Subword-informed 방식은 literal character, 즉 음절 자체가 sequence로 embedding되는 것이다. 그래서 앞에 $v_1$이 단어이고 뒤에 있는 $c_1$부터 $c_t$가 character단위로 쪼개진 것이다. \\\\begin{matrix} f_{subword} : (v,(c_1, …, c_t) \\\\rightarrow h \\\\end{matrix} ELMo와 BERT는 특정 문맥에서의 해당 단어의 쓰임이 반영한다. $w_1$부터 $w_n$은 텍스트에 존재하는 word이고, 이것들이 embedding되어서 $h_1$부터 $h_n$이 되는 것이다. \\\\begin{matrix} f_{contextual} : (w_1, …, w_N) \\\\rightarrow (h_1, … h_N) \\\\end{matrix} 2. Beyond “words in context” strong contextual representation을 위한 model들(CoVe, ELMo, ULMFiT, GPT, BERT, GPT2)의 behavior를 보완하기 위해 강력한 contextual model들이 언어 이해에 중요한 task들을 암시적으로 수행하는 것이 제안된다(syntax, coreference, QA 등).',\n",
       "  'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'text': '문맥에서의 단어는 파악했지만, 문장 자체가 지닌 중의성들과 구조적인 중의성 표현들은 어떻게 해결할 것인가에 대한 문제가 남아있다. The chef who ran to the stroe was out of food라는 문장은 중의성을 가진다. 이 문장에는 두 가지 가능성이 존재한다. 음식이 없기 때문에 셰프가 다른 store에 간 것인지 혹은 재료가 다 떨어져서 셰프가 식당으로 돌아온 것인지. 직감적으로는 후자가 맞을 것 같다. 그래서 chef가 was에 걸릴 수 있도록 하는 것이 직관에 맞는 구문분석이다.',\n",
       "  'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'text': '언어학 지식을 고려한 NLP논문들(Tal Linzen과 John Hewitt의 논문 등)은 대부분 모델의agreement와 관계절 반영을 중요 주제로 삼는다. agreement는 grammatical theory에 있어 중요한 역할을 한다. 따라서 agreement가 설명이 되어야 syntax가 어느정도 맞다고 이론적으로 이야기할 수 있다. 자연어는 sequential한 속성과 hierarchical한 속성이 공존한다. 영어의 장거리 의존 구문은 크게 관계절, wh-question, topicalization이 있다. 그 중에서 syntactic complexity(통사적 복잡도)가 가장 높은 것이 관계절이다. 따라서 자연어가 가지는 단순한 sequential한 속성이 아닌 hierarchical한 속성까지 반영하여 구문을 처리했다고 말할 수 있으려면 agreement와 장거리 의존 구문을 처리할 수 있어야 모델이 English syntax의 어떠한 특성을 반영하고 있다고 말할 수 있다. BERT와 ELMo와 같은 large neural network들이 syntax를 제대로 이해한다면 hierarchical한, in-depth한 syntactic information을 반영할 것이다. 3. observational vs. constructive evidence 3.1 observational evidence',\n",
       "  'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'text': 'observational network study는 hand-crafting(직접 input을 넣어서 원하는 대로 나오는지 보는 것)이다. Linzen 논문 같은 경우 수일치가 가능한가에 대한 가능성을 확인하는데, attractor같은 것들이 방해 요소가 된다. 예를 들어 The keys to the cabinet are on the table에서의 The keys와 are사이에 자리한 to the cabinet이 attractor이다. observational study는 model behavior과 data에 대한 질문을 제기할 수 있게 한다. 이는 가설의 혼동 요소 없이 직접적으로 model의 behavior를 보여주기 때문에 매우 유용하다. 3.2 constructive evidence',\n",
       "  'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'text': '모델을 학습시킬 때 하나의 특정 현상에 대한 feature들을 찾아서 같이 학습시키는 것이다. 그래서 중간에 나오는 trains on supervised data에서 여러 feature들(syntax나 POS 등) 중에서 하나를 고르는 것인데 이것을 prove라고 하거나 diagnostic classifier라고 한다. 이와 같은 연구의 예시로 Belinkov(2017)가 있는데, 이는 machine translation system에 대한 논문이다. 이 논문에서는 morphology를 기반으로 문장 내에서 feature를 골라 학습시킨다. (각 문장에서의 단어에 어떤 morphological feature들이 있는지를 고른다.) 이 연구에서는 모델의 hidden state에서 각각 encoding되는 정보를 특정한다. 즉, 1-layer nn에 대한 단어 w_i의 morphological 정보 y_i와 parameter값에 대해 모델에서 다음과 같이 encoding되는 것을 제안한다: \\\\begin{matrix} y_i \\\\sim 1-layer\\\\,nn (h_i) \\\\end{matrix} constructive study는 현상에 대한 구조적인 증거를 줄 수 있기 때문에 유용하다. Hewitt의 논문도 constructive study에 속하며, syntactic node를 가지고 feature를 찾아 그것을 학습시킨다. Reference',\n",
       "  'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'text': 'https://nlp.stanford.edu/~johnhew/structural-probe.html#the-structural-probe × Search',\n",
       "  'source_title': 'Contextual word representation\\n      30 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/30/Johnblog/'},\n",
       " {'text': '1. Introduction 본 논문은 통사적 정보를 찾기 위한 structure prob를 다루며 word representation 내에 syntactic tree가 반영되어 있는지 확인하는 방법에 대해 논의한다. 이를 위해 두 가지 문제제기를 할 수 있다: (1) ELMo와 BERT가 문맥에 따라 영어의 dependency tree를 encoding할 수 있나. (2) vector representation이 tree를 encoding한다는 것을 어떻게 알 수 있나. 2. Are vector spaces and trees reconcilable? 이 논문은 인간이 구사하는 문장이 chunk로 구성되어 있고, 그 chunk들이 결합되는 순서로 tree structure를 만들어볼 수 있다는 것을 기본 전제로 하고 있다. 신경망을 통해 단어들은 아래와 같이 vector로 표현될 수 있다: 3. Methods Goal: 본 논문에서는 vector space에 존재하는 vector(ELMo나 BERT로 학습된)로 tree structure를 찾아내고, ELMo와 BERT가 parse tree를 얼마나 인간과 유사하게 encode하는지 확인하고자 한다. 즉, neural network가 sentence의 parse tree를 잘 embedding할 수 있는지 확인하는 것을 목표로 한다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': 'Definition: 그래프를 embedding한다는 것은 vector space의 기하학(norm)이 그래프의 기하학에 유사하도록 각 node의 vector representation을 학습하는 것이다(Hamilton et al., 2017) 직관적으로, parse tree distance와 depth가 syntax와 무슨 상관이 이는 것일까? 단어와 단어사이의 distance metric은 node u,v와 거리 $d_T(u,v)=1$가 연결괸다는 것을 확인함으로써 tree를 포함할 수 있다. node는 norm의 크기에 따라 parents node냐 child node냐가 구분된다. distance metric은 hierarchical behavior를 설명한다. Linzen et al., 2016에서는 주어와 동사 사이에 나타나는 attractor가 존재한다고 언급되었는데, 동사는 tree 내에서 어떤 attractor보다 주어와 가깝다. 이러한 관계를 통해 hierarchical한 구조가 embedding된다는 것을 알 수 있다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': 'nn이 parse tree를 embedding하면 매우 많은 종류의 정보를 encoding해야 하기 때문에 아마 representation space 전체를 사용하지 않을 것이다. 본 연구의 probe는 word representation space의 선형 변형을 학습하는데 변형된 공간은 모든 문장에 대한 parse tree를 embedding한다. 이는 syntax를 encoding하데에 사용된 representation space의 일부분을 찾는 것과 같다. 즉, 이는 tree metric에 가장 잘 맞는 original space에서의 distance를 찾는 것이다. 3.1 The structural probe 본 논문에서 제안하는 structural probe는 pares tree가 어떻게 신경망의 hidden state에서 embedding될 수 있는지에 대해 간단한 가설을 통해 진행된다. parse tree는 아래와 같이 생겼다: 3.1.1 Trees as distances and norms 본 연구에서는 불연속 구조인 parse tree가 연속적인 vector의 sequence로 encoding되는지를 결정하는 것이 핵심이다. 여기에서 제시되는 가설은 우선 vector distance가 pares tree로 encoding되어 word representation의 linear transformation이 존재한다는 것이다. parse tree에는 두 단어에 대한 path metric d($w_i$, $w_j$)가 있다. 따라서 tree를 embedding하는 것은 tree에 의해 정의된 distance metric을 embedding하는 것과 같다. 따라서 nn을 통해 생성된 vector의 distance가 parse tree의 distance와 같다면 이는 nn이 문장 내 잠재된 tree를 embedding한 것으로 볼 수 있다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 그림을 보면 distance-1에 chef와 was가 걸리는 것을 볼 수 있다. 따라서 여기에서 chef와 was가 각각 $w_i$, $w_j$가 되고 두 단어 사이의 관계를 edge를 통해 embedding하는 것이다. chef가 was에 대한 직접적인 피지배소이기 때문에 chef와 was는 distance-1이지만 store과 was는 was에서 chef를 거쳐 ran과 to를 지나 이어지기 때문에 distance-4가된다. 즉, distance-4가 되기 위해서는 arc를 타고 들어가야 distance를 확인할 수 있다. The syntax distance hypothesis syntax distance 가설은 선형 변형(lenear transformation) $B$에 대한 것으로, 이는 L2 distance를 통해vector간의 거리가 parse tree를 표상한다는 것이다(${B^T}B$). 여기에서 L2 distance를 사용한 이유는 그냥 이게 제일 잘 돼서. 아래는 위 예시가 vector space에 표상된 그림으로, 3차원 word representaion을 2차원 공간에 encoding한 것이다.($B \\\\in \\\\mathbb{R}^{2\\\\times3}$) 2차원 공간에 encoding된 것을 선형 변형시키면 tree가 보인다:',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '3.1.2 Finding a parse tree-encoding distance metric 잠재적 tree-encoding distance는 선형변형($B \\\\in \\\\mathbb{R}^{k\\\\times n}$)을 통해 parameter화 된다. \\\\begin{matrix} {\\\\lVert h_i-h_j \\\\rVert}_B^2=(B(h_i-h_j))^T(B(h_i-h_j)) \\\\end{matrix} $Bh$는 word representation의 선형변형된 것으로, parse tree node representation과 같다. 이는 L2 distance를 positive semi-definite matrix(vector x값에 따라 0이상의 양수값으로 만드는 행렬) ($A={B^T}B$) 로 parameter화된 vector space에서 찾는 것과 같다. \\\\begin{matrix} \\\\lVert h_{i} - h_{j} \\\\rVert_{A}^{2} = (h_{i} - h_{j})^{T} A(h_i-h_j) \\\\end{matrix} 인간이 만든 parse tree의 distance와 nn이 예측한 parse tree distance의 차이를 최소화한다. \\\\begin{matrix} \\\\underset{B}{\\\\min}\\\\sum_{\\\\ell}\\\\frac{1}{\\\\left\\\\vert s_{\\\\ell} \\\\right\\\\vert}^2\\\\sum_{i,j}(d(w_i,w_j)-{\\\\lVert B(h_i - h_j)\\\\rVert}^2) \\\\end{matrix} 코퍼스 내 문장 $s_\\\\ell$과 $\\\\frac{1}{\\\\left\\\\vert s_{\\\\ell} \\\\right\\\\vert}^2$ index$\\\\ell$은 각 문장의 단어 쌍 수를 normalize한다. 중요한 것은 tree distance와 squared distance $\\\\lVert {h_i}-{h_j} \\\\rVert_B^2$ 의 차이를 최소화해야 한다는 것이다. (실제 vector 거리 ${\\\\lVert h_i - h_j \\\\rVert}_{B}$가 항상 실제 parse tree distance에서 벗어나지만 encoding된 tree정보는 일치하며, squared distance로 최적화하는 것이 가장 효과적이기 때문이다.) 3.1.3 Finding a parse depth-encoding norm',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': 'parse tree에서의 edge는 parse tree에 있는 단어들의 depth에 의해 결정된다. governance relationship(지배관계)에서 더 깊은 node는 지배 단어이다. parse tree에서의 depth는 norm과 같다. 이는 tree의 node를 결정한다. 이 tree depth norm을 ${\\\\lVert w_i \\\\rVert}$라고 한다. 우리는 squared L2 vector norm ${\\\\lVert Bh_i \\\\rVert}_{2}^{2}$ 으로 tree depth norm이 encoding되는 선형 변형이 존재한다는 것을 전제한다. 그리고 거리 가설처럼 depth norm가설은 best-approxiamted된 선형 변형을 찾을 수 있다. \\\\begin{matrix} \\\\underset{B}{\\\\min}\\\\sum_{\\\\ell}\\\\frac{1}{\\\\left\\\\vert s_{\\\\ell} \\\\right\\\\vert}\\\\sum_i(\\\\lVert w_i \\\\rVert-{\\\\lVert Bh_i \\\\rVert}^2) \\\\end{matrix} 3.2 Properties of the structural probe squared distance space가 syntax tree distance를 encoding한 representation space에 내적이 존재한다는 주장을 테스트할 수 있다. 이는 모델이 단어가 다른 단어에 의해 지배되는 것을 encoding할 뿐만 아니라 syntax tree에서의 단어들 간의 proximity를 encoding한다는 것을 의미한다. 3.3 Tree depth structural probes 단어 $w_i$에 대한 parse depth $\\\\lVert w_i \\\\rVert$는 parse tree에서 $w_i$의 edge와 tree의 root에 의해 결정된다. 본 논문에서는 tree norm을 encoding한 word representation space에 squared norm이 존재하는지 확인하고자 한다. 결과적으로 norm이 distance와 크게 다르지 않고 depth가 norm으로 표현된다고 한다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '지금까지 다룬 내용들을 정리해보면, embedding에 syntax information이 잘 반영되어 있는가를 보기 위한 과정이다. embedding공간에 tree구조가 직접적으로 나타나면 매우 쉽겠지만 그렇지 않기 때문에 선형변환을 이용한다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 그림에서 하얀 부분은 3차원 공간(embedding 공간)이고 회색 부분은 2차원이다. 본 연구에서는 3차원 공간에 있던 geometry(3차원 공간에 있는 것들을 표현한 점들과 파라미터)들이 2차원에 가서도 유지되는 선형변환 개념을 기반으로 해당 2차원 공간을 어떻게 찾을 것인가를 탐구한다. 따라서 본 논문의 probe는 선형변환(linear transformation) B를 찾는 것을 목표로 한다. The chef who ran to the store was out of food라는 문장에서 was와 chef는 6개의 단어를 끼고 있지만 syntactic residency측면에서 was와 chef의 distance는 1이고 was 와 가장 가까운 store는 distance 4가된다. 이처럼 vector space내에서 단어 간의 residency(distance)가 잘 측정되는 선형변환을 찾는 것이 본 논문의 목적이다. 즉, squared distance(squared L2 distance)와 tree distance간의 차이를 줄여나가는 방식으로 학습을 하여 적절한 선형변환을 찾고자 한다. 그리고 tree depth norm이 squared L2 vector norm으로 encoding되는 linear transformation이 존재할 것인데, 이를 찾는 문제도 linear transformation B를 찾는 문제로 치환을 하고자 한다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '정리를 하자면 본 논문에서는 parse tree에 대한 두 가지 geometry를 뽑는데 하나는 tree 구조 상의 distance, 하나는 parse tree depth이고 이 둘에 대한 선형 변환을 찾고자 한다는 것이다. 결국 depth norm과 distance norm을 가장 잘 근사하는 best-approximated model을 찾아 linear transformation B를 얻어내어 이 B를 가지고 다른 parse tree 혹은 다른 문장 embedding에 대해 선형변환을 하더라도 해당 task에 맞는 distance와 norm을 찾을 수 있게 하는 것이 최종 목표이다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '4. Evaluation Representation models 본 논문은 5.5B-word로 pre-trained된 ELMo의 weight를 사용한 ELMo model(1024 dimension)들과 BERTbase(768 dimension) 그리고 BERTlarge(1024 dimension)가 Penn Treebank의 parse tree를 얼마나 잘 재구성하는지를 통해 그들의 embedding 성능을 평가하고자 한다. BERT같은 경우에는 subword representation인데 이는 average pooling을해서 해결한다. 평가하고자 하는 representation은 ELMoK, BERTbaseK, BERTlargeK라고 부른다. Baselines 정확한 비교를 위해 linear, ELMo0, Decay0, PROJ0을 함께 살펴본다. -linear는 단순히 left-to-right chain을 따라서 parse tree가 형성되는 모델이다. 이 모델의 hidden state를 가져와서 proving한다. -ELMo0은 contextual 정보 없이 character-level에서 word embedding이 된 모델로, position정보도 없는 모델이다. -DECAY0은 weight가 단어의 수에 따라 깎이는 방식으로 학습되는 모델이다. 이 모델은 contextual 정보는 담지만 parsing하는 법은 학습하지 않은 모델이다. -PROJ0은 random initialized된 ELMo모델이다. 이 모델도 contextual 정보는 담지만 parsing하는 법은 학습하지 않은 모델이다. 4.1 Tree distance evaluation metrics',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 그림의 회색부분에 존재하는 vector 간의 거리를 기반으로 tree를 생성할 수 있다면 vector들이 tree정보를 embedding한 것으로 볼 수 있다. test sentence의 predicted parse tree distance를 구한 후 이것으로 minimum spanning tree(회색 부분)을 만든다. 그리고 predicted tree를 UUAS(undirected unlabeled attachment score)에 대한 predicted tree를 gold tree과 비교하는 방식으로 평가한다. 왜 UAS가 아니라 UUAS로 테스트하는지는 안 나와있는지 UAS로 테스트한 것보다 UUAS가 더 잘 나왔던 것 아닐까 싶다. UUAS는 UAS(unlabeled attachment score)이긴 한데 방향성이 없는 것이다. 위 예시에서 root는 are이고 이것의 nsubj는 key이다. 이런 관계에서 방향성이 없는 것이 UUAS이다. 따라서 UUAS로는 두 단어 간에 어떠한 관계가 있다는 것은 나타나지만 무엇이 root인지는 알 수 없다. 거리 상관(distance correlation)의 경우 실제 distance와 predicted distance의 Spearman correlation(스피어만 상관계수; 두 변수 간의 순위를 부여하고 검증하는 상관분석법. -1에서 1사이 값을 갖는다)을 계산한다. 길이별 문장에 대한 상관의 평균을 구하고 5-50길이의 문장에 대한 macro average(매크로 평균) 확인한다. 이를 “distance Spearman(DSpr.)” metric이라고 부른다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '4.2 Tree depth evaluation metrics tree의 depth를 재구성하는 능력은 tree depth evaluation metric을 활용하여 평가한다. 이것도 distance evaluation과 마찬가지로 실제 depth ordering과 predicted ordering 사이의 Spearman correlation확인한다. 우선 동일한 길이의 문장에 대해 평균을 내고 5050길이의 문장에 대해 평균을 낸다. 이를 “norm Spearman(NSpr.)이라고 부른다. 그리고 모델이 root를 판별할 수 있는지를 확인하기 위해 root%라는 것을 함께 활용하여 평가를 진행한다. 5. Result 위 표에서 Linear, ELMo0, DECAY0, PROJ0은 baseline이고 ELMo1, BERTbase7, BERTlarge15, BERTlarge16은 실제로 embedding이 syntactic tree정보를 제고해 낼 수 있을 만큼의 통사적 정보를 지니는지 확인하기 위해 사용된 모델이다. UUAS기준으로 BERTlarge15가 82.5로 가장 높은 점수르 보였고 root%의 경우에는 BERTlarge16이 가장 높은 점수를 보였다. 5.1 Reconstructed trees and depths',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 그림은 각 모델이 그린 parse tree이다. 윗부분의 검은 선은 실제 정답이고 아랫부분의 선은 structural prove가 예측한 것이다. BERTlarg16은 실제 정답과 거의 일치하는 것을 볼 수 있다. 본 연구에서 BERT를 학습시킬 때 parse tree를 직접 학습시키지 않았음에도 불구하고 주변 문맥을 고려하여 parse tree를 제대로 예측하였다. 이를 통해 embedding공간에 parse tree가 반영된 선형변환된 공간이 있다는 것을 확인할 수 있다. ELMo1의 경우에는 BERT보다는 부정확하다. edge들이 조금씩 다르다. PROJ0은 비교를 위해 사용된 baseline모델로, parsing능력이 없어 아주 엉망진창인 것을 확인할 수 있다. 위 시각화 자료에서 왼쪽이 gold parse distance matrix이고 오른쪽이 structural prove가 예측한 결과인데 매우 비슷하게 잘 나온 것을 확인할 수 있다. 색이 어두울수록 가까운 것이고 밝을수록 먼 것이다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 표는 parse depth에 대한 것이다. 검은색 동그라미가 실제 정답이고 파란색 네모가 BERTlarge16 그리고 빨간색 세모가 ELMo이다. 위 문장에서 includes가 root이고 그 밑으로 raising, plan, … 순으로 subtree가 나아가는 것을 볼 수 있다. BERTlarge16와 ELMo 둘 다 비슷하게 잘 예측한 것을 볼 수 있다. 이런 시각화 너무 마음에 든다… 5.2 Analysis of linear transformation rank',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 표는 본 실험이 아닌 추가적으로 진행된 실험의 결과를 시각화한 것이다. Probe의 maximum size가 어느 정도가 적당한지, 어느정도 size가 syntactic information이 vector space에 compact하게 잘 encoding되기에 적합한지 확인하기 위해 선형변환에 가장 효과적인 모델들을 대상으로 확인하였다. 그 결과 놀랍게도 효과적인 linear transformation이 요구하는 dimension이 굉장히 낮게 나왔다. 본 연구에서는 다양한 k에 대해 structural probe를 훈련시켰다, 즉 $B\\\\in\\\\mathbb{R}^k$(transformed vector $Bh$가 $\\\\mathbb{R}^k$에 있다). 위 표를 보면 $k$가 64정도 되니 성능이 수렴하기 시작한다. 따라서 64-128사이 정도의 linear transformation B가 적당하다는 것을 알 수 있다($k$가 커도 별 쓸모가 없다. 64-128 이후로 다 고만고만하다). 5.3 A brief example on subject-verb number agreement',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': 'verb와 그에 대한 subject 사이에 방해 요소가 되는 명사의 수가 아무리 많아도 verb는 subject의 수에 맞춰져야 한다. observational study에서 단수 주어와 복수 intervening noun들이 있는 문장에 대해 언어모델이 단수 동사와 복수 동사 중 어느 것에 높은 likeihood를 부여하는지를 확인하는 연구를 했다. parse tree distance formulation에서 subject-verb agreement behavior는 subject와 verb의 사이의 distance가 1이라는 것을 통해 설명될 수 있다. 본 연구에서는 이에 대해 증명하지 않았다. 하지만 아래 시각화 자료들을 보면 이것이 가능함을 알 수 있다. (조금 더 정확하게 말하자면 우리는 이 문제가 original subject-verb number agreement question과 일치한다는 것을 주장하는 것이 아니다. 모델이 정확한 동사 형태에 접근할 수 잇다는 것이다.) 아래는 BERT-large, layer 16, 4 sentences에 대한 structural probe가 예측한 distance matric과 minimum spanning tree이다. 각 문장들을 보면 점점 새로운 “attractor”가 추가되고 있다.',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': '위 parse tree를 보면 알 수 있듯이 막 엄청 정확한 것은 아닌데(“the”를 포함하지 않는다) 방해요소가 많아져도 subject와 verb가 가까운 것을 확인할 수 있다. 6. Conclusion 영어 parse tree의 geometry가 language deep model의 geometry로 유사하게 복원이 가능하다는 것이 매우 흥미롭다. 본 연구가 contextualized word representation의 geometry에 대한 syntactic property를 연구하는 것의 시작이기를 바란다. hidden state space의 linear transformation에서 찾아지는 구조가 dependency syntax만 있지는 않을 것이다. 이후 다른 graph structure를 더 찾아볼 계획이다. 7. Discussion 논문에 리뷰어의 진술을 언급하고 이에 대해 친절하게 반박하는 부분이다…? 리뷰어가 이거 그냥 headness에 대한 얘기만 하는 거 아니냐고 비판한 것에 대해 저자는 본 논문은 syntactic kowledge가 폭넓게 반영되었다는 것을 test한 것이 아닌 pair of words가 자신들의 syntactic distance를 잘 알고 있는지를 test한 것이라고 말했다. 그리고 그것도 vector space 내의 structural property를 가지고 이야기를 하는 것이기 때문에 이 점에서는 한계가 있을 수 있지만 본 논문의 의의는 embedding 내의 여러가지 type의 graph structure가 잘 embedding되어 있는지를 확인하는 것이라고 하였다. Reference Hewitt.J.”A Structural Probe for Finding Syntax in Word Representations,”Association for Computational Linguistics. 2019',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': 'https://nlp.stanford.edu/~johnhew/structural-probe.html#the-structural-probe https://github.com/john-hewitt/structural-probes × Search',\n",
       "  'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/'},\n",
       " {'text': 'Transferability(Transfer Learning(전이 학습)이 얼마나 가능한지): Pre-Trained contexualizer가 특정 Task에 대해 어느정도의 성능을 보이느냐에 따라 전이가 잘 되었는지 아닌지를 판단한다. Contextualizer: Sota를 찍은 NLP model에서 중요한 것은 Pre-trained representation이다. 전통적으로 word vector는 static한 word embedding이다. 즉 1 word, 1 vector이다. 이러한 방식은 Glove와 같은 모델에서 사용되었는데, 이러한 모델들은 단어들을 벡터화하는 것에 집중하여 단어 하나하나를 파악했기 때문에 문맥 파악에 취약하다. 최근 연구에서는 이러한 단점을 해결한 Elmo와 같은 모델을 통해 contextual word representations(CWR)을 사용한다. Glove보다 BiLM의 성능이 높기도 하지만 둘의 큰 차이점은 다음과 같다:Glove는 특정 동사를 POS tagging할 때 co-occurrence 빈도가 높은 것을 기준으로 tagging이 진행되는 반면 BiLM은 morphological inflection과 morphological derivation을 구분하여 POS tag가 붙는다.(Peters et al. 2018) 2. Model Flow',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '모델의 흐름은 위와 같다. 위 그림은 POS tagging task에 대한 그림이다. 우선 Input Token이 모델에 입력되면 이는 이미 대량의 데이터로 학습된 Pretrained Contextualizer를 통해 feature를 갖는다. 이 feature들은 Probing model을 거쳐서 결과가 나온다. 이렇게 Pretrained model을 사용했을 때가 그렇지 않은 경우보다 더 좋은 성능을 낸다고 하는데, 이러한 방식이 linguistic knowledge를 가져서 높게 나오는 것인지, 어떻게 linguistic knowledge를 가지는지 그리고 어떻게 그 지식들을 전이하는지에 대해 실험해 보는 것이 본 논문의 주요 골자이다. 3. Probing Tasks 본 연구에서는 17개의 Probing task를 통해 Linguistic Knowledge를 잘 파악했는지 확인한다. 이 Task들은 크게 Token Labeling, Segmentation, Segmentation로 나뉠 수 있다. 3.1 Token Labeling 1) part-of-speech tagging (POS) 형태소 분석 tagging이다. 이를 통해 CWR이 기본적인 통사를 파악했는지를 평가할 수 있다. dataset: the Penn Treebank (PTB; Marcus et al., 1993), the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014). 2) CCG supertagging (CCG) 어것은 문장 내 단어들의 통사적 역할에 대한 세부적인 정보를 확인하는 task이다. dataset: CCGbank (Hockenmaier and Steedman, 2007), a conversion of the PTB into CCG derivations',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '3) Syntactic constituency ancestor tagging 이 task의 경우 문장을 넣었을 때 문장 내에서 특정 단어가 어떤 단어에 대해 Parent인지, GParent인지, GGParent인지를 파악하는 것이다. 이를 통해 모델이 계층적 통사구조를 작 파악했는지 알 수 있다. dataset: phrasestructure tree (from the PTB) 어떤 문장이 입력되었을 때 문장 내의 단어들을 트리로 그렸을 때, 위 그림처럼 F자리에 오는 단어의 parent는 D이고, D의 입장에서 F와 G는 자식이다. 그리고 F와 G는 서로 sister이고 B는 F의 입장에서 grandparent이다. 4) Semantic tagging 문맥에서의 word’s semantic role(agent, patient, etc.)을 tagging하는 task이다. 이는 어휘적 의미를 파악하는지 그리고 불필요한 POS 변별하는지, 즉 POS tag에서의 유용한 것을 명확히 하는 지를 평가한다. dataset: dataset of Bjerva et al. (2016), the tagset has since been developed as part of the Parallel Meaning Bank (Abzianidze et al., 2017) 5) Preposition supersense disambiguation 여러 의미를 가질 수 있는 Preposition이 각각 어떤 의미를 위해 사용되었는지 tag해주는 task이다. 이를 통해 의미의 모호성과 어휘 의미론적 지식을 검증할 수 있다. 앞서 살펴본 tagging task들(모든 token에 대해 decision을 만들었던)과 달리 이 모델은 single-token preposition에 대해 학습과 평가가 이루어진다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': 'dataset: STREUSLE 4.0 corpus (Schneider et al., 2018) 6) Event Factuality (EF) 이것은 해당 Event가 실제로 발생했는지 안 했는지 확인하는 task이다. 예를 들어 Jo didn’t remember to leave와 Jo didn’t remember leaving에서 전자는 안 떠난 것이고 후자는 떠난 것이다. 이에 대해 factuality관점에서 바라보면 전자는 발생하지 않은 것이고 후자는 발생한 것이다. (해당 모델은 발생/비발생 예측 값을 [-3, 3]범위에서 값을 매기도록 훈련된다. dataset: Universal Decompositional Semantics It Happened v2 dataset (Rudinger et al., 2018) 3.2 Segmentation 7) Syntactic chunking (Chunk) 단어들이 서로 어떻게 관계되는지를 나누어 단어가 가지는 span과 boundary를 판별하는 task이다. dataset: CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000) 8) Named entity recognition (NER) 개체명 인식. 고유명사를 얼마나 잘 찾아내는지를 평가하는 task이다. dataset: CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003) 9) Grammatical error detection (GED) 문법 교정기 dataset: First Certificate in English (Yannakoudakis et al., 2011) dataset, converted into sequence-labeling format by Rei and Yannakoudakis (2016) 10) conjunct identification (Conj) coordination으로 연결된 것인지 판별하는 task. dataset: coordinationannotated PTB of Ficler and Goldberg (2016) 3.3 Pairwise Relations 11) Arc prediction binary classification task로, 두 token이 관계되어 있는지 확인하는 task이다. dataset: PTB (converted to UD), UD-EWT 12) Arc classification multiclass classification task로, 두 token이 관계되어 있는지, 그리고 어떤 관계이 있는지 분류하는 task이다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': 'dataset: PTB (converted to UD), UD-EWT 13) syntactic dependency arc prediction 14) syntactic dependency arc classification 15) semantic dependency arc prediction 16) semantic dependency arc classification 17) coreference arc prediction 공지시관계를 확인하는 task이다. dataset CoNLL 2012 shared task (Pradhan et al., 2012) 4. Models 4.1 Probing Model Probing Model은 선형모델을 사용했다. 4.2 Contextualizers Contextualizer로는 6가지 모델이 사용되었다. ELMo (Peters et al., 2018a): ELMo는 biLM을 통해 독립적으로 학습된 두 contextualizer의 결과를 concatenate한다. 여기서 사용된 ELMo model들은 sentence-shuffled newswire text (the 1 Billion Word Benchmark; Chelba et al., 2014) 800M token으로 훈련되었다. 1) ELMo (original): contextualiz하기 위해 2-layer LSTM을 사용한다. 2) ELMo (4- layer) 3) ELMo (transformer): 6-layer transformer를 사용한다. 4) OpenAI transformer: left-to-right 12-layer transformer language model로, contiguous text from over 7,000 unique unpublished books (BookCorpus; Zhu et al., 2015) 800M token으로 훈련되었다. BERT (Devlin et al., 2018): bidirectional transformer로 masked language modeling task 와 next sentence prediction task 훈련을 진행하였고, BookCorpus 와 English Wikipedia를 합쳐 약 3300M token으로 훈련되었다. 5) BERT (base, cased): 12-layer transformer 사용 6) BERT (large, cased): 24-layer transformer 사용',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '5. Pretrained Contextualizer Comparison: What features of language do these vectors capture, and what do they miss? 첫 번째 실험은 앞서 제시한 문제 중 첫 번째 문제와 관련되며, 어떤 언어학적 지식을 습득하고 어떤 것을 습득하지 못하는가에 대한 실험이다. 5.1 Experimental Setup 본 실험에 사용되는 probing model들은 각 contextualizer의 개별 layer를 통해 생성된 representation으로 훈련되었다. probing model들 간의 비교 외에도 noncontextual vector로 학습된 GloVe와 pretraining을 사용하지 않은 각 task의 이전 state of the art점수와의 비교도 진행된다. 5.2 Results and Discussion',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '위 표를 보면 각 모델에서 출력된 best layer를 pretrained representation으로 활용하여 각 task를 수행한 것을 볼 수 있다. Avg.는 모델들이 낸 성능의 평균값이다. BERT가 평균 성능도 높고 각 task들 사이에서도 상당 부분 높은 점수를 지니는 것을 확인할 수 있다. 심지어 몇몇 부분에서는 이전에 sota를 찍은 점수보다 높은 점수를 낸 것도 있다. ELMo-based model을 보면 recurrent model(LSTM)으로 학습된 orginal과 4-layer가 ELMo-transformer보다 높은 성능을 보이는 것도 확인할 수 있다. OpenAI같은 경우에는 ELMo기반 모델들과 BERT보다 낮은 점수를 보이고 있다. OpenAI는 여기에서 unidirectional로 훈련된 유일한 모델로, OpenAI의 저조한 성적을 통해 bidirection이 고성능 contextualizer에 핵심적인 요소라는 것을 확인할 수 있다. 위 결과를 보면 NER과 coreference task(본 논문 Appendix D에서 확인 가능)에 대한 transferable information을 학습하지 못하고 있는 것을 볼 수 있다. 이러한 취약점을 완화하기 위해 explicit entity representation을 사용한 pretrained contextualizer를 강화하는 시도를 해볼 수 있다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '(위 표는 정리된 표인데 본 논문 Appentix를 보면 결과들이 쭉 나와있다.) 5.3 Probing Failures 앞서 살펴본 표를 보면 해당 실험에서 contextualizer를 통해 추출한 feature가 각 task들에 잘 전이된 것을 확인할 수 있다. 하지만 transfer에 실패한 몇몇 task(논문 Appendix부분에서 보면 개체명 인식, 문법 판독기, conjunction 그리고 grandparents를 뽑는 task의 점수가 낮다.)들이 있다. 실패한 task의 요인은 두 가지로 볼 수 있다: (1) CWR이 적절한 정보나 예측 가능한 관련성을 encoding하지 못했거나(언어학적 지식들을 학습하지 못했다는 말) (2) probing model이 정보나 예측 가능한 관련성을 vector로부터 추출할 능력이 없는 것(proving model이 약했다는 말). 1번 요인은 CWR이 task-specific한 정보를 인코딩하지 못하고 있는 것인데, 즉 pretrained contextualizer에 정보가 충분히 들어가고 있지 않다는 것인데, 이를 해결하기 위해 task-specific contextual feature를 학습해야 한다(주입식 교육 시켜야한다는 말…). 이렇게 contextual probing model이 task-specific contextual feature를 학습하는 것은 2번 요인의 해결책이 될 수 있지만 결과적으로 probing model의 능력을 향상시키는 것과 비슷하다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '본 연구의 probing model들의 실패를 더 잘 이해하기 위해 아래 표와 같이 또 다른 실험을 진행하였다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '(1) Linear: 원래 이전에 썼던 모델 (2) MLP: linear probing model을 MLP(multilayer perceptron; here used: single 1024d hidden layer activated by ReLU)으로 교체하여 더 많은 parameter를 probing model에 추가하는 실험. (그냥 다른 task처리는 전혀 안 하고 차원을 늘려 계산을 좀 더 복잡하게 하겠다는 것) (3) LSTM+ Linear: linear output layer이전에 task-trained LSTM(unidirectional, 200 hidden units)을 사용한 contextual probing model을 사용하여 task-specific한 contextualization을 추가하는 실험. (LSTM을 task에 맞게 훈련시키는 것. 주입식교육시켜서 feature를 더 잘 추출할 수 있게 학습시킨 모델을 probing model을 사용) (4) BiLSTM+MLP: CWR의 output이 BiLSTM(512 hidden unit)에 들어가고 이 output은 다시 MLP(single 1024-dimensional hidden layer activated by ReLU)에 들어가는 것.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '위 그림을 보면 일단 MLP나 LSTM+Linear나 일단 기존의 Contextual Word Representation을 사용하고 있다. 같은 CWR을 기반으로 probing model쪽만 손본 것인데 위 표에서 NER과 grammatical error detection task의 경우 LSTM+Linear(task에 맞게 학습시킨 model)과 MLP(단순히 차원만 늘린 model)의 성능 차이가 별로 없다. 이는 Contextualizer은 개체명 인식을 위한 언어학적 지식을 충분히 학습했는데 Linear model의 차원이 부족해서 계산을 못한 것으로 볼 수 있다. 반면 조금 더 specific syntactic 지식을 요하는 conjunct identification과 great-grandparent prediction task의 경우에는 MLP와 LSTM+Linear(conjunction을 잘 추출하도록 훈련된 LSTM)의 점수 차이가 크다. 이는 Contextualizer가 conjunction에 대한 언어학적 정보를 충분히 학습하지 못했음을 보여준다. 이러한 실험은 end task가 pretraining task가 학습하지 못한 특정 정보를 요구할 때 task-trained contextualization이 중요하다는 것을 보여준다. 따라서 단순히 MLP를 쓰는 것 보다는 fine-tuning을 이용해서 task에 맞는 추출부(decoder)에 신경을 쓰는 것이 중요하다고 본다. 6. Analyzing Layerwise Transferability: How and why does transferability vary across representation layers in contextualizers? 이번 실험은 앞서 제시한 문제 중 두 번째 문제와 관련되며, 모델들의 layer에 어떤 차이가 있는지 보는 실험이다. 6.1 Experimental Setup',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': 'ELMo-based model은 contextualizer architecture만 다르기 때문에 제어된 비교에 용이하다. 따라서 여기에서는 layer들의 차이를 보는 것이 중요하기 때문에 ELMo-based model에 집중한다. ELMo-based model들은 수만개의 Word Benchmark로 훈련되었기 때문에 유연한 domain shift를 위해 softmax classifier만 비슷한 데이터로 재학습시킨다. 6.2 Results and Discussion',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': 'Recurrent model(LSTM)인 ELMo original(2-layer)과 4-layer ELMo에서는 layer 1의 output을 그대로 proving model로 사용했을 때 가장 높은 performance가 나왔다. 이는 LSTM-ELMo model의 경우, low level layer일수록 더 일반적인 정보를 가지고 있음을 나타낸다. 즉, low level layer일수록 더 transferable하다. 때문에 여러 task에 있어 좋은 점수가 잘 나오는 것이다. 반면 transformer-based model의 경우에는 첫 번째 layer의 결과도 안 좋고 마지막 결과도 안 좋은데 대부분 중간 layer의 결과들이 좋은 것을 확인할 수 있다. 이는 transformer model은 middle level layer가 일반적인 정보를 많이 가지고 있음을 나타낸다. 따라서 middle level layer의 transferability가 높다. 결과적으로 LSTM의 경우, low level layer에서 일반적인 언어지식이 가장 많이 전달될 수 있기 때문에 여러 task를 수행하기 위해서는 LSTM-layer를 많이 쌓아봐야 별 소용이 없음을 알 수 있다. 그리고 Transformer의 경우, middle level layer에서 일반적인 언어지식이 가장 잘 학습되기 때문에 middle level later의 출력을 활용하는 것이 여러 task를 수행하는 것에 도움이 된다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '위 표는 하나의 특정 task에 대한 각 모델의 layer들에 대한 PPL(perplexity, language model을 평가하는 척도로, 낮으면 낮을 수록 좋다)를 평가한 것을 나타낸다. ELMo original은 layer 2에서 PPL이 가장 낮고 ELMo 4-layer에서는 layer 4에서 가장 낮다. 이는 task specific한 정보는 layer를 쌓을수록 higher layer에 담긴다는 것을 나타낸다. 반면 transformer의 경우에는 monotonic increase pattern이 나타나지 않는다. 이러한 결과를 통해 LSTM과 Transformer에 대한 layerwise behavior의 차이도 알 수 있다. LSTM의 layer는 task specific하게 학습되는 반면 transformer을 그렇지 않다. RNN계열의 모델(ex. LSTM)과 Transformer계열의 모델(ex. BERT)의 syntactic knowledge encoding behavior pattern은 다르다. (본 논문에서는 이 차이가 가지는 의미에 대한 설명이 자세히 나오지 않는다.) RNN은 저차원의 syntax개념부터 고차원의 개념까지 monotonic increasing pattern을 보이며 encoding되고, Transformer는 non-monotonic behavior pattern을 띈다. monotonic increasing pattern은 computer vision쪽으로 생각하면 이해가 편하다. 위 그림처럼 input으로 조각들이 들어가고 hidden layer에서 처음에는 edge를, 그 다음에는 corner과 contour를 학습하고 마지막으로 object part를 학습하여 car, person, animal과 같은 object identity를 구분하는 것이 monotonic increasing pattern이다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '7. Transferring Between Tasks: How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? 마지막으로 특정 task에 대해 training했을 때 그것이 어떤 결과를 가져오는가에 대한 실험이다. 지금까지 contextualizer들은 bidirectional LM과 같은 self-supervised task를 사용했다. 이런 contextualizer 자체를 특정 task에 대해 사전훈련시킨 결과를 보도록 하겠다. pretraining task의 결과가 언어학적 지식을 습득하는데 얼마나 영향을 미치고 CWR의 전이성에 얼마나 영향을 미치는지 확인하기 위해 bidirectional LM 사전학습결과와 contextualizer를 사전학습시킨 것을 비교해보도록 하겠다. 7.1 Experimental Setup 사전학습 task들의 제어된 비교를 위해 contextualizer의 architecture와 사전학습 dataset은 고정한다. 이 실험에서 사용하는 모든 contextualizer는 모두 ELMo(original) architecture를 사용하고, pretraining task에 대한 사전학습 dataset은 PTB를 사용한다. 비교 모델은 아래와 같다: (1) GloVe: contextualization의 결과를 확인하기 위한 non-contextual baseline을 사용한다. (2) Untrained ELMo(original): pretraining의 결과를 측정하기 위해 randomly-initialized, uncontextualization basline을 사용한다. (3) ELMo(original): bidirectional LM을 다량의 데이터로 학습시킨 결과를 보기 위해 Billion Word Benchmark로 사전학습시킨 모델을 사용한다. 7.2 Results and Discussion 위 표를 보면 bi-LM의 점수가 가장 높은 것을 확인할 수 있다.',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '하지만 위 표는 특정 task에 맞춰서 사전학습한 것에 대한 결과인데 syntactic arc prediction의 경우에는 이처럼 특정 task에 맞춰서 학습한 결과가 더 높은 것을 확인할 수 있다. 이에 따라 어느정도 task specific한 contextualize를 하는 것도 고려해야 한다는 것을 알 수 있다. Reference Liu et al. “Linguistic Knowledge and Transferability of Contextual Representations,”Association for Computational Linguistics. 2019 https://arxiv.org/abs/1808.08079 https://dmitry.ai/t/topic/172 Deep Learning 2017 × Search',\n",
       "  'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/'},\n",
       " {'text': '신경망연구(Neural network research)와 언어학(Linguistics)의 융합은 두 분야의 발전을 이끌 수 있다. 언어학에서는 신경망을 통해 유의미한 언어학적 결과를 도출할 수 있고, 신경망 연구에서는 언어학적 지식(Language capabilities)을 통해 딥러닝의 성능(performance)를 개선할 수 있다. 1. the cognitive science of language: beneficial to neural network system(deep learning) 우선 언어학이 딥러닝에 도움이 되는 방법에 대해 알아보겠다. 1.1 Linguistics as a Normative standard',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '문장은 선형적이며 계층적이다. 문장 내에 존재하는 순서는 문장의 의미를 구성하는데 있어 매우 중요한 역할을 한다. RNN(Recurrent Neural Network)모델은 노드들 사이에 hidden layer를 삽입하여 hidden state의 결과가 계속 순환하며 문장 내 순서를 고려할 수 있게 만들어진 모델이다. 예를 들어 The cat is on the mat이라는 문장에서 cat 다음에 is가 올 확률이 높다는 것과 같은 방식으로 구조적으로 수용 가능한 문장을 형성할 수 있다. 하지만 통사적으로 간단한 문장이 높은 확률을 할당 받는다 하더라도 언어학자들이 연구하는 논란의 여지가 많은 문장들을 연구할 수 있는 수준이 되는 것은 아니다. 그래도 최근 언어모델(Linzen(2019)기준)은 언어학적 지식을 기반으로 통사적 능력(ability)를 측정하고 향상시키는데 높은 성능을 보이고 있다. 이렇게 언어학은 딥러닝 성능 향상의 기준점이 될 수 있다. 예를 들어 본 논문의 저자인 Tal Linzen은 2016년에 정문(grammatical)과 비문(ungrammatical)을 이용해서 언어모델이 정문에 더 높은 확률을 줄 수 있는지에 대한 실험을 하였는데 이에 대해',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '간단히 살펴보도록 하겠다:',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '1.1.1 Linzen et al.(2016): Grammatical/Ungrammatical 1) English subject-verb agreement (1) The black cat … sneezes/sneeze? 위 예문은 주어와 동사의 agreement(주어, 동사 수 일치)에 대한 문제인데, black cat에 대해 알맞은 동사를 고르는 것이다. 이 경우는 그냥 더 빈번히 결합되는 동사를 고르면 되는 것이기 때문에 언어모델의 입장에서 매우 쉬운 테스크이다. 2) Head 문장 내 여러 명사구가 나올 경우 어떤 것을 주어로 볼지에 대한 문제는 조금 더 복잡하다. (2) The ratio of men who survive to the women and children who survive … is/are?',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '위 예문에서 ratio가 주어인데 중간에 이탤릭체로 쓰여진(men, women, children)은 모두 방해요소들(attractors)이다. 이들은 주어와 달리 복수명사인데, 주어를 제대로 식별하지 못하면 동사를 are로 고르게 되는 것이다. 딥러닝은 이러한 문제를 사람보다 더 헷갈려 한다. 본 실험은 작은 규모로 진행되었는데, 4개 이상의 attractor가 있을 경우 50%이상의 오류가 나오는 것으로 확인되었다. 이후 Gulordava et al. 2018의 연구에서는 조금 더 큰 규모로 실험을 진행해서 이보다는 높은 정확도를 얻기는 했지만 여기에서도 마친가지로 동사 앞에 4개 이상의 attractor가 존재할 경우가 아닌 경우에 비해 오류율이 10배 이상 높은 것으로 나타났다. (본 논문에서 위키백과 코퍼스를 분석한 결과 95%이상의 구문에서 명사구가 없는 것으로 확인되었고, 명사구가 3개 이상인 경우는 0.25%로 매우 드물게 나타나는 것으로 확인되었다. 그니까 대부분의 문장에 대해서는 문제가 없다는 것인데, 사람만큼 완벽하게 식별하지는 못한다는 것이다. 근데',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '사람과 최대한 유사한 언어 기능을 구사하는 기계를 만드는게 목표니까… 그 이후 더 개선된 모델들이 계속 나오는 중이다.)',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '이외에 신경망 네트워크를 이용한 모델에 대한 연구들은 다음과 같다:',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'Tran, Bisazza, and Monz (2018): 모델의 probability와 prediction accuracy간의 관계를 연구. Vaswani et al. (2017): Attention only model을 사용할 경우 attractor가 4개인 문장에 대해 RNN에서 약 3배 정도의 오류가 나왔다는 것을 확인한 연구. Kuncoro et al. (2018): 통사적인 내용을 명시적으로 모델링한 신경망 시스템으로 오류를 줄일 수 있다는 연구.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '지금까지 기술한 내용을 요약하자면, 언어학이 딥러닝의 기준점이 될 수 있다는 것이다. 1.2 Psycholinguistics and Experimental Control',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '이번에는 심리언어학적 실험통제가 딥러닝에 어떠한 도움을 줄 수 있는지를 다뤄보겠다. 딥러닝은 훈련을 하고 훈련 결과를 테스트한 후에 테스트 결과를 다시 훈련에 반영하는 과정을 반복한다. 하지만 이러한 훈련과 테스트 과정에는 동일한 데이터셋이 사용된다. 즉, 코퍼스 하나를 (일반적으로)7:3으로 나눠서 7은 traning set으로, 3은 test set으로 사용하는데 여기에서 문제가 나타난다. 우리가 코퍼스를 딥러닝 모델에 열심히 돌리는 이유는 뭔가 살펴보고자 하는 property가 있기 때문인데, 코퍼스에는 이와 관련된 정보가 따로 주석되지 않았기에 문제를 해결하기 어려운 것이다. 예를 들어 yes/no question 형식의 문장을 생성하고자 할 때, 코퍼스에는 문장 형식(의문문, 평서문, 명령문 등)에 대한 정보가 없기 때문에 모델이 yes/no question 형식을 찾아내는 것이 매우 어렵다. 그렇다면 이미 parsing이 된 Penn Treebank와 같은 코퍼스를 사용하면 되지 않냐는 의문이 제기될 수 있지만, 그러한 코퍼스들은 아직',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '딥러닝에 사용할 만큼의 규모로 구축되지 않았다는 문제가 있다. 그렇다면 이제 또 다른 의문이 제기될 수 있다. 모델이 입력되는 대규모 코퍼스를 바로바로 parsing하면 되지 않느냐 물을 수 있다. 이러한 방법은 통사론적으로 매우 간단한 문장에서는 제대로 작동할 수 있지만 통사론적으로 조금만 복잡한 문장의 경우에는 parsing이 제대로 이루어지지 않는다. 하지만 우리는 통사론적으로 복잡한 문장을 딥러닝이 잘 분석하고 이해하기를 원하기 때문에 이와 같은 방식으로 도출된 결과는 딥러닝 모델에 사용할 수 없다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '이러한 문제들을 해결하기 위해 심리언어학적 실험통제를 딥러닝에 대입하는 방안을 모색하였다. 심리언어학은 지금까지 사람을 대상으로 언어 현상을 실험하기 위해 통제된 실험 데이터셋(controlled data set)을 구축해왔기 때문이다. 이러한 데이터셋은 인간을 대상으로 실험한 결과이기 때문에 기계가 처리하기에는 어려운 결과들로 이루어져 있다. 따라서 이를 통해 모델의 언어능력(language competence)을 보다 확실하게 확인할 수 있다. Marvin and Linzen (2018)의 연구가 이와 관련한 대표적인 연구로, 아래 예문 3, 4, 5번과 같은 비문-정문 pair로 모델의 언어 능력을 평가한 실험이다. (3) The farmer that the parents love swims/*swim. (4) The manager that the architects like doubted himself/*themselves. (5) a. No authors that the security guards like have ever been famous. b. *The authors that no security guards like have ever been famous.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '예문 3번의 경우, that the parents love라는 관계절을 고려하여 주어와 동사의 수일치를 이룰 수 있는지를 볼 수 있다. 예문 4번의 경우는 anaphora의 distribution에 대해 알아보는 문장으로, that the architects like가 manager에 IP(Inflection Phrase), TP(tense phrase) 혹은 CP(Complementizer Phrase)로 귀속될 수 있기 때문에 architects는 뒤에 나오는 anaphora를 C-commend(성분통어)하지 못한다. 따라서 binding theory에 따라 architects와 수가 맞더라도 themselves가 올 경우 비문이 된다. 예문 5번은 NPI(부정극어)에 대한 distribution을 알아보는 문장이다. 이러한 문장은 모두 2009년에 사람을 대상으로 진행된 실험에서 가져온 문장으로, tagging이 전혀 되지 않은 raw 코퍼스에서는 찾아보기 힘들다. 이에 따라 tagging되지 않은 코퍼스로 훈련한 RNN모델에 위와 같은 문장으로 test를 돌려본 결과, 정확도가 극적으로 떨어지는 것을 확인할 수 있었다. 따라서 이러한 실험을 통해 모델의 취약점을 찾아 보완할 수 있다. 하지만 이와 같은 실험을 진행할 때 local heuristics를 방지해야 한다. 즉, 모델이 잔머리 굴리는 것을 방지해야 한다. (6) a. The little boy who is crying is hurt. b. Is the little boy who is crying hurt? c. *Is the little boy who crying is hurt?',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'local heuristics를 확인하기 위해서는 bigram model을 써보면 된다.bigram은 문장을 단순히 선형적으로만 봐서 문장의 구조를 파악하지 못하는 애다. 얘가 예측을 잘했다는 것은 그 문장은 모델이 잔머리 굴리기 딱 좋은 문장이라는 말이다. 위 예문은 6a를 yes/no question 형태로 만드는 예시이다. 위 예문에 대해서 단순히 바로 직전의 단어를 가지고 예측하는 모델인 bigram model이 6c가 비문이라는 것을 잘 예측하였다. 이는 그냥 단어쌍의 빈도를 기반으로 도출된 결과이다. 즉, who is라는 단어쌍이 who crying이라는 단어쌍 빈도보다 높았기 때문이다. 이런 것을 local heuristics라고 한다. 결과는 제대로 나왔지만 모델이 통사를 제대로 파악한 것은 아닌 것이다. 따라서 예문 6과 같은 문장으로 test를 진행하고서 모델의 통사 파악 능력을 높게 평가하는 것은 의미가 없다는 것이다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'local heuristics를 방지하기 위해 Adger(2003)연구에서는 언어학 교과서나 논문 등에서 나온 minimal pair를 가지고 모델을 테스트했는데, 여기에서도 bigram이 예측을 매우 잘했다. 심지어 bigram이 RNN보다 높은 성능으로 측정된 결과도 있었는데, 이는 언어학 교과서나 논문에서 발췌한 minimal pair도 그냥 가져와서 쓰면 local heuristics를 피할 수 없다는 것을 증명한다. Gulordava et al. (2018)의 RNN이 selectional preference를 학습하여 문장을 의미론적으로 예측했는지에 대해 실험하였다. (7) The dogs playing in the neighborhood park … barks/bark? 예를 들어 예문 7에서의 동사 예측은 통사적 분석이 아닌 반드시 selectional preferences와 semantic plausibility를 고려하여 이루어져야 한다. 왜냐하면 neighborhoods와 parks는 짖을 수 없기 때문에 the dog만이 head of the subject가 될 수 있는 것이다. (8) The ideas rowing in the lamp economy … barks/bark?',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '예문 8은 촘스키의 ‘colorless green ideas’라는 문장과 같이 의미론적 측면은 완전히 배제하고 통사적 측면만을 고려하여 만들어진 문장이다. 이 문장을 RNN이 제대로 이해했다면 barks와 bark를 5:5 확률로 예측할 것이다. 하지만 실제 실험에서는 bark가 더 높은 확률로 예측되었다. 따라서 Gulordava et al. (2018)의 실험을 통해 RNN이 통사론적 측면은 어느정도 학습했지만 의미론적 측면은 잘 이해하지 못하는 것으로 확인되었다. 2. neural network system(deep learning): beneficial to the cognitive science of language 이제 딥러닝이 언어학에 도움이되는 방법에 대해 알아보겠다. 2.1 Innate Biases in Human Language Acquisition 딥러닝은 언어습득(language acquisition)에 내재된 bias의 존재 여부를 판단하는 언어처리 도구로서 유용하게 사용될 수 있다. 언어습득에 내재된 bias의 존재에 대해 연구하는 이유는 poverty of stimulus라는 개념에 있다. poverty of stimulus에 대한 논증은 직관에 근거하며, 딱히 과학적인 근거는 없다. 따라서 이러한 직관이 정확한지, vaild한 intuition인지, 즉 인간에게 실제로 언어에 대한 constrain이 내재되어 있는지 살펴보기 위해 딥러닝을 사용할 수 있다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'poverty of stimulus는 말 그대로 자극의 빈곤이다. Poverty of stimulus는 language acquisition에서 가장 중요한 개념 중 하나이며, 매우 특이한 현상이다. 간단하게 말하자면 인간에게는 언어에 대한 어떤 constrain이 내제되어 있다는 개념이다. 인간이 L1을 배우는 과정에서 산출해낼 수 있는 많은 문장에 비해 실제 받아들이는, 자극이 되는 문장의 수가 매우 제한적이라는 개념이다. 즉 통사론에서 다루는 복잡한 통사구조(e.g. cleft construction(분열문))의 경우, 코퍼스에서는 0.1%정도밖에 되지 않고, 아이가 언어를 배우는 과정에서도 매우 적은 빈도로 접할 수 있다. 하지만 아이가 약 8세정도가 되면 cleft문장을 완벽하게 구사한다. 이탈리아 언어학자 Antonella Sorace는 이탈리아어에서 자동사(intransitive)의 unaccusative(비대격)과 unergative(비능동격)의 형태가 다르다는 것을 가르치지 않아도 아이들이 자연스럽게 구사한다는 것을 발견했다. 이렇게 인간은 적은 작은으로도 critical period를 지나면 언어를 아주 fluent, grammatical, well-formed로 구사한다. 이것이 기계와 인간의 언어학습에 있어 가장 중요하고',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '근본적인 차이이다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '인간의 innate Bias의 존재 여부를 확인하기 위해 우선 언어 습득 모델을 모델링한다. 그리고 이 모델에 constraint를 준 버전과 안 준 버전을 비교한다. 그 결과 constraint가 있는 모델이 언어를 잘 습득하고 처리해낸다면 constraint를 필요로 한다는 결과가 도출되고, 반대로 constraint가 없는 모델이 언어를 잘 처리한다면 constraint는 딱히 필요 없는 것이다. 이렇게 딥러닝을 통해 모호하고 직관에 근거했던 이론이 empirical evidence를 얻게 될 수 있다. 이와 관련하여 McCoy et al. (2018) 연구를 살펴보도록 하겠다. 해당 연구에서는 모델의 yes/no question formation 학습 여부를 테스트하였는데, 여기에서 사용된 모델은 seq2seq)이다. (10) My walrus that will eat can giggle. (11) a. Can my walrus that will eat giggle? b. *Will my walrus that eat can giggle?',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '이 실험에서는 예문10과 같은 평서문이 주어지지 않은 상태에서 11번을 형성하는지에 대해 실험하였다. 그 결과 몇몇 network가 structural components를 잘 학습하였는데, 이 network들과 어린 아이들이 보이는 오류 양상에는 차이가 있는 것을 발견하였다. 따라서 이러한 연구를 통해 neural network와 인간의 학습 방식은 다르다는 것을 확인할 수 있었고, 그에 따라 neural network에는 syntactic constraint가 딱히 필요하지 않다는 것도 알 수 있었다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'McCoy et al. (2018)에서 진행된 위와 같은 실험에도 한계점이 있다. 문장 a와 b가 있을 때 b가 비문이라면, 제대로 학습된 모델은 a에 bias를 줄 것이고 제대로 학습되지 않은 모델은 그러지 못할 것이다. 하지만 해당 실험에서는 두 부류의 모델 간의 bias 할당 차이가 거의 없었고 hidden layer의 equation에서만 차이를 보였는데 해당 연구에서는 이에 대해 설명하지 않았다. 이는 딥러닝 모델의 전반적인 문제이긴 한데, 모델의 black box 안에서는 무슨 일이 일어나는지 알 수 없다. 최근 explainable AI를 시도하는 연구들이 있지만 내가 모르는 것인지 아직 이렇다 할 결과는 없는 것 같다. McCoy et al. (2018) 연구와는 달리 neural network에 explicit bias를 줘버리는 연구도 제안되었지만 아직까지 syntax를 explicit하게 준 모델이 주지 않은 모델에 비해 월등히 좋은 성능을 낸 모델은 없었다고 한다. 2.2 Modeling Human Sentence Processing 2.2.1 Markov Chain',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '기계가 사람처럼 문장을 처리하기 위해서는 사람의 reading behavior를 파악해야 한다. 인간의 독해력에는 뒷단어 예측 실력이 영향을 준다. 즉, 앞선 단어를 읽고 뒤에 나타날 단어를 예측할 수 있으면 빠른 속도 독해를 할 수 있고 단어를 건너뛰고도 문장의 내용을 파악할 수 있다. 이와 같은 예측 능력은 predictability estimate(예측 가능성의 수치)를 통해 모델링될 수 있다. 문장 내 단어를 예측한다는 것은 확률변수가 아닌 확률과정 개념으로 보는 것이 타당하다. 확률과정이 도입된 개념은 Markov Chain이라고 하는데, 이는 어떠한 상태가 이전 상태에 영향을 받는 것을 나타낸다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'Smith & Levy (2013)의 연구에 따르면 n-1에 기반하여 다음 단어를 예측하는 N-gram모델로도 reading behavior를 잘 모사했다고 한다. 2.2.2 Grammatical Language Model vs. RNN Language Model 사람은 Incremental Parsing과 같은 문법 기반의 언어모델을 통해 문장을 처리한다. 하지만 이런 Parser기반의 언어모델이 단순한 Sequential model보다 더 효율적이라는 것을 수학적으로 증명하기란 쉽지 않다. 증명이 어려워서 심지어 Frank & Bod(2011)에서는 기계가 문장을 이해하는데 있어 통사적 정보가 불필요하다는 주장이 나오기도 했다. 근데 이건 아닌 것 같다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '문법 기반의 언어모델들(Grammatical Language Model)은 parsing된 코퍼스를 사용하는데, parsing된 코퍼스는 몇몇 한계점을 지닌다. 우선 사람이 직접 parsing을 해야 한다는 번거로움이 있고, 사람이 직접 만들기 때문에 단기간에 대규모 코퍼스 구축이 힘들어 대부분 크기가 작다. 그리고 인건비가 많이 들어가서 구축 비용이 비싸다. 반면 RNN모델은 매우 큰 코퍼스를 사용하더라도 훈련이 잘 된다. 그리고 Goodkind & Bicknell(2018)에 의하면 N-gram모델보다 human reading behavior를 예측하는데 높은 성능을 보인다고 한다. Goodkind & Bicknell(2018)가 도출한 결과에서 중요한 것은 RNN이 N-gram과 같이 단순히 문장의 sequence를 잘 기억하여 좋은 성능을 보이는 것이 아니라는 것이다. 이들은 RNN과 문법기반 언어모델 모두 garden-path sentence(구문분석 난이도가 높은 문장)를 처리하는데 효과적이라는 점을 언급하였다. 2.2.3 Human behavior in question might arise form statistical patterns in the input',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'Linzen and Leonard (2018)는 RNN과 인간이 범하는 오류의 패턴을 관계절과 전치사구의 attractor을 비교한 Bock and Cutting (1992)을 이용하여 상세히 분석하였다. 또한 이들은 주어가 단수일 경우와 복수일 경우의 수 일치도 비교하였는데 실험 참가자들이 주어의 head가 복수일 때보다 단수일 때 더 많은 attraction error를 냈다고 했다. 이러한 수의 불일치는 makedness(유표성)에 기인한다. 즉 복수명사는 단수 주어에 의해서 interrupt될 수 있는 privative feature(탈취자질)이다. 반면 단수 명사는 복수 주어와 encoding하는데 interrupt될 수 있는 자질을 가지지 않는다. 이러한 결과를 주목해봐야 하는 이유는 RNN에 유표성 개념을 주입하지 않았음에도 불구하고 인간과 비슷한 불일치를 보였다는 것이다.',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '반면 인간은 관계절 속에 attractor가 있을 때 오류를 적해 범했지만 RNN은 전치사절 속에 attractor가 있는 경우 더 많은 오류를 범했다는 결과도 있다. 이러한 결과는 RNN이 인식한 syntactic representation과 인간의 syntactic representation에는 차이가 존재한다는 것을 나타낸다. 이러한 결과를 통해 human data에 맞춰진 모델 설계가 필요하다고 볼 수 있는데, 이 부분이 매우 어렵다. 고전적인 언어 문법 모델의 경우에는 수작업으로 모델 수정이 가능했지만 딥러닝 모델에는 black box가 존재하여 안에서 뭐가 어떻게 돌아가는지 알 수 없기 때문에 수정작업 자체가 vague할 수 있다. 3. Competence model vs. Performance model',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'Linguistics as normative standard for neural network와 Modelling failable human language processing 접근법에는 큰 차이가 존재한다. 전자는 neural network가 언어학 이론으로 규범되는 규칙을 오류 없이 수행하는 것이 목표인 모델이고, 후자는 인간도 실수할 수 있는 언어처리를 모델링하는 것이 목표인 모델이다. 이 두 접근법은 각각 정해진 규범을 완벽하게 따르는 것을 목표로 하는 linguistic competence와 실제 발화 방식에 초점을 둔 linguistics performance에 기준점을 둔다는 차이를 보인다. 이는 곧 하나의 모델로 두 목표를 이룰 수 없음을 말한다. 즉, Competence model과 Performance model의 경로가 다르다는 것이다. 이 둘의 목표를 모두 이룰 수 있는 모델은 아직없다. 4. Conclusion',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': '지금까지 딥러닝과 언어학이 상호 나눌 수 있는 이익에 대해 알아보았다. 언어학 분야에서는 언어 능력의 표준을 정의하는 것에 특화되어 있다. 이러한 competence를 통해 딥러닝 분야는 표준 규범이 지켜졌는지 테스트할 수 있고, 지켜지지 못한 경우에는 그의 원인을 해석해볼 수 있다. 그리고 딥러닝은 언어학 분야에 대해 언어습득 모델 구축과 문장 처리를 위한 유용한 플렛폼을 제공할 수 있다. Reference',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'},\n",
       " {'text': 'Linzen et al. “What Can Linguistics and Deep Learning Contribute to Each Other?,” Johns Hopkins University. 2019 × Search',\n",
       "  'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "  'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/'}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02ef18-050a-4dcc-a035-b792ba2af6b3",
   "metadata": {},
   "source": [
    "## save chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5460018-2134-45bb-862a-fa4e4c86998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.6.5.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# db client 지정\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=\"http://192.168.2.186:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e39e94c5-8834-470c-8f69-f19d56e20ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batch.configure(batch_size=100)\n",
    "# client.data_object.create(\n",
    "#     class_name=\"JeopardyQuestion\",\n",
    "#     data_object={\n",
    "#         \"question\": \"This vector DB is OSS & supports automatic property type inference on import\",\n",
    "#         # \"answer\": \"Weaviate\",  # schema properties can be omitted\n",
    "#         \"newProperty\": 123,  # will be automatically added as a number property\n",
    "#     }\n",
    "# )\n",
    "with client.batch as batch:\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        vector = get_embedding(chunk[\"text\"])\n",
    "        batch.add_data_object(data_object=chunk, class_name=\"B_with_title\", vector=vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddc627-b1f0-41ce-8e32-bf3ce146209b",
   "metadata": {},
   "source": [
    "### save check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348aec9f-1429-400c-a8f2-b7037c3afbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.query.get(\"B_with_title\", [\"text\",\"source_title\",\"source\"]).do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "336f8dee-7da6-4d35-8e7b-17a34baa0028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Get': {'B_with_title': [{'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/',\n",
       "     'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "     'text': '실험 2도 1과 같이 모든 embedding level에서 주어 동사 일치와 불일치 간의 surprisal이 뚜렷하게 구분된다. 잘 파악하고 있다. 6.3 Experiment 3 result: Filler-gap surprisal in subject-inverted interrogatives 앞서 말했듯이 첫 번째와 네 번째 박스가 비문이고 두 번째와 세 번째가 정문이다. 비문에 해당하는 surprisal이 정문보다 높게 나와 모델이 학습을 잘 하는 것처럼 보이지만 본 논문에 따르면 wh-GAP과 nwh-nGAP이 둘 다 정문이기 때문에 surprisal이 비슷하게 나와야 하지만 wh-gap의 surprisal이 더 높게 나타나고 있다. 본 논문에서는 이와 같은 점이 조금 아쉽지만 전반적으로 주어-동사 도치 의문문의 dependency를 어느정도 잘 처리하고 있다고 볼 수 있다고 해석하였다. !!!!!!!! wh-GAP과 nwh-nGAP의 surprisal이 비슷해야 한다고…? 글쎄… 이는 유표성과 관련된다. wh-GAP과 nwh-nGAP 중 nwh-nGAP이 더 unmarked form이다. 자연어에서는 wh-question보다 non wh-question이 더 많이 쓰이기 때문에 사용 빈도 측면에서도, syntactic complexity 측면에서도 nwh-nGAP의 surprisal이 낮은 것이 당연하다. 오히려 모델이 이를 잘 포착한 것으로 보인다. 별로 아쉬운 결과가 아니라고 생각한다. 6.4 Experiment 4 result: Filler-gap surprisal in uninverted indirect interrogatives'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Label smoothing이란, 일반적으로 label은 one-hot-vector로 표현되는데 label smoothing을 통해 정답은 1에 가까운 값으로, 오답은 0에 가까운 값으로 표현하도록 하는 것이다. 이는 label의 noise현상을 방지해주는 기술이다. 해당 작업이 필요한 이유는 자연어 번역 시 번역의 결과가 다양할 수 있기 때문이다. 예를 들어 ‘sleepy’라는 단어를 한국어로 ‘잠이 온다’와 ‘졸리다’로 번역이 가능한데 상이한 결과가 one-hot-vector로 표현될 경우 전혀 다른 one-hot-vector가 정답 값으로 동시에 존재하기 때문에 학습에 혼동을 줄 수 있다. 따라서 label smoothing을 통해 학습의 혼돈을 막고 효율성을 높이는 효과를 볼 수 있다. - Test 반면 test과정에서는 output이 나오면 그 output을 다시 입력하여 또 다른 output을 도출시킨다. 따라서 최종적으로 도출된 벡터는 문장 속에서 존재하는 단어벡터가 되어 문맥을 고려한 하나의 벡터가 된다. 즉 같은 단어여도 문맥에 따라 달리 embedding된다. Reference'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/',\n",
       "     'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "     'text': 'Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) Lee,Gichang.Sentence embeddings using korean corpora.seoul:acornpub,2019 P. Bojanowski, E. Grave, A. Joulin, T. Mikolov. Enriching Word Vectors with Subword Information,2017 × Search'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': 'Linzen and Leonard (2018)는 RNN과 인간이 범하는 오류의 패턴을 관계절과 전치사구의 attractor을 비교한 Bock and Cutting (1992)을 이용하여 상세히 분석하였다. 또한 이들은 주어가 단수일 경우와 복수일 경우의 수 일치도 비교하였는데 실험 참가자들이 주어의 head가 복수일 때보다 단수일 때 더 많은 attraction error를 냈다고 했다. 이러한 수의 불일치는 makedness(유표성)에 기인한다. 즉 복수명사는 단수 주어에 의해서 interrupt될 수 있는 privative feature(탈취자질)이다. 반면 단수 명사는 복수 주어와 encoding하는데 interrupt될 수 있는 자질을 가지지 않는다. 이러한 결과를 주목해봐야 하는 이유는 RNN에 유표성 개념을 주입하지 않았음에도 불구하고 인간과 비슷한 불일치를 보였다는 것이다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/',\n",
       "     'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "     'text': '이렇게 $L(\\\\theta)$를 최대화하는 학습 과정은 positive sample 단어쌍의 유사도는 높이고 negative sample 단어쌍의 유사도는 낮추는 방향으로 학습이 진행된다. 결국 FastText도 Skip-gram과 같이 center word와 주변 단어 쌍이 positive sample에 속하는지 negative sample에 속하는지 이진분류(binary classification)하는 과정에서 학습이 진행되고 학습을 통해 도출된 word embedding vector는 유사성에 관한 정보를 지니게 되는 것이다. Reference'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': '지금까지 딥러닝과 언어학이 상호 나눌 수 있는 이익에 대해 알아보았다. 언어학 분야에서는 언어 능력의 표준을 정의하는 것에 특화되어 있다. 이러한 competence를 통해 딥러닝 분야는 표준 규범이 지켜졌는지 테스트할 수 있고, 지켜지지 못한 경우에는 그의 원인을 해석해볼 수 있다. 그리고 딥러닝은 언어학 분야에 대해 언어습득 모델 구축과 문장 처리를 위한 유용한 플렛폼을 제공할 수 있다. Reference'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
       "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "     'text': 'Supervised Fine-tuning : labeled data로 모델 학습 Unsupervised Fine-tuning : un-labled data로 모델 학습 Transfer Learning : pre-trained model을 freeze 시키고 마지막 layer만 pre-trained model이 학습한 task와 동일한 task에 대해 다른 데이터로 학습시키는 것. (Fine-tuning은 pre-trained model을 가져와서 model parameter 전체를 작은 Learning Rate로 학습시킴) 2.2 Hyperparameter Learning Rate : 학습 시 weight update 속도를 결정하는 값으로, 매 iteration마다 weight가 얼마나 크게 조정될지를 결정하는 요소이다. 예를 들어 Learning Rate가 0.01이라면, 매 iteration마다 weight가 변화하는 양은 0.01배로 조정다. Learning Rate 값이 작을 때는 모델이 천천히 학습한다. 수렴하는 데 시간이 오래 걸리지만, 최적의 값을 찾을 가능성이 높다. Learning Rate 값이 클 때는 모델이 빠르게 학습한다. 하지만 너무 크면 최적의 값을 지나쳐 버리거나 학습이 불안정해질 수 있다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/',\n",
       "     'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "     'text': '1.1.2 Clustering Classifier가 훈련되고 나면 clustering단계에서 test sentence에 적용된다. clustering에는 두 가지 과정이 있다: closest-first clustering과 best-first clustering. - closest-first clustering closest-first clustering에서 classifier는 오른쪽에서 왼쪽으로, 즉 mention $i-1$에서 mention 1까지로 실행된다. 그리고 확률 0.5가 넘는 first antecedent는 $i$와 연결된다. 선행사가 확률 0.5를 넘지 못하는 경우에는 $i$에 대한 선행사가 선택되지 않는다. - best-first clustering best-first clustering에서 classifier는 모든 선행사들에 대해서 실행되는데, 가장 유력한 antecedent mention이 $i$의 선행사로 선택된다. 1.1.3 Problems mention-pair model은 단순하다는 장점이 있지만 두 가지 주요한 문제가 있다. 1) classifier가 직접적으로 후보 선행사들을 비교하지 못한다. 따라서 두 선행사 사이에서 실제로 더 나은 것이 무엇인지 결정하는 훈련이 이루어지지 못한다. 2) 담화 모델을 무시하며, entity가 아닌 단지 mention만을 살핀다. 각 classifier decistion은 동일한 entity의 다른 mention들을 고려하지 않고 한 쌍에 대해서만 완전히 국부적으로 이루어진다. 1.2 The Mention-Rank Architecture'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/',\n",
       "     'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "     'text': '$\\\\pi_\\\\theta$와 $\\\\pi_{ref}$에 대한 preferred response, rejected response에 대한 각 token별 예측 확률(정답 token을 예측할 확률)의 곱을 각각 구하여 $\\\\pi_\\\\theta$와 $\\\\pi_{ref}$ 간의 비율을 구한다. 이때 prompt에 해당하는 token들은 점수에 반영되지 않는다. 산출된 비율은 아래 수식에서 확인할 수 있듯 loss 산출에 사용되고, 이 loss는 gradient descent update과정에서 model weight를 update하는데에 사용된다. 수식에서 $\\\\beta$는 hyperparameter로, 논문 저자는 0.1로 지정하였고, $\\\\sigma$ sigmoid(logistic) 함수이다. Cross-entropy loss function used in DPO pipeline(https://medium.com/@pakhapoomsarapat/forget-rlhf-because-dpo-is-what-you-actually-need-f10ce82c9b95) DPO 논문에서는 classification loss를 활용하여 Reinforcement Learning(RL)를 대체하는 방법을 제안한다. loss는 모델이 어떠한 데이터에 대하여 잘 학습되고 있는지 측정하는 지표로, 모델은 loss를 최소화 하는 방향으로 학습을 진행한다. DPO data에는 preferred response, rejected response가 포함되어 있는데, 이 두 답변에 대한 점수를 구한 후 3. Odds Ration Preference Optimization (ORPO) ORPO: Monolithic Preference Optimization without Reference Model'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/RegularExpression/',\n",
       "     'source_title': 'Regular Expression;with code\\n      31 Oct 2019',\n",
       "     'text': '정규표현식 | compile | 영어 re 라이브러리의 compile을 사용하면 정규표현식을 사용할 때마다 정의하지 않고 한번 정의한 후에 불러오는 방식으로 쓸 수 있어서 빠르고 편리하다. \\'\\'\\' compile을 쓴 버전과 안 쓴 버전의 속도 차를 확인해보자 \\'\\'\\' import re import time # 시간을 체크하기 위해 time을 불러온다. text1 = \\'abc def abg\\' normal_start = time.time() # 시작 r1 = \\'ab.\\' for i in range(1001): # i를 1001번 돌린다. re.match(r1, text1) # text1이 ab어쩌고인지 확인해라 print(\"compile 안 쓴 버전: \", time.time() - normal_start) # 끝시간 - 시작시간 compile_start = time.time() # 시작 r2 = re.compile(\\'ab.\\') # compile for i in range(1001): # i를 1001번 돌린다. result = r2.match(text1) # re가 아니라 re.compile한 객체를 불러온다 print(\\'compile 쓴 버전: \\', time.time() - compile_start) compile 안 쓴 버전: 0.0010042190551757812 compile 쓴 버전: 0.0009706020355224609 정규표현식 | search | 영어 match는 문자열 처음에 해당 표현이 없으면 바로 끝나고 search는 문자열 전체를 검사해서 해당 표현에 매칭되는게 있는지 찾아낸다. r1 = re.compile(\\'ab.\\') print(\"a에 대해 search: \",r1.search(\\'a euosblkg abc\\')) print(\"a에 대해 match: \",r1.match(\\'a euosblkg abc\\')) print(\"abhey에 대해 search: \",r1.search(\\'jigdas abhey\\')) print(\"abhey에 대해 match: \",r1.match(\\'jigdas abhey\\'))'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/',\n",
       "     'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "     'text': '2.1 Naive RAG의 문제점 Naive RAG의 각 단계마다 문제점들이 존재한다. 1) Query 사용자의 질문이 명확하지 않은 경우, similarity search 과정에서 오류가 발생할 수 있다. 2) Indexing 2.1) Parsing: PDF와 같은 비정형 문서 내 이미지 및 표에 담긴 유용한 정보에 대한 추출이 불완전하다. 2.2) Chunking: 파일 특성을 고려하지 않고 일률적인 크기로 chunking함으로써 의미상 포함되어야 할 정보가 고정된 size 문제로 잘려 각 chunk에 불완전한 정보가 담겨있을 가능성이 있다. 2.3) Indexing: vector db의 indexing 구조가 파일 유형마다 최적화되어 있지 않아 retreival 과정에 부정적인 영향을 미칠 수 있다. 2.4) Embedding Model: 임베딩 모델의 semantic representation 성능이 좋지 않을 경우 retreival 과정에 부정적인 영향을 미칠 수 있다. 3) Retrieval 3.1) 검색된 chunk들과 질문의 관련성이 낮을 가능성이 있다. 3.2) 여러 검색 알고리즘을 종합적으로 사용할 수 없어 검색 기능이 제한적이다. 3.3) 검색된 chunk들에 유사한 정보가 중첩되는 경우, LLM이 생성 시 참고할 정보가 제한적이다. 4) Generation'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/',\n",
       "     'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "     'text': '실험 2에서도 결과가 좋지 않았다. 3.2 Experiment 3: Filler-gap surprisal in subject-inverted interrogatives 실험3은 주어가 도치된 의문문의 dependency에 대한 실험이다. 아래 예시 문장의 경우, talk about의 목적어가 what으로 대체되고 what이 절의 맨앞으로 이동한 것이다. 이때 what이 원래 있던 자리에 GAP이 생기는데 이 GAP과 what 사이에 dependency가 발생한다. GAP자리에 목적어 it을 삽입하면 talk about의 목적어가 두개(what과 it)가 되어버리기 때문에 비문이 된다. 따라서 실험3은 what과 GAP사이의 dependency를 모델이 잘 처리할 수 있는지 알아보는 것이다. 20개의 item 대해 WH-question이 있는 문장과 없는 문장 그리고 GAP이 있는 것과 없는 것 그리고 내포수준 1-4까지 해서 20x2x2x4, 총 320문장으로 실험을 진행한다. 그리고 GAP뒤에 나오는 item, 위 예시에서는 at뒤에 나오는 item의 softmax값(확률값)을 추출한다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/',\n",
       "     'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "     'text': '3. 앞서 얻은 Embedded vector들의 평균을 구하여 Hidden layer값을 구한다: \\\\begin{matrix} \\\\hat{v}=\\\\frac{v_{c-m}+v_{c-m+1}+\\\\dotsm+v_{c+m}}{2m}\\\\in\\\\mathbb{R}^n \\\\end{matrix} 4. Hidden layer값에 $W’$를 내적하여 score값을 구한다: \\\\begin{matrix} z=\\\\mathbf{W’}\\\\hat{v}\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} 5. 마지막으로 score값을 확률값으로 변환하기 위해 softmax를 적용시킨다: \\\\begin{matrix} \\\\hat{y}=softmax(z)\\\\in\\\\mathbb{R}^{|V|} \\\\end{matrix} Model Training 이제 $\\\\hat{y}$(예측값)과 $y$(정답값)이 일치하는 방향으로 학습을 진행한다. 이는 파라미터들($W$, $W’$)의 학습을 통해 가능한데 학습을 위한 목적함수(objective function)는 loss fuction으로 cross entropy를 사용하여 다음과 같이 정의된다: \\\\begin{matrix} H(\\\\hat{y},y)=-\\\\sum^{|V|}_ {j=1}y_j\\\\log(\\\\hat{y_j}) \\\\end{matrix} 위에서 언급했 듯이 $y$는 하나의 one-hot-vector이다. 위 목적함수에서 $y_j$를 발견할 수 있는데 이 또한 하나의 one-hot-vector이기 때문에 위 수식을 더 간단히 하면 다음과 같이 표현될 수 있다: \\\\begin{matrix} H(\\\\hat{y},y)=-y_i\\\\log(\\\\hat{y}_ {i}) \\\\end{matrix}'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
       "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "     'text': '3. Multimodel Transformers 최근 Transformer를 활용한 Multimodal 연구가 활발히 이루어지고 있다. 연구 결과, discriminative task와 generative tasks에서 모두 여러 modlaity에 대해 호환되는 것으로 확인되었다. 3.1 Multimodal Input 3.1.1 Tokenization and Embedding Processing 1) input tokenizing : 입력된 input의 modality에맞게 tokenizing한다. 2) representation embedding space 선택: tokenizing된 것들으 token의 modality에 따라 그에 맞는 representation embedding space를 선택한다. 예를 들어 아래 표와 같이 token의 modality가 RGB이면 token의 단위는 patch, embedding은 linear projection. token의 modality가 text이면 token 단위는 word, embedding은 learned embedding(Model parameter를 통해 embedding한 결과) 아래 표와 같이 다양한 modality에 대해 다양한 tokenizing 방식과 embedding space가 존재한다. Multimodal Learning with Transformers:A Survey 3.1.1 Token Embedding Fusion 각 modality별로 tokenizing-embedding을 모두 마친 이후 중요한 것 각 modality별 embedding을 어떻게 합쳐서 multimodal 처리를 가능하게 할 것인가이다. 가장 단순한 fusion은 여러 modality들에 대한 embedding들을 token-wise sum하는 것이다. transformer는 하나의 position에 대해 여러 token embedding을 반영할 수 있기 때문에 가능한 방법이다. 예를 들어 BERT의 경우 한 position embedding에 token embedding과 segment embedding을 position을 기준으로 원소값별로 더하여(element-wise sum하여) 모델에 입력한다. 이를 시각적으로 표현하면 아래 이미지와 같다. Multimodal Learning with Transformers:A Survey(edited by author)'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': '간단히 살펴보도록 하겠다:'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/',\n",
       "     'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "     'text': '이 그림에서 보이는 동그라미들을 Neuron이라고 부른다. 딥러닝에서는 그림과 같은 구조 통해 Neuron들이 input data(x)의 features를 찾아낸다. 그림을 예로 들자면 위와 같은 구조를 통해 기계는 고양이에 대한 정보를 고양이의 얼굴, 꼬리, 발, 털 등으로 분해하여 각각의 Neuron에 담아 고양이에 대한 정보를 학습한다. 이처럼 딥러닝은 raw data(e.g., sound, characters of words)인 input data로부터 좋은 features를 자동으로 추출하여 학습하는데 이를 다른 말로 Representation learning이라고도 한다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) × Search'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/',\n",
       "     'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "     'text': 'Full Fine-Tuning에는 데이터, 컴퓨터 자원, 시간 등 대규모 자원이 요구되는 문제가 있다. Large Language Model이 많이 나오며 이렇게 자원이 많이 요구되는 Full Fine-Tuning을 대체하며 LLM을 효율적으로 학습시킬 수 있는 방법론들이 활발이 연구되고 있다. 해당 연구는 주로 LLM tuning 이후의 성능을 더 좋게 하거나 모델을 보다 빠르게 훈련시킬 수 있도록 하는 방향으로 진행되고 있다. 이와 같은 연구 중 하나인 Parameter-Efficient Fine-Tuning (PEFT)에는 다양한 변gud들이 존재하는데 대부분 상대적으로 적은 dataset으로 model의 일부 parameter를 업데이트하거나 소수의 새로운 파라미터를 추가하는 방법들이다. PEFT의 장점은 크게 세 가지로 볼 수 있다: Reduced memory footprint PEFT은 model의 파라미터 중 제한된 일부 parameter만 업데이트하고 model의 원래 parameter는 유지시켜 학습 시 요구되는 메모리를 크게 줄일 수 있다. Faster training 훈련 할 parameter, 즉 업데이트되는 parameter의 수가 적기 때문에 자연스럽게 훈련 시간도 단축된다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/',\n",
       "     'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "     'text': 'ORPO는 SFT와 preference alignment algorithm를 single process로 결합하였다. ORPO가 두 단계를 결합할 수 있던 요인에는 목표 함수 수정에 있다. ORPO는 CLM의 목표 함수를 수정하여 negative log-likelihood(NLL) loss와 odds ratio (OR)를 결합하였다. 이것을 OR loss라고 부르는데 이는 rejected response에 대해서는 약한 패널티를 주고, preferred response에 대해서는 강한 보상을 주어 모델이 학습 과정에서 자연스럽게 인간의 선호도도 학습할 수 있도록 한다. 3.1 Preference Pairs ORPO 학습 시, data에는 Preference Pairs가 포함되어야 한다. Preference Pairs는 주어진 입력에 대한 선호 및 비선호 예시가 구체적으로 작성된 요소이다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/',\n",
       "     'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "     'text': '(LaTaX 수식 오류 수정 예정) in-context learning을 통해 LM을 학습시키는 것만으로 fine-tuning 없이 downstream task를 풀 수 있는 방법론이 GPT2를 통해 처음으로 제안되었다. 이 방법론을 기반으로 GPT3는 few-shot setting으로도 다양한 task에 대해 fine-tuning 방법론보다 성능이 좋을 수 있다는 것을 증명하였다. 하지만 GPT와 같이 큰 모델을 학습시킬 수 없기 때문에 GPT보다 상대적으로 작은 bert계열의 LM 뒤에 특정 task를 수행할 layer를 추가하여 fine-tuning하는 방식이 많이 사용되는데, fine-tuning 학습을 위한 labeled data를 확보하기 위해서는 많은 비용이 든다는 문제가 있다. 이에 따라 in-context learning을 활용하여 Pre-trained LM을 효과적으로 사용하는 다양한 연구들이 진행되었는데, 그 중 최근 많은 관심을 받고 있는 연구가 Prompt based learning이다. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference 1. Pattern-Exploiting Training(PET)'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
       "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "     'text': 'ELMo-based model은 contextualizer architecture만 다르기 때문에 제어된 비교에 용이하다. 따라서 여기에서는 layer들의 차이를 보는 것이 중요하기 때문에 ELMo-based model에 집중한다. ELMo-based model들은 수만개의 Word Benchmark로 훈련되었기 때문에 유연한 domain shift를 위해 softmax classifier만 비슷한 데이터로 재학습시킨다. 6.2 Results and Discussion'},\n",
       "    {'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/',\n",
       "     'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "     'text': \"Korquad 2.0 data 예시(1세트) → Q, A, context Korquad 2.0 data sample: {'version': 'KorQuAD_2.0_train', 'data': {'title': '예고범', 'url': 'https://ko.wikipedia.org/wiki/예고범', 'context': WIKIPEDIA CONTEXT 'qas': [{'answer': {'text': '나카무라 요시히로, 히라바야시 카츠토시, 사와다 메구미', 'html_answer_start': 21842, 'html_answer_text': '나카무라 요시히로, 히라바야시 카츠토시, 사와다 메구미', 'answer_start': 6302}, 'question': '드라마 예고범의 감독은 누구일까?', 'id': '8089'}]}} Korquad 1.0 Korquad 1.0 data to Dolly data: Q → instruction A → response context → context category → open_qa\"},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': 'Smith & Levy (2013)의 연구에 따르면 n-1에 기반하여 다음 단어를 예측하는 N-gram모델로도 reading behavior를 잘 모사했다고 한다. 2.2.2 Grammatical Language Model vs. RNN Language Model 사람은 Incremental Parsing과 같은 문법 기반의 언어모델을 통해 문장을 처리한다. 하지만 이런 Parser기반의 언어모델이 단순한 Sequential model보다 더 효율적이라는 것을 수학적으로 증명하기란 쉽지 않다. 증명이 어려워서 심지어 Frank & Bod(2011)에서는 기계가 문장을 이해하는데 있어 통사적 정보가 불필요하다는 주장이 나오기도 했다. 근데 이건 아닌 것 같다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/',\n",
       "     'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "     'text': 'Low-Rank Adaption (LoRA) QLoRA Low-Rank Adaption (LoRA) QLoRA는 4-bit Quantized language model into Low Rank Adapters의 줄임말로, 양자화를 거친 모델에 대해 LoRA를 활용해 tuning하는 기법을 뜻한다. 계산량 및 메모리 감축을 위해 QLoRA의 저자들은 크게 세 가지 기법을 사용했다. 4-bit NormalFloat Pretrained Model의 weight를 4-bit로 양자화시켰을 때 그 데이터 타입을 NormalFloat이라고 한다. 일반적으로 컴퓨터는 실수를 32-bit로 floating(fp32)하고 모델들은 보통 FP16으로 경량화되어 사용된다. Double Quantized 이 기법은 이중 양자화로, 양자화된 것을 또 양자화하는 것이다. 앞서 4-bit NormalFloat으로 양자화 된 weight를 또 양자화하는 것이 아니라 weight를 양자화하면서 발생한 quantization constant(양자화 상수)를 양자화하여 조금 더 가볍게 만드는 것이다. Paged Optimizer 이 기법은 모델 자체에 대한 경량화는 아니지만 제한된 컴퓨터 자원에서 모델을 사용할 때 GPU가 사용하는 VRAM page를 CPU의 RAM에도 일부 저장할 수 있게 해서 메모리를 최대한 사용핟로록 하도록 하는 방법이다. 이 기법의 적용으로 동일한 크기의 모델에 대해 OOM 발생을 피할 수 있는 같다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': '그리고 해당 모델 내부에서 Residual 연산이 계속되기 때문에 모델 전반에 흐르는 차원을 $d_{model}$(=512)로 고정하여 연산을 간편하게 하였다. Residual connection이후에는 layer normalization을 통해 학습의 효율을 높였다. 5. Position-wise Feed forward neural network(fully connected layer) 이 과정은 point wise로 진행되는 일반적인 feed forward network이다. 수식은 다음과 같다: \\\\begin{matrix} \\\\text{FFNN}(x)=\\\\text{MAX}(0, xW_1+b_1)W_2+b_2 \\\\end{matrix} 위 식을 보면 0보다 작으면 0을 내보내고 0보다 크면 weight bias 값을 내보낸다. 이 부분이 흡사 relu activation과 닮아 있는 것을 알 수 있다. 아래는 수식 이해를 돕기 위한 그림이다: Illustrated by the author'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
       "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "     'text': 'model의 전체 layer를 학습시키기 위해서는 많은 계산비용이 요구되는데 이는 곧 긴 훈련 시간과 큰 컴퓨터 자원 요구로 이어진다. Catastrophic forgetting문제 발생 가능성이 있다. 이는 모델이 모델이 새로운 task에 대해 학습하는 동안 pre-training 과정에서 배운 general knowledge를 잊어버리는 문제이다. 이는 잘 학습된 사전학습 모델을 무쓸모로 만든다. 모델의 전체 wight가 업데이트되기 때문에 사전학습 모델과 동일한 크기의 모델이 서버에 또 저장되기 때문에 용량 문제가 생길 수 있다. 위와 같은 문제를 완화하기 위해 Parameter-Efficient Fine-Tuning와 같은 대안들이 많이 연구되고 있다. 3.2 Parameter-Efficient Fine-Tuning (PEFT) Parameter-Efficient Finetuning (PEFT) methods 3.3 Representation Fine-tuning (ReFT) Pre-trained Model의 가중치는 frozen시키고, Model의 representation의 일부를 조작하여 downstream task를 해결하도록 하는 방법이다. ReFT의 종류 중 하나인 Low-rank Linear Subspace(LoReFT)는 PEFT보다 10배에서 50배 더 parmeter를 효율적으로 사용한다고 한다. 4. Fine-tuning variations : RLHF/PPO, DPO, ORPO Fine-tuning variations : RLHF/PPO, DPO, ORPO Reference'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
       "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "     'text': '위 표는 하나의 특정 task에 대한 각 모델의 layer들에 대한 PPL(perplexity, language model을 평가하는 척도로, 낮으면 낮을 수록 좋다)를 평가한 것을 나타낸다. ELMo original은 layer 2에서 PPL이 가장 낮고 ELMo 4-layer에서는 layer 4에서 가장 낮다. 이는 task specific한 정보는 layer를 쌓을수록 higher layer에 담긴다는 것을 나타낸다. 반면 transformer의 경우에는 monotonic increase pattern이 나타나지 않는다. 이러한 결과를 통해 LSTM과 Transformer에 대한 layerwise behavior의 차이도 알 수 있다. LSTM의 layer는 task specific하게 학습되는 반면 transformer을 그렇지 않다. RNN계열의 모델(ex. LSTM)과 Transformer계열의 모델(ex. BERT)의 syntactic knowledge encoding behavior pattern은 다르다. (본 논문에서는 이 차이가 가지는 의미에 대한 설명이 자세히 나오지 않는다.) RNN은 저차원의 syntax개념부터 고차원의 개념까지 monotonic increasing pattern을 보이며 encoding되고, Transformer는 non-monotonic behavior pattern을 띈다. monotonic increasing pattern은 computer vision쪽으로 생각하면 이해가 편하다. 위 그림처럼 input으로 조각들이 들어가고 hidden layer에서 처음에는 edge를, 그 다음에는 corner과 contour를 학습하고 마지막으로 object part를 학습하여 car, person, animal과 같은 object identity를 구분하는 것이 monotonic increasing pattern이다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Decoder는 self-attention, encoder-decoder attention 그리고 feed Forward, 이렇게 총 3개의 sub-layer로 구성되어 있다. Decoder의 self-attention은 현재 단어 이후에 나올 단어들에 attention이 적용되지 않도록 masking을 한 masked multi-head attention을 사용한다. 그리고 이후 seq2seq에서 사용된 attention과 유사한 encoder-decoder attention도 사용된다. Decoder도 encoder와 마찬가지로 N개의 layer가 사용될 수 있으며 해당 논문에서 encoder와 같이 6개의 layer를 사용하였다. 그리고 attention과 feedforward layer 또한 encoder처럼 여러 번 사용되었다. 1. Embedding Embedding부터 차례대로 살펴보겠다. 위에서 언급했 듯이 encoder에는 input이, decoder에는 right shifted된 output이 입력된다. Embedding은 일반적으로 자연어처리 과제 수행에 사용되는 Embedding Algorithm이 사용된다. 문장이 입력된 후에는 token별로 분리한 후 embedding과정을 거치는데 embedding시 중요한 것은 embedding vector의 차원이 $d_{model}$차원과 같아야 한다는 것이다. embedding vector의 차원이자 모델 내부에서 흐르는 행렬의 크기는 항상 ($\\\\text{seq_len}$, $d_{model}$)인데, $\\\\text{seq_len}$의 크기는 사용자가 설정할 수 있는 hyperparameter로, 일반적으로 train dataset에서 가장 긴 문장의 길이를 크기로 설정한다. 2. Positional Encoding'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': 'Linguistics as normative standard for neural network와 Modelling failable human language processing 접근법에는 큰 차이가 존재한다. 전자는 neural network가 언어학 이론으로 규범되는 규칙을 오류 없이 수행하는 것이 목표인 모델이고, 후자는 인간도 실수할 수 있는 언어처리를 모델링하는 것이 목표인 모델이다. 이 두 접근법은 각각 정해진 규범을 완벽하게 따르는 것을 목표로 하는 linguistic competence와 실제 발화 방식에 초점을 둔 linguistics performance에 기준점을 둔다는 차이를 보인다. 이는 곧 하나의 모델로 두 목표를 이룰 수 없음을 말한다. 즉, Competence model과 Performance model의 경로가 다르다는 것이다. 이 둘의 목표를 모두 이룰 수 있는 모델은 아직없다. 4. Conclusion'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Transformer는 이전 게시물에서 소개한 기존 Attention기법을 발전시킨 모델로 Attention Is All You Need(2017)에서 처음 소개되었다. Transformer network의 가장 큰 특징은 자연어 처리과제에 보편적으로 사용되는 RNN계열의 모델을 전혀 사용하지 않고 Attention만으로 작업을 수행한다는 것이다. 기존 기계번역에서 사용되던 RNN기반의 encoder-decoder는 RNN을 사용하기 때문에 순차적으로 연산이 진행되어 문장내 단어의 순서나 위치는 잘 반영하나 연쇄적인 연산으로 인해 첫 입력 값과 마지막 입력 값의 거리가 멀어질수록 초반에 입력된 정보에 대한 손실이 생기며 학습이 느리다는 문제가 있었다. 그리고 Transformer가 나오기 이전에 사용된 Attention mechanism은 기존 RNN based encoder-decoder에 Attention을 추가한 것이기 때문에 여전히 연산 속도는 느리다는 문제가 있었다. 반면 Transformer는 RNN을 완전히 제거하고 각 단어에 대한 중요 정보 인코딩을 행렬곱을 통해 한번에 연산함으로써 학습 속도를 올렸다. 또한 추가적으로 다양한 기술들을 적용하여 성능을 높여 큰 주목을 받았다. Architecture of Transformer'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
       "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "     'text': '(1) Uni-modal과 multimodal input 혹은 task에 대한 pipeline 통합하는 시도 실제 상황에서는 modality의 누락으로 인해 uni-modal data를 처리해야 할 수도 있기 때문에 uni-modal 그리고 multi modal 두 가지 pipeline을 통합하는 것도 매우 유의미한 연구이다. (2) Multimodal Understanding과 Generation을 위한 Pipeline을 통합하는 것이다. 자연어처리 측면에서 이해해 보자면 natural language understanding은 BERT와 같은 transformer encoder가 잘하는 task, 그리고 natural language generation은 GPT와 같은 transformer decoder가 잘하는 task이다. 이처럼 multimodal에서도 understanding과 generation을 둘 다 수행할 수 있도록 multi task learning을 시도하는 연구들이 있었다고 한다. (3) Task 자체를 통합하고 변형하는 시도 CLIP이 매우 대표적인 시도이다. CLIP은 Zero-shot Recognition을 Retrieval Task로 바꿔서 Model을 변경하지 않고도 해당 Task를 수행할 수 있도록 했다. 5.7. Interpretability 해석 가능성. 이는 multimodal transformer 뿐만 아니라 deeplearnign 모델들에 있어서도 공통적인 challange이다. transformer가 왜 그리고 어떻게 multimodal learning에서 좋은 성능을 보이는가에 대한 해석 가능성도 중요한 연구 방향임을 해당 연구진들은 밝혔다. Reference Multimodal Learning with Transformers:A Survey https://www.pinecone.io/learn/series/image-search/vision-transformers/ × Search'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/',\n",
       "     'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "     'text': '실내, 실외, 밈, 그림, 스케치 등을 포함한 24개의 이미지와 60개의 질문을 수집한다 각 이미지에는 highly-detailed, manually-curated description이 있다. Visual Instruction Tuning LLaVA, BLIP 그리고 OpenFlamingo에 대해 비교 실험을 진행한 결과, BLIP-2보다는 29%이상, OpenFlamingo보다는 48%이상 좋은 성능을 보였다. Visual Instruction Tuning 4.2 ScienceQA ScienceQA는 3 subjects, 26 topics, 127 categories, and 379 skills에 걸쳐 다양한 도메인을 포함한 21k(21,000개)의 multimodal multiple choice question을 포함한다. 해당 데이터셋은 trainset 12726개, validation set 4241개, 그리고 teset set 4241개로 나뉜다. GPT-3.5 model (text-davinci-002) with and without chainof-thought (CoT), LLaMA-Adapter, multimodal chain-of-thought (MM-CoT)로 실험한 결과는 아래 표와 같다. Visual Instruction Tuning LLaVa Model만 이용한 결과, SOTA(당시 91.68%)에 근접한 성능(90.92%)을 내는 것으로 확인되었다. LLaVa와 GPT-4를 함께 사용한 실험도 했다. (i) GPT-4 complement: GPT-4의 답이 틀렸을 때 LLaVA 담면 사용. 이때 90.97%의 성능 기록 (ii) GPT-4 as the judge: LLaVA와 GPT-4의 답변이 다른 경우, GPT-4에게 두 답변을 넣고 최종 결론을 요구하였고, 그 결과 SOTA(당시 92.53%)를 달성하였다. 4.2.1 Ablations ScienceQA에서 일부 선택사항을 제거하는 Ablation study도 진행했다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
       "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "     'text': '하지만 위 표는 특정 task에 맞춰서 사전학습한 것에 대한 결과인데 syntactic arc prediction의 경우에는 이처럼 특정 task에 맞춰서 학습한 결과가 더 높은 것을 확인할 수 있다. 이에 따라 어느정도 task specific한 contextualize를 하는 것도 고려해야 한다는 것을 알 수 있다. Reference Liu et al. “Linguistic Knowledge and Transferability of Contextual Representations,”Association for Computational Linguistics. 2019 https://arxiv.org/abs/1808.08079 https://dmitry.ai/t/topic/172 Deep Learning 2017 × Search'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
       "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "     'text': 'Batch Size : 모델이 학습할 때 한 번에 처리하는 데이터 샘플의 개수이다. Batch Size가 작으면 한 번에 적은 양의 데이터를 처리한다. 메모리 사용량은 적지만 느리게 학습될 가능성이 높다. Batch Size가 크면 한 번에 많은 데이터를 처리한다. 학습이 빠를 수 있지만 메모리 사용이 많다는 문제가 있다. Epochs : 모델이 전체 데이터셋을 보는 횟수이다. Warm-up Steps : 훈련 초기에 Learning Rate 점진적으로 증가시켜 안정성을 높이는 요소이다. 2.3 Regularization Techniques 모델 학습 시 overfitting(과적합) 방지를 위해 몇 가지 정규화 기법이 사용된다. Dropout : 학습 시 일 neuron을 랜덤하게 비활성화하는 기법. 이를 통해 특정 뉴런에 모델이 너무 의존하지 않도록 할 수 있고, 다양한 neuron 조합을 학습하게 되어 모델의 일반화 능력을 향상시킬 수 있다. 예를 들어 Dropout 비율이 0.5라면, 학습 중 각 학습 단계마다 neuron의 절반을 무작위로 비활성화한다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/30/NLP/',\n",
       "     'source_title': 'What is Natural Language Processing(NLP)?\\n      30 Oct 2019',\n",
       "     'text': 'NLP란 컴퓨터가 인간의 언어를 이해할 수 있게 처리하는 것이 NLP이다. 인간은 언어라는 도구로 구어와 문어를 구사한다. 컴퓨터는 인간이 둘 중 어떤 방식으로 언어를 전달하더라도 그의 의미를 이해할 수 있어야한다. 컴퓨터가 인간의 언어를 이해하기 위해서 우선 언어의 형태를 분석하고(형태소 분석) 그를 기반으로 각각의 의미를 분석한 후 전체 의미를 해석할 수 있어야한다. 예를 들어 누군가 완벽히 조립해 놓은 레고 모형이 있다고 생각해보자. 이 모형이 어떻게 만들어진 것인지 알기 위해 우선 레고를 하나하나 분해 한 후 각각의 레고 조각들이 어느 부위에서 어떻게 쓰였던 것인지 그의 역할에 대해 생각해 보고 다시 그 레고를 스스로 조립해보면 해당 모형에 대해 이해할 수 있을 것이다. 이와 같은 흐름으로 컴퓨터는 인간의 언어를 이해한다. NLP의 최종 목표는 다양한 task(질의 응답, 번역, 다양한 업무 수행, 예약하기, 물건구매 등)를 처리하기 위해 컴퓨터가 자연어를'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/',\n",
       "     'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "     'text': \"[[0.72033345 0.54783215 0. 0.42544054 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0. 0. 0.42544054 0. 0.54783215 0.72033345 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0.18177122 0.2390073 0.14116156 0.2390073 0.18177122 0. 0.2390073 0.2390073 0.2390073 0.2390073 0.47801461 0.2390073 0.47801461 0.2390073 ]] {'crime': 1, 'is': 3, 'common': 0, 'logic': 5, 'rare': 6, 'therefore': 12, 'it': 4, 'upon': 13, 'the': 11, 'rather': 7, 'than': 9, 'that': 10, 'you': 14, 'should': 8, 'dwell': 2} import pandas as pd tfidf_index = tfidf1.vocabulary_ tfidf_result = tfidf1.transform(doc1).toarray() tfidf_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.) # 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, # 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다. # 이렇게하면 index 순으로 정렬된다. for k, v in sorted(tfidf_index.items(), key= lambda i : i[1]): tfidf_list.append(k) # 리스트에 순서대로 담는다 # 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트 tfidf_df = DataFrame(tfidf_result, columns= tfidf_list) tfidf_df common crime dwell is it logic rare rather should than that the therefore upon you\"},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/',\n",
       "     'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "     'text': '- Task(a) Task(a)는 두 문장간의 상관관계를 파악하는 과제이다. 예를 들어 앞 문장과 뒷 문장의 흐름이 자연스러운지, 두 문장의 의미가 유사한지를 파악하는 것이다. 해당 task에 사용되는 dataset은 SWAG(The Situations With Adversarial Generations)와 GLUE dataset에 속하는 MNLI(Multi-Genre Natural Language Inference), QQP(Quora Question Pairs), QNLI(Question Natural Language Inference), STS-B(The Semantic Textual Similarity Benchmark), MRPC(Microsoft Research Paraphrase Corpus), RET(Recognizing Textual Entailment)이다. SWAG : 현재 문장 다음에 이어질 자연스러운 문장을 선택하기 위한 dataset MNLI : 현재 문장 다음에 이어지는 문장이 문맥상 이어지는 문장인지, 반대되는 문장인지, 상관 없는 문장인지 분류하기 위한 dataset QQP : 두 질문이 의미상 같은지 다른지 분류하기 위한 dataset QNLI : 질의응답 dataset STS-B : 두 문장의 유사성을 파악하기 위한 dataset MRPC : 뉴스의 내용과 사람이 만든 문장이 의미상 같은 문장인지 비교를 위한 dataset RET : MNLI와 유사하나 상대적으로 훨씬 적은 학습 dataset - Taks(b) Taks(b)는 input으로 하나의 문장을 입력하고 분류하는 것이다. 예를 들어 영화 리뷰 데이터를 입력한 후 해당 문장이 긍정인지 부정인지 혹은 중립인지를 판별하는 것이다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': '딥러닝에 사용할 만큼의 규모로 구축되지 않았다는 문제가 있다. 그렇다면 이제 또 다른 의문이 제기될 수 있다. 모델이 입력되는 대규모 코퍼스를 바로바로 parsing하면 되지 않느냐 물을 수 있다. 이러한 방법은 통사론적으로 매우 간단한 문장에서는 제대로 작동할 수 있지만 통사론적으로 조금만 복잡한 문장의 경우에는 parsing이 제대로 이루어지지 않는다. 하지만 우리는 통사론적으로 복잡한 문장을 딥러닝이 잘 분석하고 이해하기를 원하기 때문에 이와 같은 방식으로 도출된 결과는 딥러닝 모델에 사용할 수 없다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
       "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "     'text': 'Perplexity : 언어 모델의 예측 성능을 평가하는 지표로, 낮을수록 모델의 예측이 정확함을 의미한다. $PPL = e^{-\\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log p(w_i)}$ 언어 모델의 생성 품질이나 자연스러운 언어생성 평가는 어렵다. 3. Supervised Fine-Tuning (SFT) Pre-training 과정에서는 앞서 언급한 CLM과 같이 단순히 다음 token을 예측하는 task를 수행하며 학습이 진행된다. 하지만 보다 task/domain specific한 모델을 만들기 위해서는 supervised fine-tuning(SFT) 접근 방식 필요하다. SFT는 일반적으로 pre-trained Model에 대해 labeled data를 학습시키는 과정으로 진행된다. 예를 들어 CLM으로 학습된 모델로 conversation 혹은 instruction following 과제를 잘 수행하는 모델을 만들기 위해 이와 같은 target application을 잘 나타내는 dataset을 학습시킨다. 이렇게 학습한 모델은 특수한 요구사항 혹은 과제에 더 적합한 답변을 생성할 수 있다. 3.1 Full Fine-Tuning Pre-trained Model에 대해 Pre-trained Model이 학습한 데이터셋보다 작은 데이터로 모델 전체 layer를 다시 훈련 시키는 것이다. 즉, 모델 전체 parameter를 업데이트시키는 것이다. Full Fine-Tuning에는 몇 가지 문제점이 있다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': 'Linzen et al. “What Can Linguistics and Deep Learning Contribute to Each Other?,” Johns Hopkins University. 2019 × Search'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/',\n",
       "     'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "     'text': '위 그림에서 하얀 부분은 3차원 공간(embedding 공간)이고 회색 부분은 2차원이다. 본 연구에서는 3차원 공간에 있던 geometry(3차원 공간에 있는 것들을 표현한 점들과 파라미터)들이 2차원에 가서도 유지되는 선형변환 개념을 기반으로 해당 2차원 공간을 어떻게 찾을 것인가를 탐구한다. 따라서 본 논문의 probe는 선형변환(linear transformation) B를 찾는 것을 목표로 한다. The chef who ran to the store was out of food라는 문장에서 was와 chef는 6개의 단어를 끼고 있지만 syntactic residency측면에서 was와 chef의 distance는 1이고 was 와 가장 가까운 store는 distance 4가된다. 이처럼 vector space내에서 단어 간의 residency(distance)가 잘 측정되는 선형변환을 찾는 것이 본 논문의 목적이다. 즉, squared distance(squared L2 distance)와 tree distance간의 차이를 줄여나가는 방식으로 학습을 하여 적절한 선형변환을 찾고자 한다. 그리고 tree depth norm이 squared L2 vector norm으로 encoding되는 linear transformation이 존재할 것인데, 이를 찾는 문제도 linear transformation B를 찾는 문제로 치환을 하고자 한다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/12/15/PET/',\n",
       "     'source_title': 'Prompt-Based Learning 1 | PET(Pattern Exploiting Training), iPET(Iterative Pattern Exploiting Training)\\n      15 Dec 2022',\n",
       "     'text': '1.2 Auxiliary Language Modeling 이 방법론은 PVP에 대해 PLM을 finetune하는 것이라 볼 수 있다, 즉 MLM에 대해 추가 학습을 수행하는 것이라 할 수 있다. 그렇기 때문에 catastrophic forgetting이 발생할 수 있는데 저자는 language modeling을 auxiliary task로 활용함으로써 해결하였다. 이를 수식화 하면 아래와 같다. \\\\(\\\\text{L}_{ce}\\\\) 는 cross-entropy loss로, pattern exploit training에 대한 loss이고, $\\\\text{L}_{MLM}$은 기존 LM이 masked language modeling task에 대해 학습한 loss이다. 기존에 학습된 MLM을 추론에 바로 사용할 수도 있지만 추가적인 학습을 진행할 때에는auxiliary LM loss를 부가하여 사용하면 좋다고 한다. 저자는 실험을 통해 $\\\\alpha=\\\\text{10}^{-4}$일 때 좋은 성능을 보이는 것을 발견했다고 한다. 지금까지 설명한 개념들을 정리하면 아래와 같이 표현할 수 있다. 1.3 Combining PVPs 여러 개의 PVP 학습 결과를 combining하여 사용하는 것이 pattern exploit training이다. PVP를 활용한 접근에서 중요한 것은 어떤 PVP가 해당 task에 잘 맞는지 알아내는 건데, 이 문제를 해결하기 위해 본 논문에서는 knowledge distillation과 유사한 전략을 사용하였다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Train과정에서는 이전에 encoder와 decoder의 각 layer를 거칠 때 output으로 나온 것이 다음 단계에 다시 입력되지 않고 label이 또다시 입력된다. 그리고 최종적으로 나온 output들을 모두 concatenate한 후 벡터로 전환된 label과 비교하여 그들의 차이를 줄이는 방향으로 학습을 진행한다. 해당 논문에서 학습 시 사용된 optimizer는 adam이고, 정규화를 위해 residual과 attention에 대해 dropout(0.1)을 해주었다. 그리고 추가적으로 성능향상을 위해 label smoothing(0.1)작업도 수행하였다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/01/Word-Vectors/',\n",
       "     'source_title': 'How to represent words?( Word Vectors)\\n      01 Nov 2019',\n",
       "     'text': 'one-hot-encoding은 이렇게 단어 벡터가 굉장히 크며 sparse하고 단어 간의 유사도도 나타낼 수 없다는 단점을 지니고 있다. 따라서 지금까지 설명한 문제점들을 해결하기 위해 우선 차원의 크기를 ${ R }^{ v×M }$보다 작게 만들어야 하고 단어들 간의 연관성을 나타내는 subspace를 찾아서 encode하는 방식을 찾아야한다. 이후 게시물에서 차원문제와 유사도문제를 해결한 방안들을 소개하도록 하겠다. Reference Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra, Richard Socher.”CS224n: Natural Language Processing with Deep Learning: Part1,”(winter 2017) × Search'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': '기계가 사람처럼 문장을 처리하기 위해서는 사람의 reading behavior를 파악해야 한다. 인간의 독해력에는 뒷단어 예측 실력이 영향을 준다. 즉, 앞선 단어를 읽고 뒤에 나타날 단어를 예측할 수 있으면 빠른 속도 독해를 할 수 있고 단어를 건너뛰고도 문장의 내용을 파악할 수 있다. 이와 같은 예측 능력은 predictability estimate(예측 가능성의 수치)를 통해 모델링될 수 있다. 문장 내 단어를 예측한다는 것은 확률변수가 아닌 확률과정 개념으로 보는 것이 타당하다. 확률과정이 도입된 개념은 Markov Chain이라고 하는데, 이는 어떠한 상태가 이전 상태에 영향을 받는 것을 나타낸다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/12/04/Mistral/',\n",
       "     'source_title': 'Mistral 7B\\n      04 Dec 2023',\n",
       "     'text': '2. Sliding window to speed-up inference and reduce memory pressure attention 연산 횟수는 sequence length에 대해 이차적(quadratic)이고 memory pressure은 sequence length에 대해 선형적이다. 이로 인해 추론 시 cache 가용성이 감소하기 때문에 데이터 처리량은 줄고 시간은 더 지연된다. 이 문제를 완화하기 위해 Mistal은 각 token이 window size만큼의 과거 토큰에만 attention을 하는 Sliding Window Attention을 사용한다. 아래 그림은 window size가 3인 경우이다. 해당 방법론의 특이 사항은 sliding window 범위 밖에 있는 token도 next word prediction에 영향을 미친다는 점이다. 각 attention layer에서 정보는 최대 window size 만큼의 token만 전달될 수 있다. 따라서 2개의 attention layer를 거친 후에는 2*window size만큼의 정보가 전달될 수 있다. 예를 들어 sequence length가 16k이고 sliding window size가 4k일 때 4 layer를 거친 후에는 정보가 전체 sequence length로 전달된다. 이와 같이 해당 방법론이 sliding window 밖의 정보도 전달하지만 sequence 길이가 너무 길어지면 모델이 더 이상 full context를 전부 사용하진 않는 현상이 관찰되었다고 한다 3. Rolling buffer cache Mistral에는 rolling buffer cache가 적용되었다. cache는 W(window size)로 고정되고 (key, value)값을 cache position(i%W) 내의 position i에 저장한다. position i가 W보다 크면 과거 cache의 value들이 덮어씌워진다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/22/Bert/',\n",
       "     'source_title': 'BERT | Pre-training of Deep Bidirectional Transformers for Language Understanding\\n      22 Nov 2019',\n",
       "     'text': 'Pre-training procedure - Training options 본 논문에서 학습 시 사용한 corpus와 설정한 hyperparameter들은 다음과 같다: Corpus : BooksCorpus(800M words) + English Wikipedia(2500M words) Tokens : 37000WordPiece tokens Train batch size : 256 sequences (256sequences *512tokens = 128000 tokens/batch) Steps :1M Epoch : 40epochs Adam learning rate : 1e-4 Weight decay : 0.01 Drop out probability : 0.1 Activation function : GELU - Environmental setup Bert base : 4Cloud TPU(16TPU chips total) Bert large : 16Cloud TPU(64 TPU chips total) ≈72 P100 GPU Training time : 4days Fine-tuning pre-trained 모델에 classification layer를 추가하여 fine-tuning을 통해 특정 task를 수행한다. 이렇게 학습된 BERT의 parameter는 똑같이 사용고 task에 맞는 output layer만 추가하면 다양한 task를 하나의 모델을 통해 수행할 수 있다.이것이 BERT가 대부분의 NLP task에 대해 SotA를 달성 할 수 있었던 핵심으로 생각된다. fine-tuning시에는 input으로 목적에 맞는 labeled dataset을 사용해 fine-tuning하여 최종 모델을 만든다. 해당 논문에서 실험한 fine-tuning task들은 아래 그림에서 확인할 수 있다:'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
       "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "     'text': '최근 자연어처리 분야에서는 대용량의 corpus를 학습한 LM 뒤에 downstream task를 위한 layer를 붙여 특정 task를 풀 수 있도록 하는 방법론이 많이 사용된다. Language Model(LM)은 대용량의 unlabeled data를 학습하여 언어 자체에 대한 feature를 학습한 모델이다. 최근 LM의 학습에 사용되는 task는 크게 masked language modeling과 language modeling이 있다. 전자의 경우는 입력된 text의 일부 token에 대해 masking처리를 한 후, 앞뒤 문맥을 통해 masking token의 original token을 예측하는 task로, bert계열의 model들이 이러한 방식으로 학습된다. 후자는 token들을 하나씩 집어넣어서 첫 번째 token을 기반으로 다음 token을 예측하고 그 token을 가지고 다음 token을 예측하는 것을 sequence가 끝날 때까지 반복하는 방식으로 학습하는 task이다. 이 방법론은 gpt계열의 model에서 주로 사용된다. 그리고 bert와 gpt 계열 모델의 구조는 대부분 각각 transformer의 encoder와 decoder 기반으로 형성되어 있다. 따라서 bert 계열에서는 self-attention을, gpt 계열에서는 masked-self-attention을 사용한다. Unidirectional Language Model(Auto-regressive model)'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Illustrated by the author 여러 번 수행한 attention의 결과들(attention weight들)은 concatenate한 후 가중치 행렬 $W^{o}$와 내적하면 Multi-head attention의 최종 결과 값이 나온다. $W^{o}$도 다른 가중치들과 같이 모델과 함께 학습된다. \\\\begin{matrix} \\\\text{Attention}(Q, K, V) =\\\\text{Concat}({head}_ {1}, \\\\dots, {head}_ {h}){W}^{o} \\\\ where \\\\ {head}_ {i}=\\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\end{matrix} Illustrated by the author'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/',\n",
       "     'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "     'text': '위 식에서 $P(+|c_P,o_p)$는 FastText Model에서 positive sample인 center word($c$)가 output context word($o$)와 함께 쓰일 확률이다. 확률은 다음과 같이 정의될 수 있다: \\\\begin{matrix} P(+|c_P,o_p)=\\\\frac{ 1 }{ 1+exp\\u2061({ -u }_ { c }{ v }_ { o }) }=\\\\frac{ 1 }{ 1+exp\\u2061(-\\\\sum_{ {g} \\\\in { G_t } }z_g^{\\\\intercal}v_o)} \\\\end{matrix} 해당 모델의 목표는 예측된 positive sample인 center word와 context word쌍이 positive일 확률이 높이는 것이기 때문에 위 식의 결과를 최대화하는 방향으로 학습이 진행돼야 한다. 이를 위해 분모지수를 최소화해야 하는데 이는 벡터 $z$과 $v_o$의 내적 값을 높임으로써 가능하다. 벡터의 내적 값은 cosine similarity와 비례하기 때문에 내적 값이 커지면 벡터 간의 유사도 또한 높아지는 결과를 얻을 수 있다. 예를 들어 ‘백설공주’가 $c$이고 ‘먹었다’가 $o$일때 $z$는 “<백설”, “백설공”, “설공주”, “공주>”, “<백설공주>”이니, $z$와 $v_o$($o$에 해당하는 단어벡터)간의 유사도를 높이는 것이 해당 모델의 학습 목표라 생각하면 된다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/',\n",
       "     'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "     'text': 'llama_index -> FlagEmbeddingReranker(?) huggingface rerank model langchain_community -> JinaRerank langchain_cohere -> CohereRerank Application Interface FastAPI (graphic interface o) Flask Gradio (graphic interface o) Streamlit (graphic interface o) cherrypy Django Chainlit ... RAG 개발 예시 (인터넷 접속 가능 환경) 기능별 구성 요약 Illustrated by the author RAG Framework : Langchain Workflow control : LangGraph LLM : Mixtral-8x7b Inference accelerate : GROQ text embedding : Openai vector DB : Wevieate web search : Tavily Application Interface : Chainlit 단계별 설명 및 코드 Vector DB 준비 : Weaviate Illustrated by the author Weaviate setting weaviate docker setting weaviate docker-compose file: (https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker)[https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker] docker-compose -f wevieate.yml up -d -> weaviate client url 생성 create weaviate class client = weaviate.Client(\"weaviate client url\") class_obj = { \"class\": \"Test\", \"vectorizer\": \"none\", } client.schema.create_class(class_obj) prepare Documents for Retrieval crawling import requests from bs4 import BeautifulSoup soup1 = BeautifulSoup(html1, \\'html.parser\\') request1 = requests.get(\"https://finddme.github.io/\") html1 = request1.text links1 = soup1.select(\\'h4 > a\\') urls=[] tags=[\"llm\",\"dev\",\"natural\"] for link in links1: if link.has_attr(\\'href\\'): href=link.get(\\'href\\') for t in tags: if t in href.split(\"/\")[1]: urls.append(\"https://finddme.github.io\"+href) split / chunking docs = [WebBaseLoader(url).load() for url in urls] docs_list = [item for sublist in docs for item in sublist] text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=0 ) doc_splits = text_splitter.split_documents(docs_list) space_check=re.compile(\"\\\\s{1,}\") chunks=[] pass_first=[\"finddme \",\"⊹ Portfolio\",\"© 2024 viein_serenity_singularity_simplicity_savage. This work is liscensed under CC BY-NC 4.0.\"] for i in doc_splits: c={\\'text\\':i.page_content, \\'title\\':i.metadata[\"source\"].split(\"/\")[-2]} if c[\"text\"].split(\"\\\\n\")[0] in pass_first:pass else: save_c={\\'text\\':re.sub(space_check,\" \",c[\\'text\\']),\\'title\\':c[\\'title\\']} chunks.append(save_c) chunks save'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/04/lora/',\n",
       "     'source_title': 'PEFT : Parameter-Efficient Finetuning methods \\n      04 Oct 2023',\n",
       "     'text': '× Search'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2023/01/13/Ptuning2/',\n",
       "     'source_title': 'Prompt-Based Learning 3 | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\n      13 Jan 2023',\n",
       "     'text': '1. Introduction PLM은 광범위한 NLU task에 대해 성능을 향상시켰다. 널리 사용되는 방법인 fine-tuning은 PLM의 전체 parameter를 target task에 대해 update한다. fine-tuning은 좋은 성능을 내지만 모든 parameter에 대한 gradients와 optimizer state가 저장된다. 그리고 model의 parameter를 복사하여 가지고 있는 것은 PLM의 크기가 크기 때문에 매우 불편하다. 반면 Prompting은 PLM의 모든 parameter를 freeze시키고 prompt 형태의 query를 LM에 넣는 방식을 사용한다. 감성분석 task에 대해 예를 들어 보면 e.g. “Amazing movie!”라는 sample에 “This movie is [MASK]”라는 prompt를 붙여 PLM이 mask token이 “good”과 “bad” 중 어떤 token의 확률이 높은지 예측하도록 한다. Prompting은 따로 학습을 진행하지 않고 단순히 PLM을 copy하여 사용한다. 하지만 discrete prompting은 fine-tuning에 비해 suboptimal하다. Prompt tuning은 continuous prompt를 tuning하는 방식이다. Liu et al. (2021); Lester et al. (2021)은 original input sequence의 embedding(continuous prompt)에 대해 추가로 학습을 진행하는 것을 제안하였다. 이 방법론의 경우 학습 시 continuous prompt만 update된다. prompt tuning은 prompting이 다양한 task에서 좋은 성능을 내도록 했지만 model size가 충분히 크지 않을 때(10 billion 이하일 때)는 여전히 fine-tuning보다 성능이 좋지 않다. 특히 QA task와 같은 sequence labeling task에서는 fine-tuning보다 현저히 성능이 떨어진다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/',\n",
       "     'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "     'text': '236 혐오 서운하다 슬픔 울적하다 0.583 237 혐오 섭섭하다 혐오 서운하다 0.330 238 혐오 섭섭하다 혐오 속상하다 0.508 239 혐오 섭섭하다 혐오 괘씸하다 0.566 240 혐오 섭섭하다 분노 화나다 0.593 241 혐오 속상하다 혐오 서운하다 0.330 242 혐오 속상하다 슬픔 슬프다 0.320 243 혐오 속상하다 분노 화나다 0.482 244 혐오 속상하다 슬픔 우울하다 0.485 245 혐오 속상하다 슬픔 서럽다 0.487 246 혐오 속상하다 슬픔 울적하다 0.499 247 혐오 속상하다 미안함 죄송스럽다 0.528 248 혐오 속상하다 동정 안타깝다 0.534 249 혐오 속상하다 슬픔 억울하다 0.569 250 혐오 속상하다 수치심 비참하다 0.569 251 혐오 속상하다 슬픔 비통하다 0.581 252 혐오 속상하다 후회 아쉽다 0.586 253 혐오 싫다 혐오 짜증나다 0.543 254 혐오 싫다 혐오 지긋지긋하다 0.548 255 혐오 싫다 수치심 부끄럽다 0.530 256 혐오 싫다 수치심 창피하다 0.568 257 혐오 싫다 수치심 민망하다 0.581 258 혐오 싫다 고뇌 괴롭다 0.622'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
       "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "     'text': '일반적으로 multimodal transformer 모델은 input단에서 하는 Fusion인 Early Fusion, Intermediate Representation단에서 Middle Fusion, Prediction단(Transformer의 output)에서 Late Fusion 이렇게 세 Level에서 modality간의 정보를 Fusion한다. 이에 대해 해당 연구진들은 Future Work로서 Multimodal Transformer의 Fusion을 조금 더 향상시키고, 해석 가능하게 하기 위하여 Modality간의 interaction이 어떻게 진행되는지를 연구하고 Modality간의 Fusion을 정량적으로 측정하는 것을 연구하는 것도 의미 있을 것이라고 한다. 5.2. Alignment Modality 간의 Alignment는 실제 Multimodal 활용 측면에서 매우 중요하다. 다양한 downstream task에 대해 연구가 활발히 진행되고 있다. (예를 들어 Text-to-Speech Alignment, Text-to-Video Retrieval 등) Paired Sample에 대한 Contrastive Learning을 통해 두 modality를 공통된 REpresentation Space로 매핑하는 것이 대표적인 방법이다. 그러나 이를 위해 수백 수십억개의 학습 데이터가 필요하기 때문에 막대한 cost가 필요하다는 것이 문제라 이 문제를 해결하기 위한 연구도 multimodal 연구에서 중요한 과제 중 하나일 것이다. 5.3. Transferabililty'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/',\n",
       "     'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "     'text': '위 부분을 살펴보기 위해 나중에 계산될 span representations $g_i$에 대해서 잠시 얘기하도록 하겠다. span representations $g_i$는 스팬에서의 첫 번째 단어와 마지막 단어의 contextual representation 그리고 스팬에 있는 headword에 대한 representation 그리고 feature 하나로 구성된다. 여기에서 스팬의 첫 번째 단어와 마지막 단어의 contextual representation이 standard biLSTM으로 계산된다. 그리고 biLSTM은 ELMo와 같은 contextual word embedding을 기반으로 해서 각 단어에 대한 representation $w_t$를 input으로 받는다 (ELMo대신 BERT를 사용하면 성능이 훨씬 향상된다). 그러니까 biLSTM은 $w_t$를 input으로 받아서 output으로 $h_t$를 내는 것이다. 위 부분은 스팬의 head를 나타내기 위한 부분이다. 시스템은 스팬의 head를 나타내기 위해 스팬의 단어들에 대해 attention을 사용하였다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/',\n",
       "     'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "     'text': '$m(i)$: span $i$가 mention인지에 대한 여부; $m(j)$: span $j$가 mention인지에 대한 여부; $c(i, j): $j$가 $i$의 선행사인지에 대한 여부를 포함한다. dummy 선행사의 경우에, score $s(i, \\\\epsilon )$은 0으로 고정된다. 이 방법은 nondummy score가 양수면 모델이 가장 높은 점수의 선행사를 예측하지만 만약 모든 점수가 다 음수라면 제외하는 방법이다.'},\n",
       "    {'source': 'https://finddme.github.io/dev%20log/2022/09/25/NER/',\n",
       "     'source_title': 'NER(Named Entity Recognition)\\n      25 Sep 2022',\n",
       "     'text': '중복 문장 제거 중복 문장 추출 후 분리하여 저장 -> 모델 1차 학습 후 autolabeling을 통해 class tagging 데이터 라벨 불균형 완화를 위해 수량이 적은 라벨 데이터 제외(1500건 이하 데이터 제외) BIO-tagging NER 학습 시 필요한 BIO tag 부착. BIO-tag: token classification task와 같이 다수의 token이 하나의 개체임을 표시하기 위해 일반적으로 사용되는 tag B(begin tag): 개체의 시작과 개체명 정보를 포함 I(inside tag): 개체 시작 이후 끝까지 개체 내부를 표시 O(outside tag): 개체외부 표 2.3 Data class [사람, 수, 음식, 직업/직책, 제작물/작품, 조직/기관, 시설물/건물, 국가명, 행정구역, 학문/이론, IT 용어, 제품, 동물 신체, 날짜, 질병/증상/증세, 모양 및 형태 표현, 세포/조직/기관, 동물 종류, 음료, 교통수단, 기간, 패션의류/잡화, 자연, 천체명칭, 기후/날씨, 식물, 스포츠, 계절, 언어, 문화/문명, 사건, 화폐(통화), 약] 3. Model 3.1 Model Architecture BERT Electra'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/',\n",
       "     'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "     'text': 'TIES는 기존의 병합법들보다 뛰어난 성능을 보였고, 특히 부호 간섭 문제를 효과적으로 해결하여 모델의 전반적인 성능을 향상시킨다고 알려져 있다. models: - model: mistralai/Mistral-7B-v0.1 # no parameters necessary for base model - model: OpenPipe/mistral-ft-optimized-1218 parameters: density: 0.5 weight: 0.5 - model: mlabonne/NeuralHermes-2.5-Mistral-7B parameters: density: 0.5 weight: 0.3 merge_method: ties base_model: mistralai/Mistral-7B-v0.1 parameters: normalize: true dtype: float16 위 구성은 mistralai/Mistral-7B-v0.1와 OpenPipe/mistral-ft-optimized-1218를 TIES기법으로 병합하는 예시이다. mistral-ft-optimized-1218(50%) + NeuralHermes-2.5-Mistral-7B(30%) weight의 합이 100이 되지 않지만 나머지 20은 normalize: true 를 통해 parameter를 내부적으로 자동 정규화시킨다. density는 두 모델 모두 parameter 중 50%만 유지. 나머지 50%는 base_model에서 가져옴. 4. DARE DARE는 주로 유사한 모델들 간의 병합에 많이 사용된다. DARE는 TIES와 접근 방식은 유사하지만 두 가지 큰 차이점이 있다. Pruning(가지치기) : fine-tuning된 weight를 무작위로 base model 값으로 재설정한다. Rescaling : model의 output의 기대값을 대체로 동일하게 유지하기 위해 weight를 rescaling한다. 둘 이상의 병합 모델의 rescaling된 weight를 base model에 scale factor와 함께 추가한다. DARE는 아래와 같이 구현된다:'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/',\n",
       "     'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "     'text': 'Multimodal instruction-following data 구축 vision-language instruction-following data의 수량이 대규모 모델을 학습하기에 부족한 것이 본 연구의 큰 문제였다. 본 논문에서는 이 문제를 ChatGPT/GPT-4를 사용하여 image-text pair data를 instruction-following format으로 변환하는 pipeline을 구축함으로써 해결하였다. 해당 데이터셋을 통해 모델이 이미지에 대해 더 정교하게 이해함과 동시에 user instruction을 더 잘 수행할 수 있게 되었다. Large multimodal model 개발 본 논문에서는 visual encoder로 CLIP을, language decoder로 Vicuna(LLaMa를 instruction tuning한 모델) 선정한 후 이를 연결하여 Large multimodal model을 개발하였고, 이 모델에 instructional vision-language data을 학습시켰다. Multimodal instruction-following benchmark 생성(LLaVA-Bench) 본 논문에서는 두 가지 Multimodal instruction-following benchmark를 생성하여 공개하였다. Open-source multimodal instruction data, codebase, model checkpoints, visual chat demo를 모두 공개한다. Instruction Tuning'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/20/merge/',\n",
       "     'source_title': 'Merge algorithms\\n      20 Jun 2024',\n",
       "     'text': '장점 Smooth Transitions : 부드러운 parameter 전환. 고차원 vector interpolate 시 부드럽게 parameter를 전환시킨다. Preservation of Characteristics : SLERP은 모델의 특성을 잘 보존하며 결합이 진행된다. 즉, 결합하는 모델들의 고유한 특성과 곡률을 유지한다. Nuanced Blending : 세밀한 혼합이 가능하다. vector space의 여러 속성들을 고려하여 두 모델의 특성을 세밀하게 반영하여 혼합한다. 단점 한번에 두 모델만 결합할 수 있다. EmbeddedLLM/Mistral-7B-Merge-14-v0.1에서 시도된 것처럼 계층적으로 둘 이상의 모델을 결합할 수도 있다. 해당 모델의 merge.yml은 https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1/blob/main/merge.yml에서 확인 가능하다. .yml 파일에서 $t$는 보간 계수이다. 3. TIES Task Vector Arithmetic과 SLERP는 비교적 오래된 모델 병합 기법인 반면 TIES는 LLM이 유행하며 새롭게 등장한 기법이다. 기존의 방법들은 각 model parameter간의 간섭을 처리하는데에 문제가 발생하곤 하는데 이 간섭은 모델 병합 이후 모델 성능 저하의 요인이 된다. 간섭의 종류에는 아래와 같이 두 가지가 있다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/',\n",
       "     'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "     'text': '입력된 질문과 retrieved chunk를 LLM에 함께 입력하여 답변을 생성한다. 아래와 같이 user_message(사용자 질문)과 context(retrieval 결과)를 함께 LLM에 입력하며, context를 참고하여 user_message에 대한 답변을 생성하라는 prompt를 포함시켜 LLM에 입력한다. prompt=f\"\"\" <|im_start|>system You are a helpful assistant chatbot. Write a response that appropriately completes the request, referring to given Context. When generating responses, it is crucial to adhere to the following conditions. - Do not generate new question. You must respond only to the given question. - You should never repeat the same sentence. - Do not repeat the questions in your response. - You must answer in Korean Language. Do not use any other languages except Korean. Here is context to help: context: {context}<|im_end|> <|im_start|>user {user_message}<|im_end|> <|im_start|>assistant \"\"\" 2. Naive RAG Naive RAG(standard RAG)은 가장 기본적인 RAG이다. Illustrated by the author'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/',\n",
       "     'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "     'text': '그림 22.3은 single candidate anaphor she에 대한 계산 예를 보여준다. mention-ranking system은 후보 anaphoric mention she에 대해 이전의 모든 mention들, 그리고 추가적으로 특수한 dummy mention $\\\\epsilon$에 대해 소프트맥스를 계산하여 확률분포를 할당한다(각 후보 선행사들에 대한 확률을 구한다). 이전의 mention-pair model의 훈련은 단순했는데, mention-ranking model의 훈련은 비교적 까다롭다. 왜냐하면 mention-pair model에서는 positive-sample / negative-sample을 만들어서 훈련에 사용되는 sample pair를 다 알았는데, mention-ranking model은 각 anaphor에 대해서 훈련을 위해 사용할 수 있는 모든 gold antecedent들을 알 수 없기 때문이다. 대신 각 mention에 대한 best antecedent는 잠재되어 있다; 즉 각 mention에 대해서 선택할 수 있는 legal gold antecednet의 전체 클러스터를 가지고 있다. 이런 잠재된 선행사를 모델링하는 다양한 방법들이 존재하는데, 이는 3장에서 상세히 다룰 것이다. Mention-ranking model 은 hand-build feature와 neural representation learning 둘 다를 통해 구현될 수 있느데, 전자는 2장에서, 후자는 3장에서 자세히 다룰 것이다. 1.3 Entity-based Models 앞서 소개한 mention-pair model과 mention-ranking model 둘 다 mention-based algorithm이었고, 이번에 소개할 모델은 entity-based model이다. - entity-based model'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Multi-head attention의 경우 여러 layer에 대해 행렬 연산들이 모두 병렬적으로 동시에 수행되는데 이러한 병렬화는 하나의 문장을 여러 관점에서 바라볼 수 있다는 장점을 지닌다. 자연어 문장에는 공지시관계의 단어들이 빈번히 사용되며, 모호성과 중의성 또한 존재하는데 이러한 자연어의 특징을 기계가 이해하는 데에 병렬화가 중요한 역할을 한 것이다. 병렬적 연산이 어떤 결과를 주는지 예시와 함께 살펴보면 도움이 될 것이다. 예를 들어 “The animal didn’t cross the street because it was too tired.”라는 문장에서 “it”이 지시하는 것이 무엇인지 쉽게 알아낼 수 있지만 기계의 경우에는 그렇지 않다. 따라서 기계는 해당 문장을 다각에서 바라본 후 확률적으로 “it”과 가장 연관성이 가장 높은 단어를 알아내는 과정을 거쳐야한다. 다음 그림은 해당 과정을 시각화한 것이다:'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/',\n",
       "     'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "     'text': '1. Introduction 많은 논문들에서 BERT가 어떻게 언어를 이해하는지에 대해 다루는데 본 논문에서는 BERT가 통사적 지식을 어떻게 가지는지를 평가해보고자 한다. BERT의 통사적 지식은 한가지 방법으로만 평가될 수는 없다. 따라서 크게 두 가지 방법을 통해 실험을 진행한다. 1) negative polarity item(NPI) : NPI는 부정적인 언어환경에서 나오는 단어이다. 2) five approaches: Boolean acceptability : 문장의 참과 거짓을 판단. Absolute Minimal Pair : 두 개의 문장이 있을 때 그 문장들이 단어나 미묘하게 다른 표현들을 포착할 수 있는지 실험 Gradient Minimal Pair : Absolute Minimal Pair에 대해 확률적으로 rough하게 봐서 일정 수준을 넘어가면 참. Cloze Test : 특정 단어를 가리고 테스트. Feature Probing : NPI, licensor, scope을 기준으로 문장을 학습하고 inference하는 과정'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': 'Illustrated by the author 위 그림은 단어 벡터 하나에 대한 $Q$, $K$, $V$벡터를 구하는 과정을 표현한 것이다. 하지만 이전에도 언급했 듯이 문장은 단어벡터들을 합친 행렬이기 때문에 실제 연산은 다음과 같이 행렬곱으로 진행된다: \\\\begin{matrix} xW_i^Q=q_i\\\\text{with dimensions} \\\\ \\\\text{seq_len}\\\\times d_q \\\\end{matrix} \\\\begin{matrix} xW_i^K=K_i\\\\text{with dimensions} \\\\ \\\\text{seq_len}\\\\times d_k \\\\end{matrix} \\\\begin{matrix} xW_i^V=V_i\\\\text{with dimensions} \\\\ \\\\text{seq_len}\\\\times d_v \\\\end{matrix} Illustrated by the author'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': '지금까지 기술한 내용을 요약하자면, 언어학이 딥러닝의 기준점이 될 수 있다는 것이다. 1.2 Psycholinguistics and Experimental Control'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/',\n",
       "     'source_title': 'RAG(Retrieval-Augmented Generation)\\n      21 Feb 2024',\n",
       "     'text': '하지만 이 방법은 두 sub query에 대해 검색된 chunk가 상이하여 서로 다른 문서로부터 답을 찾을 가능성이 있다는 문제가 있다. 7) Rewrite-Retrieve-Read Query Rewriting for Retrieval-Augmented Large Language Models 논문에서 제안된 방법으로, 사용자의 질문이 LLM 검색에 최적화되지 않은 경우가 많아 LLM을 통해 질문을 재 작성하는 방법을 소개한다. 8) Step-Back Prompting prompting을 통해 LLM이 질문의 핵심을 파악하도록하는 것이다. 질문이 너무 구체적인 경우, LLM은 특정 세부 사항에 꽂혀서 제대로된 답을 하지 못한다. 예를 들어 “2023년 4월부터 2024 5월까지 나온 LLM 관련 논문은 몇 개야?”라는 질문은 너무 세세하게 명시된 기간으로 인해 LLM이 적절한 답을 반환하지 못할 가능성이 높다. 이 예시의 경우, “LLM 관련 논문 수를 연도별로 알려줘”라는 query로 묻는 것이 더 효과적이다. Step-Back Prompting은 두 단계를 통해 진행된다. 1. 추상화 : LLM이 query에 대한 high-level concept 혹은 기본 개념에 관한 광범위한 질문을 만들도록 prompting한다. 2. 추론 : 추상화된 질문에 대한 답을 추론한다. 이를 abstract reasoning이라고 부른다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/',\n",
       "     'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "     'text': 'TF-IDF(Term Frequency - Inverse Document Frequency) 문서에 등장한 단어를 단순히 count하는 방법이 아니라 다른 문서에 비해 더 많이 나온 단어를 알아보기 위한 방법이다. 다른 문서보다 특정 문서에서 더 많이 등장한 단어가 해당 문서의 핵심어일 가능성이 높다는 가정 하에 만들어진 방법이다. TF-IDF는 Term Frequency * Inverse Document Frequency로 계산된다. Term Frequency (앞서 다룬 DTM이랑 같다. 특정 문서에 나온 단어들의 빈도수), Inverse Document Frequency({전체문서 수}/{특정 단어가 들어있는 문서의 개수 + 1}에 로그를 씌워준 것) scikit-learn에서 tfidfvectorizer를 사용하여 구현할 수 있다. from sklearn.feature_extraction.text import TfidfVectorizer tfidf1 = TfidfVectorizer().fit(doc1) # doc1에 대해 tf-idf를 fit시킨다. print(tfidf1.transform(doc1).toarray()) print(tfidf1.vocabulary_)'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/29/vlm_architecture/',\n",
       "     'source_title': 'VLM : Architecture / Fusion Methods\\n      29 May 2024',\n",
       "     'text': '3.1 Contrastive Learning Contrastive Learning(대조 학습)은 입력된 image와 text를 동일한 vector space에 놓고 쌍을 이룬 image-text의 각 embedding 거리를 최소화 하면서 맞춰가는 방법이다.(맞지 않는 쌍 간의 거리는 최대화 시키다.) 이때 거리 최소화 작업에 사용되는 vector간 거리 산출 공식은 보통 cosine distance formula이다. OpenAI도 CLIP(Contrastive Language-Image Pre-training) 모델에서 cosine distance formula를 사용하였다. https://towardsdatascience.com/clip-intuitively-and-exhaustively-explained-1d02c07dbf40 3.2 PrefixLM PrefixLM은 Transformer의 Encoder와 Decoder가 결합된 모델로, prefix token set이 주어졌을 때 다음 token을 예측하는 모델이다. PrefixLM은 visual embedding과 text embedding을 병렬적으로 처리한다. 아래 그림과 같이 Transformer의 Encoder가 concat된 image와 text emebedding(-> prefix emebedding)을 입력 받고 Decoder 그 뒤에 이어질 token을 예측한다. 예측 token은 text이다. SIMVLM: SIMPLE VISUAL LANGUAGE MODEL PRETRAINING WITH WEAK SUPERVISION 3.3 Multi-modal Fusing with Cross Attention Multi-modal Fusing with Cross Attention은 cross-attention mechanism을 통해 시각적 정보를 language model에 통합시키는 방법이다. 아래 이미지와 같이 visual encoder를 통해 image를 embedding한 후 이 embedding 결과를 language model의 cross-attention layer에 입력한다. 이 방법을 사용한 대표적인 모델은 VisualGPT이다. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/',\n",
       "     'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "     'text': '위 표에서 npl(빨간 박스)은 복수명사, nsg(초록 박스)는 단수명사를 나타낸다. 그리고 표의 x축에는 vpl(복수 동사)와 vsg(단수동사)가 나와있고 위에 제시된 숫자는 삽입절 내포 수준을 나타낸다. RNN이 제대로 학습했다면 명사가 복수일 때 동사가 복수여야 하기 때문에 초록색 박스보다 빨간색 박스의 수치가 낮게 나타나야 한다. 하지만 표를 보면 두 박스가 나타내는 수치의 차이가 거의 없다. google model에서는 삽입절이 많을수록 surprisal값이 높아지는 것을 볼 수 있다(예측을 잘 하지 못한다). 2.3.2 Experiment 2 result: agreement in indirect interrogatives'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
       "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "     'text': 'Modality들의 token embedding sequence concatenate하여 Transformer layer에 입력하는 방법이다. 이 방법은 modality들의 token position이 하나의 sequence로 처리되어 각 modality의 context를 반영할 수 있다는 장점을 가진다. VideoBERT는 multimodal Transformer의 초기 연구 중 하나로, 이와 같은 방식을 통해 video와 text를 결합하여 global multimodal context를 잘 encoding한 모델이다. 이 방법은 concatenation으로 길어진 Sequence로 인해 계산 복잡도가 증가된다는 단점을 가진다. 이는 Attention 자체의 한계점으로, sequence가 길어질수록 계산 복잡도가 높아진다. Multimodal Learning with Transformers:A Survey 3.2.3 Hierarchical attention (multi-stream to one-stream) Modality에 따라 각각 독립적인 Transformer를 통해 input을 encoding한 이후 각 Transformer stream들의 output embedding을 concat하여 하나의 또다른 Transformer에 입력함으로써 Embedding들을 융합시키는 방식이다. 이러한 방식은 late interaction/fusion로 분류될 수도 있고 early concatenation의 특수 케이스로 분류될 수도 있다. Multimodal Learning with Transformers:A Survey 3.2.4 Hierarchical attention (one-stream to multi-stream)'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/10/31/NLPBasics/',\n",
       "     'source_title': 'NLP Basics;with code\\n      31 Oct 2019',\n",
       "     'text': '# 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, # 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다. # 이렇게하면 index 순으로 정렬된다. for k, v in sorted(bos_index.items(), key= lambda i : i[1]): bos_list.append(k) # 리스트에 순서대로 담는다 # 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트 bos_df = DataFrame(bos_result, columns= bos_list) bos_df better betty bit bitter bought butter make 0 2 1 1 1 1 2 1 BOS | 한국어 kor_bos_sentence = [\\'내가 어제 호떡을 먹고 싶어서 사 온 건데 나를 빼고 다 먹었더라고\\'] count_words1 = CountVectorizer() kor_bow = count_words1.fit_transform(kor_bos_sentence) print(kor_bow.toarray()) print(count_words1.vocabulary_) # 한국어는 띄어쓰기를 기준으로 나뉜 벡터를 세면 부정확하다. [[1 1 1 1 1 1 1 1 1]] {\\'내가\\': 2, \\'어제\\': 7, \\'호떡을\\': 8, \\'먹고\\': 3, \\'싶어서\\': 6, \\'건데\\': 0, \\'나를\\': 1, \\'빼고\\': 5, \\'먹었더라고\\': 4} from konlpy.tag import Mecab import re import MeCab kor_bos_sentence2 = \\'내가 어제 호떡을 먹고 싶어서 사 온 건데 나를 빼고 다 먹었더라고.\\' kor_tagger = Mecab(dicpath=r\"C:\\\\mecab\\\\mecab-ko-dic\") # 온점은 없애고 형태소 분석한 결과를 kor_tokens에 넣는다 kor_tokens = kor_tagger.morphs(re.sub(\\'(\\\\.)\\', \"\", kor_bos_sentence2)) print(kor_tokens)'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/',\n",
       "     'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "     'text': '3) 특히 한국어 같은 경우 생략 현상과 강세 변화에 따른 의미 변화가 다른 언어에 비해 더 많이 나타난다. 1.2 History 이어서 기계번역의 역사에 대해서도 짚어보고 가겠다. 1) RBMT(Rule-Based Machine Translation) : 전통적인 번역 방식인 규칙 기반 기계번역이다. 말 그대로 형태, 통사, 의미 정보가 포함된 광범위한 어휘소와 대규모 규칙을 필요로 하는 번역 방식이다. 해당 규칙은 사람이 만들어야 하기 때문에 많은 자원과 시간이 소모된다. 또한 언어는 매일매일 지속적으로 변화한하기에 번역 언어쌍 확장은 필수적으로 이루어져야 하는데 이때 번역 품질 향상을 위해 전문가가 매번 새로운 규칙을 찾아내 적용해야한다. 따라서 지속적이 투자가 필요하기 때문에 실제 산업에서는 사용률이 저조했다. 2) SMT(Statistical Machine Translation) : 통계 기반 기계번역으로 대량의 양방향 코퍼스에서 통계를 얻어내 번역 시스템을 구성한다. 언어쌍 업로드가 RBMT에 비해 비교적 빠르고 간단하다. 3) NMT(Neural Machine Translation) :'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/03/07/John2019/',\n",
       "     'source_title': 'A Structural Probe for Finding Syntax in Word Representations(Hewitt.J(2019))\\n      07 Mar 2021',\n",
       "     'text': '4.2 Tree depth evaluation metrics tree의 depth를 재구성하는 능력은 tree depth evaluation metric을 활용하여 평가한다. 이것도 distance evaluation과 마찬가지로 실제 depth ordering과 predicted ordering 사이의 Spearman correlation확인한다. 우선 동일한 길이의 문장에 대해 평균을 내고 5050길이의 문장에 대해 평균을 낸다. 이를 “norm Spearman(NSpr.)이라고 부른다. 그리고 모델이 root를 판별할 수 있는지를 확인하기 위해 root%라는 것을 함께 활용하여 평가를 진행한다. 5. Result 위 표에서 Linear, ELMo0, DECAY0, PROJ0은 baseline이고 ELMo1, BERTbase7, BERTlarge15, BERTlarge16은 실제로 embedding이 syntactic tree정보를 제고해 낼 수 있을 만큼의 통사적 정보를 지니는지 확인하기 위해 사용된 모델이다. UUAS기준으로 BERTlarge15가 82.5로 가장 높은 점수르 보였고 root%의 경우에는 BERTlarge16이 가장 높은 점수를 보였다. 5.1 Reconstructed trees and depths'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/',\n",
       "     'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "     'text': 'NLP Basics 실습 코드: https://github.com/finddme/Finddme_Blog_Code/blob/master/NLP_Code/Sentimental_Categorize1.ipynb 홍종선(2009)에서는 의미적 차원과 표현 구조를 검토하여 감정동사들을 총19유형(감동, 감사, 기쁨, 고뇌, 고독, 놀람, 동정, 두려움, 미안함, 분노, 상쾌함,수용, 수치심, 슬픔, 욕구, 이완, 허무, 혐오, 후회)으로 분류했다. 표현 구조를고려하여 분류된 감정동사들의 벡터 값이 유사할 것이라는 가정 하에 Word2Vec을 활용하여 이를 검토해보고자 한다. 데이터는 AI 허브에서 제공하는 감성분석 데이터에서 content만 따로 추출하여 데이터로 사용한다. import codecs from bs4 import BeautifulSoup from konlpy.tag import Okt from gensim.models import word2vec text = codecs.open(\"sent.content.txt\", \"r\", encoding = \"utf-8\") f = open(\"sent.content.txt\", \"r\", encoding=\"utf-8\") print(f.read(20)) 곧 만기 은퇴 앞두다 있다 노후 준비 word_dict = {} lines = f.read().split(\"\\\\n\") print(lines[10]) 친구 내 자다 되다 건 다 너 덕분 고맙다 문자 하다 통 넣다 morph_analyzer = Okt() morph_analyzer1 = Okt() for line in lines: morph_analysed = morph_analyzer.pos(line) for word in morph_analysed: if word[1] == \"Noun\": if not (word[0] in word_dict): word_dict[word[0]] = 0 word_dict[word[0]] += 1'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/',\n",
       "     'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "     'text': 'Word2vec모델은 출력층으로부터 나온 score vector를 softmax에 통과시켜 확률 값으로 만든 후 이를 각 위치의 정답 값과 비교하며 역전파(backpropagation)하는 구조를 지닌다. 해당 과정에서 softmax regression적용을 위해 center word와 문서 내의 다른 모든 단어들을 모두 내적한 후 다시 $exp$를 취하게 되는데 이때 계산량이 매우 커지게 된다. 즉, 정답 context word가 나올 확률은 올리고 나머지 단어들이 나올 확률은 낮추며 학습을 하는데 데이터가 매우 큰 경우 이 확률들을 모두 계산할 때 계산량이 매우 커지게 된다. 따라서 softmax를 적용할 때 전체 단어를 대상으로 확률 값을 구하지 않고 일부 단어만 뽑아서 계산을 하는 negative sampling기법을 사용하여 계산량을 줄였다. 해당 기법을 적용하기 위해서는 positive sample과 negative sample이 필요하다. Positive sample은 이전에 설명한 바와 같이 코퍼스 전체를 설정한 window size만큼 고려하여 슬라이딩 하며 학습 데이터를 만든다. 즉, center word 앞뒤로 설정한 window size만큼 보면서 전체 코퍼스를 훑는다는 것이다. 다음은 window size를 2로 설정하여 positive sample을 만든 것이다:'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/17/Liu2019/',\n",
       "     'source_title': 'Linguistic Knowledge and Transferability of Contextual Representations(Liu(2019))\\n      17 Jan 2021',\n",
       "     'text': '3) Syntactic constituency ancestor tagging 이 task의 경우 문장을 넣었을 때 문장 내에서 특정 단어가 어떤 단어에 대해 Parent인지, GParent인지, GGParent인지를 파악하는 것이다. 이를 통해 모델이 계층적 통사구조를 작 파악했는지 알 수 있다. dataset: phrasestructure tree (from the PTB) 어떤 문장이 입력되었을 때 문장 내의 단어들을 트리로 그렸을 때, 위 그림처럼 F자리에 오는 단어의 parent는 D이고, D의 입장에서 F와 G는 자식이다. 그리고 F와 G는 서로 sister이고 B는 F의 입장에서 grandparent이다. 4) Semantic tagging 문맥에서의 word’s semantic role(agent, patient, etc.)을 tagging하는 task이다. 이는 어휘적 의미를 파악하는지 그리고 불필요한 POS 변별하는지, 즉 POS tag에서의 유용한 것을 명확히 하는 지를 평가한다. dataset: dataset of Bjerva et al. (2016), the tagset has since been developed as part of the Parallel Meaning Bank (Abzianidze et al., 2017) 5) Preposition supersense disambiguation 여러 의미를 가질 수 있는 Preposition이 각각 어떤 의미를 위해 사용되었는지 tag해주는 task이다. 이를 통해 의미의 모호성과 어휘 의미론적 지식을 검증할 수 있다. 앞서 살펴본 tagging task들(모든 token에 대해 decision을 만들었던)과 달리 이 모델은 single-token preposition에 대해 학습과 평가가 이루어진다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
       "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "     'text': '이 방법론을 통해 fine-tuning과정으로 downstream data에 대한 parameter를 따로 update하지 않고도 task에 대한 정답을 추론할 수 있는 zero-shot inference가 가능하다는 것을 밝혔다. zero-shot inference는 task description을 준 이후 prompt를 던져주고 정답을 맞추라고 하는 것이다. Scaling Laws for Neural Language Models OpenAI는 Jared et al. 2020(Scaling Laws for Neural Language Models)를 통해 model size, data size 그리고 연산량을 증가시키면 LM의 performance는 계속 향상한다는 것을 실험적으로 밝혀냈다. 하지만 이 각각은 모두 bottleneck이 되지 않는 적정 정도 안에서 증량하는 것이 중요하다. Model size나 연산량만 커지면 overfitting, data size만 커지면 underfitting이 발생하기 때문이다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/07/FastText/',\n",
       "     'source_title': 'FastText | Enriching Word Vectors with Subword Information\\n      07 Nov 2019',\n",
       "     'text': '위와 같이 FastText모델에서 “백설공주”라는 단어는 단어르 window size에 따라 분리시킨 것과 “백설공주” 전체를 embedding한 것의 합으로 표현된다. 이는 다음과 같이 정의될 수 있다: \\\\begin{matrix} { u }_ { 백설공주 }={ z }_ { <백설 }+{ z }_ { 백설공 }+{ z }_ { 설공주 }+{ z }_ { 공주> }+{ z }_ { <백설공주> } \\\\end{matrix} \\\\begin{matrix} u_t=\\\\sum_{ {g} \\\\in { G_t } }{z_g} \\\\end{matrix} ($G_t$는 target word($t$)에 속한 문자 단위 n-gram집합을 가리킨다.) Model Training \\\\begin{matrix} L(\\\\theta)=logP\\u2061(+|{ c }_ { P },{ o }_ { p })+\\\\sum_{i=1}^klogP\\u2061(-|{ c }_ { n_i },{ o }_ { n_i }) \\\\end{matrix} 위 식은 한 iteration마다 모델 파라미터($\\\\theta$)가 업데이트되는데 이때 한 쌍의 positive sample ($c_P$, $o_p$ : 실제 같이 쓰이는 단어쌍들의 집합)과 $k$쌍의 negative sample($c_{n_i}$, $o_{n_i}$ : 코퍼스 전체에서 랜덤 추출한 함께 사용되지 않은 단어 쌍)이 학습된다는 것을 표현한 log-likelihood function이다. Skip-gram의 log-likelihood function과 같지만 center word($c$)와 context word($o$)를 학습할 때 center word로부터 나온 문자(character)단위의 n-gram vector인 $z$의 등장으로 Skip-gram과 확률이 다르게 정의된다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/Skipgram/',\n",
       "     'source_title': 'Word2vec | Skip-gram\\n      06 Nov 2019',\n",
       "     'text': 'Illustrated by the author'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/',\n",
       "     'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "     'text': 'Efficiency SFT와 preference alignment 개념을 간단하게 결합하여 SFT 수행 이후 별도의 preference alignment 과정을 거칠 필요가 없어 학습 시간과 계산 자원이 줄어들었다. Improved Alignment preference pairs를 통해 fine-tuning을 진행하면서 인간의 선호도를 학습하기 때문에 반환된 답변과 선호도 간의 align이 더 잘 된다. Reduced Bias preference pairs는 데이터 내에 존재하는 bias를 완화하는데 도움을 준다. preference pairs 구축에는 인간이 개입되기 때문에 모델이 편향된 텍스트를 생성하기보다는 더욱 객관적이고 윤리적인 답변을 반환하는 것에 도움을 줄 수 있다. Flexibility ORPO는 preference pairs를 생성할 수 있다는 조건만 충족된다면 매우 다양한 도메인에 간단히 적용될 수 있다. ORPO의 objective function은 SFT loss와 relative ratio loss (LOR)로 구성된다. LOR항은 favored response와 disfavored response간의 likelihood를 최대화함으로써 rejected response를 반환한 모델에 패널티를 적용한다. Reference Training language models to follow instructions with human feedback Direct Preference Optimization: Your Language Model is Secretly a Reward Model ORPO: Monolithic Preference Optimization without Reference Model × Search'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/04/04/finetuning_variations/',\n",
       "     'source_title': 'Fine-tuning variations : RLHF/PPO, DPO, ORPO\\n      04 Apr 2024',\n",
       "     'text': '1. RLHF/PPO GPT-3.5와 같이 LLM 유행 초기에 많이 사용된 방법이다. 이는 크게 두 단계로 작동된다. Step 1. Reward Model 구축 prompt dataset(instruction, context, output pair)으로 모델을 fine-tuning 시킨다. 이때 두 개 이상의 추론 결과를 반환하도록 한다. human labeler에게 추론 결과에 대한 점수/순위를 매기게 한다.(“Anthropic/hh-rlhf”와 같이 honesty와 harmlessness를 검증하는 open-sourced preference ranking dataset도 있다.) 매겨진 점수의 normalization 과정을 거친 후 이것을 single sample-reward pair로 만들어 reward model을 학습시킨다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/06/SentimentalCategorize/',\n",
       "     'source_title': 'Word2Vec(Sentimental Categorize);with code\\n      06 Nov 2019',\n",
       "     'text': '[\\'곧 만기 은퇴 앞두다 있다 노후 준비 미리 꼼꼼하다 해 두 어서 기쁘다\\', \\'노후 준비 미리 꼼꼼하다 해 두 어서 기쁘다\\', \\'이렇게 노후 준비 하다 수 있다 건 옆 도와주다 친구 덕분 이 고마움 뭐 갚다\\', \\'노후 준비 도운 친구 고마움 느끼다 있다 상대방 고마움 표현 하다 수 있다 좋다 방법 있다\\'] data_file = \"sent.content.txt\" with open(data_file, \"w\", encoding=\"utf-8\") as fw: fw.write(\"\\\\n\".join(results)) from gensim.models import word2vec data = word2vec.LineSentence(data_file) print(data) model = word2vec.Word2Vec(data, size=100, window=10, hs=1, min_count=2, sg=1) # CBOW, Skip-gram(0) model.init_sims(replace=True) #필요없는 메모리는 unload #model.save(\"senti_cat.model\") print(\"done\") <gensim.models.word2vec.LineSentence object at 0x000001E2CC7345C8> done ##model = word2vec.Word2Vec.load(\"senti_cat.model\") ##print(model.similarity(\"기쁘다\", \"슬프다\")) ##print(model.similarity(\"기쁘다\", \"행복하다\")) print(model.most_similar(\"기쁘다\")) [(\\'행복하다\\', 0.6797296404838562), (\\'뿌듯하다\\', 0.6624724864959717), (\\'자랑스럽다\\', 0.6465882062911987), (\\'값지다\\', 0.6333721280097961), (\\'다행\\', 0.6318965554237366), (\\'최고\\', 0.6219849586486816), (\\'센스\\', 0.6149460077285767), (\\'만족스럽다\\', 0.6145679950714111), (\\'예식\\', 0.6099244356155396), (\\'축하\\', 0.5977829098701477)] C:\\\\Users\\\\yein4\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead). \"\"\"Entry point for launching an IPython kernel. from gensim.models import KeyedVectors model.wv.save_word2vec_format(\"senti_cat_model\")'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/09/28/Multimodal/',\n",
       "     'source_title': 'Multimodal Learning with Transformers : A Survey\\n      28 Sep 2023',\n",
       "     'text': '모든 transformer계열 모델들이 지적받는 문제인 효율성. model parameter 크기가 크기 때문에 학습에 많은 데이터를 필요로 한다. 왜냐하면 transformer라는 모델 자체가 layer를 많이 쌓을수록 downstream task에 대한 성능이 높아진다는 것이 그동안 다양한 연구를 통해 실증적으로 입증된 것이기 떄문이다. Multi modal transformer에서도 각 modality별로 별개의 transformer 방법론을 사용할 경우 이에따라 단일 modality보다 더 많은 모델 파라미터가 사용된다는 단점이 존재한다. 모델 파라미터가 더 많기 때문에 이에 준하는 더 많은 학습 데이터가 필요하게 된다. 그리고 Self-attention의 특성 상 입력 sequence의 길에에 따라 시간과 메모리 복잡도가 기하급수적으로 증가한다는 단점도 존재한다. 이전 연구들 중에서 이러한 efficiency 문제를 완화하기 위해 제안된 방법들은 주로 Knowledge Distillation 혹은 각 modality에 대해서 서로 다른 크기의 model을 사용하는 방법 혹은 self-attention을 효율적으로 전개하는 시도들이 있었다. 5.5. Robustness'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/30/LMsummary/',\n",
       "     'source_title': 'Language Models Summary(unfinished post)\\n      30 Nov 2022',\n",
       "     'text': 'ELECTRA모델은 generator와 discriminator로 구성되어 있다. discriminator에서는 token이 generator에서 다른 token으로 대체되었는지를 예측한다. generator와 discriminator의 크기가 동일하다면 weight sharing이 가능하다. Hybrid Language Model XLNet × Search'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/07/Warstadt/',\n",
       "     'source_title': 'Investigating BERT’s Knowledge of Language:Five Analysis Methods with NPIs(Warstadt et al.(2019))\\n      07 Apr 2021',\n",
       "     'text': 'Scope Scope는 Licensor가 미치는 영향이다. Table 2를 보면 Licensor whether 뒤에 대괄호가 있는데 이것이 Licensor가 미치는 영향의 범위, 즉 Scope이다. 따라서 이 범위 안에 NPI가 존재할 경우에는 Scope가 1이되고 그렇지 않을 경우에는 0이 된다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/05/CBOW(Continuous-Bag-Of-Words-Model)/',\n",
       "     'source_title': 'Word2vec | CBOW(Continuous Bag Of Words Model)\\n      05 Nov 2019',\n",
       "     'text': 'Word2Vec에서 embedding공간 내에서 단어가 유사도를 나타내는 기준은 cosine similarity이다. 이는 두 vector사이의 각도를 이용하여 유사도를 구하는 방식이며 cosine similarity값이 1에 가까울 수록 유사도가 높은 것이다. CBOW(Continuous Bag Of Words Model) CBOW는 context word(주변 단어)를 통해 center word를 예측하는 모델이다. CBOW에서는 우선 하나의 center word를 두고 몇 개의 context word를 볼 것인지 window size를 정한다. 따라서 window의 크기가 $m$이라면 center word 예측을 위해 참고하는 주변 단어의 개수는 $2m$개가 되는 것이다. 그리고 window를 sliding하며 train dataset을 만든다: 해당 모델의 input(${ x }^{ (c) }$)은 one-hot-encoding된 context words이고 output($y$)은 one-hot-encoding된 center word이다: Architecture of CBOW 모델의 전체적인 구조가 진행되는 과정을 살펴보겠다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/24/CoreferenceResolution/',\n",
       "     'source_title': 'Coreference Resolution | Speech and Language Processing(Daniel Jurafsky and James H. Martin, 2019)\\n      24 Apr 2021',\n",
       "     'text': '위 그림은 the company의 가능한 세 가지 선행사에 대한 score $s$를 계싼하는 것을 보여주는 그림이다. antecedent score부분을 보면 the company에 대해서 General Electric이 선행사인 경우와 the Postal Service가 선행사인 경우 각각이 들어가는 것을 볼 수 있고, 이들의 요소별 곱도 들어가며, 추가적으로 다른 feature들도 들어간다. $m(i)$와 $c(i,j)$는 scoring 함수로, 각각 mention score, antecedent score를 나타내며 둘 다 스팬 $i$를 나타내는 백테 $g_i$를 기반으로 한다. FFNN(Feed Forward Neural Network). 가중치의 반복적인 업데이트. 1) 인풋 $x$를 받아서 2) 이것의 $y=Wx+b$를 계산하고 3) 여기에 activation function(sigmoid, tanh, ReLU, etc.)를 적용 선행사 score $c(i, j)$는 input으로 스팬 $i$와 $j$의 representation을 취하며, $g_i \\\\circ g_j$($g_i$와 $g_j$의 요소별 곱)은 두 스팬의 요소별 유사도이다. 그리고 마지막에는 mention distances 그리고 화자와 장르에 대한 정보와 같은 유용한 feature들을 인코딩한 feature vector $\\\\varphi(i, j)$가 들어간다. 이제 coreference score $s$를 보겠다. score $s(i, j)$는 세 가지 요소를 포함한다:'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/',\n",
       "     'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "     'text': 'Sparse MoE Layers (Experts) : transformer architecture의 dense feed-forward network layer를 대체하는 layer이다. (dense feed-forward network layer 여러개라고 볼 수 있음.)각 MoE layer는 expert라고 불린다. expert는 특정 유형의 정보를 처리하는데 특화된 독립적인 신경망(모델)이다. 입력에 대해 일부 expert만 활성화된다. Gate Network / Router : Router는 token의 특성과 Router가 학습한 parameter에 따라 어떤 expert에게 token을 보낼지 결정한다. 즉 어떤 token이 어떤 expert에 의해 처리될지, 어떤 expert를 활성화시킬지 결정한다. 입력의 각 부분이 가장 적합한 expert에 의해 처리되도록 한다. 이는 expert를 잘 선택할 수 있도록 학습된 모델이다. 아래 이미지에서 왼쪽은 일반적인 GPT 계열 모델(dense model)의 구조이고, 오른쪽은 GPT 계열 모델 구조에서 feedforward network가 Router + Exterts로 대체된 구조이다. 오른쪽 이미지를 보면 현재 time step에서의 token은 Router를 통해 Expert2와 Expert4이 활성화되어 이를 통과하고 있다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2023/10/02/finetuning/',\n",
       "     'source_title': 'About Fine-tuning\\n      02 Oct 2023',\n",
       "     'text': '2. Training Tranformers 계열의 모델은 label이 없는 데이터에 대해서도 supervised learning을 한다. 이는 un-labeled data를 입력 받아 모델 내부에서 각 모델마다 설정된 task에 맞게 데이터를 처리하여 학습에 사용하기 때문이다. 예를 들어 BERT는 입력 문장의 일부를 masking 처리하여 자체적으로 masking한 부분을 예측하며 학습을 수행하고, GPT의 경우에는 현재 시점 이후의 token들을 보지 않고 다음 token들을 예측해 나가며 학습을 수행한다. 이와 같이 알고리즘이 자체적으로 스스로 통제/감독하며 학습하는 방식을 self-supervised learning이라고 한다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': '문장은 선형적이며 계층적이다. 문장 내에 존재하는 순서는 문장의 의미를 구성하는데 있어 매우 중요한 역할을 한다. RNN(Recurrent Neural Network)모델은 노드들 사이에 hidden layer를 삽입하여 hidden state의 결과가 계속 순환하며 문장 내 순서를 고려할 수 있게 만들어진 모델이다. 예를 들어 The cat is on the mat이라는 문장에서 cat 다음에 is가 올 확률이 높다는 것과 같은 방식으로 구조적으로 수용 가능한 문장을 형성할 수 있다. 하지만 통사적으로 간단한 문장이 높은 확률을 할당 받는다 하더라도 언어학자들이 연구하는 논란의 여지가 많은 문장들을 연구할 수 있는 수준이 되는 것은 아니다. 그래도 최근 언어모델(Linzen(2019)기준)은 언어학적 지식을 기반으로 통사적 능력(ability)를 측정하고 향상시키는데 높은 성능을 보이고 있다. 이렇게 언어학은 딥러닝 성능 향상의 기준점이 될 수 있다. 예를 들어 본 논문의 저자인 Tal Linzen은 2016년에 정문(grammatical)과 비문(ungrammatical)을 이용해서 언어모델이 정문에 더 높은 확률을 줄 수 있는지에 대한 실험을 하였는데 이에 대해'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/01/16/TalLinzen2019/',\n",
       "     'source_title': 'What Can Linguistics and Deep Learning Contribute to Each Other? (Linzen et al. (2019))\\n      16 Jan 2021',\n",
       "     'text': 'local heuristics를 확인하기 위해서는 bigram model을 써보면 된다.bigram은 문장을 단순히 선형적으로만 봐서 문장의 구조를 파악하지 못하는 애다. 얘가 예측을 잘했다는 것은 그 문장은 모델이 잔머리 굴리기 딱 좋은 문장이라는 말이다. 위 예문은 6a를 yes/no question 형태로 만드는 예시이다. 위 예문에 대해서 단순히 바로 직전의 단어를 가지고 예측하는 모델인 bigram model이 6c가 비문이라는 것을 잘 예측하였다. 이는 그냥 단어쌍의 빈도를 기반으로 도출된 결과이다. 즉, who is라는 단어쌍이 who crying이라는 단어쌍 빈도보다 높았기 때문이다. 이런 것을 local heuristics라고 한다. 결과는 제대로 나왔지만 모델이 통사를 제대로 파악한 것은 아닌 것이다. 따라서 예문 6과 같은 문장으로 test를 진행하고서 모델의 통사 파악 능력을 높게 평가하는 것은 의미가 없다는 것이다.'},\n",
       "    {'source': 'https://finddme.github.io/dev%20log/2024/01/02/rag/',\n",
       "     'source_title': 'RAG\\n      02 Jan 2024',\n",
       "     'text': 'return source file 좋아요 싫어요 버튼 추가 [답변 일시, 소요 시간, 질문, 답변, 사용자 ip] 저장 기능 추가 RAG 기반 질문-대답 챗 Weaviate의 retrieval 결과를 받아 prompt와 함께 LLM에 입력 후 결과 반환 RAG 이후 LLM 답변 시 참조된 문서 요약(source 번호 입력) 질의응답 내용과 참조 문서 정보를 저장하여 참조 문서 번호 입력 시 해당 문서에 대한 요약 결과 반환 문서 제목 입력 후 문서 요약 -문서 제목 입력 시 문서 요약 결과 반환 문서 전체 길이에 따라 LLM 입력 횟수 증가+Json 형식 답변 반환 표 출력을 위한 json 형식 반환 “표“, “json” 포함 질문 입력 시 json 형식 반환 prompt와 함께 LLM에 입력 후 결과 반환 7. Problems and Solutions 속도 문제 LLM 추론 속도 문제: vllm 적용 similarity search 연산 속도 문제: 문서가 많아 시간이 생각보다 많이 소요되어 binary vectorstore, metadata filtering, hierarchical similarity search 적용 query 유형에 따라 LLM 추론 품질이 상이한 문제 query 유형 분류를 통해 다른 prompt를 타도록 pipeline 구성 LLM 영어 반환 문제 port-retrieval 과정으로 영어 답변 반환 시 번역 과정 추가'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing%20and%20linguistics/2021/04/04/DaCosta/',\n",
       "     'source_title': 'Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies(Da Costa.J et al.(2020))\\n      04 Apr 2021',\n",
       "     'text': '두 번째 실험의 결과도 첫 번째 실험처럼 전반적으로 겹침이 많은 것을 볼 수 있다. 따라서 Gulordava와 Google model은 장거리 의존 구문의 형태 통사적인 특징을 잘 학습하지 못한다는 것을 알 수 있다. 3. Transformer-XL Transformer-XL은 기존 Transformer의 self-attention network에 ‘recurrence’개념을 도입한 모델이다. 기존 Transformer는 현재의 정보를 처리할 때 이전 정보가 단절되는데 Transformer-XL은 recurrence를 도입하여 이전 정보가 현재 정보에도 전파되도록 한다. Dai et al.(2019)에 따르면 RNN보다 80%더 긴 dependency를 학습할 수 있다고 한다. 이를 증명하기 위해 본 논문에서는 RNN으로 진행한 실험을 동일하게 진행하였다. 3.1 Experiment 1, 2 result: agreement in clefts, agreement in indirect interrogatives 위 그래프는 실험 1의 결과를 시각화한 것이다. 내포수준 1과 3에서만 주-동 수 일치 surprisal이 불일치 surprisal보다 낮게 나왔다. 즉 내포수준 1과 3에서만 제대로 구분을 하고 나머지는 못하는 것이다. 따라서 Transformer-XL의 성능이 RNN보다 아주 조금 더 좋다는 것을 알 수 있다.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2022/11/29/DAPT/',\n",
       "     'source_title': 'DAPT-TAPT | Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\\n      29 Nov 2022',\n",
       "     'text': 'RC-500: low-resource setting된 RCT data. 18040개의 RCT data 중 500개만 labeled data로 사용하고 나머지는 unlabeled data로 TAPT학습에 사용한다. (동일한 dataset에서 빼 온 것이기 때문에 동일한 task distribution에 속한다) HYPERPARTISAN: 5000개의 unlabeled data를 TAPT에 사용 IMDB: task annotator가 50000개의 unlabeled data를 직접 수집 2) Results'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/',\n",
       "     'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "     'text': '\\\\log\\u2061 P_{ \\\\theta }(Y|X)=\\\\sum_{t=1}^{m} \\\\log\\u2061 P_{ \\\\theta }(y_t|X, y_{<t}) \\\\end{matrix} 위 수식은 이전 time-step의 결과와 encoder로부터 도출된 결과($X$)를 기반으로 하여 target값($Y$)을 유추해내는 작업을 표현하고 있다. 아래 수식은 Decoder부분을 수식으로 표현한 것이다. \\\\begin{matrix} { h }_ { t }^{ tg\\u2061t }={ RNN }_ { dec } ({ emb }_ { tgt }({ y }_ { t-1 }), { h }_ { t-1 }^{ tgt }) \\\\ where \\\\ { h }_ { 0 }^{ tg\\u2061t } = { h }_ { n }^{ src } \\\\ and \\\\ y_0=BOS \\\\end{matrix} 위 수식을 살펴보면 ${ h }_{ t }^{ tg\\u2061t }$ 는 $t$번째 target state의 hidden vector인데 이는 Decoder부분의 RNN cell에 이전 state의 결과 값($y_{t-1}$)을 embedding한 값과 이전 state에서 나온 hidden vector값이 입력되어 도출된다. 여기에서 주의 깊게 살펴볼 점은 decoder부분의 첫 시작에서는 $y$의 초기 값으로써 $BOS$를 입력한다는 것이다. $\\\\circ$ Generator Generator부분에서는 Decoder부분의 RNN Cell을 거쳐 나온 값들이 softmax를 거쳐 0부터 1사이의 확률 값으로 변환되어 가장 높은 확률 값이 해당 state의 출력 값으로 도출된다. 아래는 Generator 부분을 수식화 한 것이다. \\\\begin{matrix} \\\\hat{ y }_ { t }=softmax({ linear }_ { hs\\\\rightarrow \\\\left\\\\vert { V }_ { tgt } \\\\right\\\\vert }({ h }_ { t }^{ tg\\u2061t })) \\\\ and \\\\ \\\\hat{ y }_ { m }=EOS \\\\end{matrix} Where $hs$ is hidden size of RNN, and $\\\\left\\\\vert { V }_{ tgt } \\\\right\\\\vert$ is size of output vocabulary.'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/19/Transformer/',\n",
       "     'source_title': 'Transformer | Attention Is All You Need\\n      19 Nov 2019',\n",
       "     'text': '위 그림과 같이 여러 layer를 거친 후 마지막 layer의 출력 값이 encoder의 최종 출력 값이 되어 decoder로 넘어간다: Illustrated by the author - Decoder Illustrated by the author'},\n",
       "    {'source': 'https://finddme.github.io/dev%20log/2023/03/31/llm_tuning_merge/',\n",
       "     'source_title': 'LLM Tuning / Merge\\n      31 Mar 2023',\n",
       "     'text': '한국어 답변을 안정적으로 반환하던 Model 1보다 한국어 추론 성능이 저하됨 Model 1보다 json 형식을 반환하는 비율이 높아졌지만 완전한 json 형식이 아닌 경우가 있어 json 형식 완성을 위한 후처리 모듈이 필요함 6. Problems and Solutions LLM 추론 시 한국어 입력에 대한 영어 답변 반환 현상 -> DPO trainset rejected에 영어 답변 입력 json format 답변 반환을 위해 json format output 데이터를 학습한 모델을 merge json 형식이 완전하지 않아 표 생성이 되지 않는 문제 -> json 형식 완성을 위한 후처리 모듈 생성 × Search'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/',\n",
       "     'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "     'text': 'def forward(self, inputs: torch.Tensor): inputs_squashed = inputs.view(-1, inputs.shape[-1]) # 입력된 tensor 크기를 input tensor의 마지막 차원으로 통일 gate_logits = self.gate(inputs_squashed) # gate linear layer 통과 weights, selected_experts = torch.topk( gate_logits, self.args.num_experts_per_tok ) # gate_logits에서 top-K개의 expert 뽑기 weights = nn.functional.softmax( weights, dim=1, dtype=torch.float, ).type_as(inputs) # top-K개의 expert에 대한 weight를 softmax에 넣어 score 구하기 results = torch.zeros_like(inputs_squashed) # 나중에 결과 담을 inputs_squashed tensor와 같은 크기의 초기화된 tensor 생성 for i, expert in enumerate(self.experts): batch_idx, nth_expert = torch.where(selected_experts == i) # selected_experts에서 값이 i인 요소들의 index를 반환. results[batch_idx] += weights[batch_idx, nth_expert, None] * expert( inputs_squashed[batch_idx] ) # 결과 반영 return results.view_as(inputs) class TransformerBlock(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.n_heads = args.n_heads self.dim = args.dim self.attention = Attention(args) self.feed_forward = MoeLayer( experts=[FeedForward(args=args) for _ in range(args.moe.num_experts)], gate=nn.Linear(args.dim, args.moe.num_experts, bias=False), moe_args=args.moe, ) self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps) self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps) self.args = args'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/11/Seq2Seq/',\n",
       "     'source_title': 'Sequence to Sequence(Seq2Seq)\\n      11 Nov 2019',\n",
       "     'text': 'Illustrated by the author'}]}}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd43eead-0d3e-42fd-bf6c-8f054bdc8904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Aggregate': {'B_with_title': [{'meta': {'count': 521}}]}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query.aggregate(\"B_with_title\").with_meta_count().do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b492154-e3e8-4381-94b1-e00eb797672c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909d4a12-94e1-488d-9e03-b255102fcb58",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb1502-0214-465f-bb2c-6c074c06eae7",
   "metadata": {},
   "source": [
    "## Retrieval check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1ee36d-4d29-4ea4-8b68-e380ace7a3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.7.1.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    url=\"http://192.168.2.186:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca8ac2c-b1ea-44c0-9cbe-71c7ae0e5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_class=\"B_with_title\"\n",
    "property_list = list(map(lambda x: x[\"name\"], client.schema.get(weaviate_class)['properties']))\n",
    "query=\"LLaMa3의 구조에 대해 설명해줘\"\n",
    "query_vector = get_embedding(query)\n",
    "response = client.query.get(weaviate_class,property_list).with_hybrid(query, vector=query_vector).with_limit(6).do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a7ffe6b-6433-47e7-845c-babce3cc3e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Get': {'B_with_title': [{'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/',\n",
       "     'source_title': 'LLaMa 3\\n      01 May 2024',\n",
       "     'text': 'LLaMa 3 LLaMA 3는 Meta에서 공개한 Open Source LLM model로, LLaMa2에 이어 공개된 모델이다. LLaMA 3는 Instruct Model과 Pre-trained Model에 대해 각각 8B, 70B 두 사이즈가 공개되었다. Pretraining과 Post-training 방법의 개선으로 공개된 8B, 70B의 Pretrained, Instruction-fine-tuned model이 2024 4월18일 기준 해당 parameter scale 모델 중 가장 좋은 성능을 보인다고 한다. Post-training과정에서는 false refusal rate를 줄이고, model의 alignment를 개선하고, model response의 다양성을 증가시켰다. 특히 LLaMa 3는 2보다 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있을 것으로 보인다. LLaMa 3 개발 시 Meta팀은 benchmark 성능 향상 뿐만 아니라 실제 추론 능력(real-world scenario) 최적화에도 집중하였다고 한다. 실제 추론 능력 검증을 위해 새로운 high-quality human evaluation set을 개발했다고 한다. 해당 데이터는 12가지 use case에 대한 총 1,800개 prompt로 구성되어 있다. (12가지 use case: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, summarization). 아래 표는 해당 evaluation set에 대한 Claude Sonnet, Mistral Medium, GPT-3.5의 추론 결과를 비교한 것이다. https://ai.meta.com/blog/meta-llama-3/ LLaMA 3 개발팀은 모델 개발 시 Model architecture, Training data, Scaling up pretraining, Instruction fine-tuning을 중점적으로 개선했다고 한다. Model architecture'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/07/01/self_ref_rag/',\n",
       "     'source_title': 'Self-Reflective RAG (세 가지 Grader) : Langchain | LangGraph | GROQ | Wevieate | Tavily | Chainlit\\n      01 Jul 2024',\n",
       "     'text': 'output: LLaMa 3은 Meta에서 개발한 Open Source LLM(Large Language Model) 모델로, LLaMa 2를 이어 발표된 모델입니다. 이 모델은 Instruct Model과 Pre-trained Model로 8B, 70B 두 사이즈가 공개되었으며, 이는 2024년 4월 18일 기준 해당 파라미터 스케일 모델 중 가장 좋은 성능을 보이고 있습니다. LLaMa 3는 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있습니다. 이 모델은 standard decoder-only transformer architecture를 기반으로 하고, 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding합니다. LLaMa 3는 15T 개의 token으로 학습되었으며, 이는 LLaMa 2보다 약 7배 더 큰 학습 데이터를 사용하였습니다. Graph visualization ref. https://langchain-ai.github.io/langgraph/how-tos/visualization/#using-mermaid-pyppeteer from IPython.display import Image, display from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeColors print(app.get_graph().draw_mermaid()) display( Image( app.get_graph().draw_mermaid_png( draw_method=MermaidDrawMethod.API, ) ) ) × Search'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/01/llama3/',\n",
       "     'source_title': 'LLaMa 3\\n      01 May 2024',\n",
       "     'text': 'Base architecture: LLaMa 3는 standard decoder-only transformer architecture를 기반으로 개발되었다. Tokenizer size: 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding하여 모델 성능을 향상시킴. -> LLaMa 2보다 4배 큰 tokenizer size Attention method: 8B, 70B 모두 GQA를 사용하여 모델 추론 효율성을 높였다. -> LLaMa 2는 70에만 GQA 적용 context window: 8,192 token -> LLaMa 2는 4,096였다. Tokenizer size가 커지면 encoding을 효과적/효율적으로 할 수 있는 이유: tokenzer size가 4배 커지면 동일한 데이터가 약 4배정도 압축되어 모델에 입력될 수 있다. 예를 들어 입력한 데이터가 LLaMa 2의 tokenizer로 200 token으로 encoding 될 때, LLaMa 3 tokenizer로는 100개가 되지 않는 token 수로 encoding될 수 있기 때문이다. 그리고 tokenizer size가 커진 만큼 모델이 많이 학습하지 않은 한국어 데이터에 대한 tuning을 수행할 때 vocab에 따로 한국어 token을 추가하지 않아도 된다. Training data size LLaMa 3는 15T개의 token으로 학습되었다. (LLaMa 2는 2T로, 2에 비해 3는 약 7배 더 큰 학습데이터를 사용하였다.) 수집된 데이터는 모두 publicly available source에서 수집되었다. code data: LLaMa 2에 사용된 code data 수보다 4배 더 많음 multilingual use case를 위해 전체 training data의 5% 이상을 high-quality non-English(30개 이상) data로 구성'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/05/11/llava/',\n",
       "     'source_title': 'LLaVA : Visual Instruction Tuning\\n      11 May 2024',\n",
       "     'text': 'Stage 1. Pre-training for Feature Alignment: CC3M의 subset을 기반으로 projection layer matrix만 업데이트한다. Stage 2. Fine-tuning End-to-End: 투영 행렬과 LLM 모두 두 가지 다른 사용 시나리오에 맞게 업데이트됩니다: Multimodal Chatbot(Visual Chat): LLaVA는 일상적인 사용자 지향 application(downstream) 수행을 위해 multimodal instruction-following data로 fine-tuning을 진행한다. Science QA: LLaVA는 science domain을 위해 multimodal reasonsing dataset으로 로 fine-tuning을 진행한다.'},\n",
       "    {'source': 'https://finddme.github.io/llm%20/%20multimodal/2024/06/22/merge2/',\n",
       "     'source_title': 'MoE, MoA 믹스는 다가와 아 오에이\\n      22 Jun 2024',\n",
       "     'text': '위 이미지는 MoA 작동 구조이다. MoA는 여러 layer로 구성되어 있다. 각 MoA layer에는 여러 LLM들이 있는데 위 그림에서 ‘A’들이 다 LLM이다. 각 LLM은 MoA layer에 입력된 prompt를 입력 받아 response를 생성한다. MoA layer 1의 LLM들의 응답은 layer 2에 전달된다. layer 2에도 LLM들이 있는데 layer 1의 LLM과 동일할 수도 아닐 수도 있다. 이런식으로 MoA layer를 다 거친 후 final layer에 들어가는데 final layer에는 하나의 LLM이 있다. final layer의 LLM은 input prompt + 이전 layer들에서 수집된 response를 입력 받아 최종 response를 만들어낸다. final layer에서 사용되는 prompt를 해당 논문에서는 “Aggregate-and-Synthesize prompt”라고 부르는데, 각 layer들의 응답들을 삽입하고 모델에게 이를 평가하여 최종 응답을 도출하도록 유도하는 prompt이다. Mixture-of-Agents Enhances Large Language Model Capabilities MoE에서는 각 모델들의 일부를 병합하지만 MoA에서는 각 모델 통으로 사용한다. Reference MoE Mixtral of Experts Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity A REVIEW OF SPARSE EXPERT MODELS IN DEEP LEARNING https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562 MoA Mixture-of-Agents Enhances Large Language Model Capabilities https://github.com/togethercomputer/MoA × Search'},\n",
       "    {'source': 'https://finddme.github.io/natural%20language%20processing/2019/11/04/NPLM(Neural-Probabilistic-Language-Model)/',\n",
       "     'source_title': 'NPLM(Neural Probabilistic Language Model)\\n      04 Nov 2019',\n",
       "     'text': 'NPLM의 구조를 보면 크게 input layer와 hidden layer 그리고 output layer가 있다. 설명을 위해 간단한 예를 들겠다. “퇴근 시간 서울역은 최악이다”라는 문장에서 [퇴근], [시간], [서울역은] 이렇게 세 개의 단어가 주어진 경우에 [최악이다]라는 단어를 예측한다고 가정하겠다. 이때 Input layer에서는 우선 주어진 세 개의 벡터에 대해 행렬$C$와 내적한다. 이후 내적하여 나온 값들을 concatenate하면 입력 벡터 $x$가 만들어진다. 입력 벡터 $x$에 대한 정의는 다음과 같다: \\\\begin{matrix} x=[{ { x }_ { t-1 } },{ { x }_ { t-2 } },… ,{ { x }_ { t-n+1 } }] \\\\end{matrix} 위 식에서 $n$은 n-gram의 개수이다. 즉 예측 대상 단어를 포함한 단어의 개수이고 $t$는 문장 내에서 해당 단어가 등장한 순서를 나타낸다. 이렇게 구한 $x$를 $H$와 내적한 뒤 bias term ’$d$’를 더한 후 tanh를 적용시켜 hidden layer를 만들고, 거기에 $U$를 내적한 후, bias term ’$b$’를 더해주면 $y_{w_t}$가 구해진다: \\\\begin{matrix} { y }_ { { w }_ { t } }=b+U\\\\cdot tanh(d+H_x) \\\\end{matrix}'}]}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588f298-fac5-458e-a4d4-e9c22bfc2d0b",
   "metadata": {},
   "source": [
    "## Retrieval def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5706332-f7b5-4c96-b3e2-a89e8c4b7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE from Vector Store DB---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    query_vector = get_embedding(question)\n",
    "    documents = client.query.get(\"B_with_title\", [\"text\",\"source_title\",\"source\"]).with_hybrid(question, vector=query_vector).with_limit(6).do()\n",
    "    web_search = \"No\"\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e83654-bd21-4f2d-9917-d40b1bbc3338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8324ba6b-b83b-49e4-8ca0-2cd2f77aabec",
   "metadata": {},
   "source": [
    "# Reranker\n",
    "\n",
    "ref.\n",
    "\n",
    "- Cross Encoder Reranker: (langchain) https://python.langchain.com/v0.2/docs/integrations/document_transformers/cross_encoder_reranker/\n",
    "\n",
    "- (weaviate) reranker-cohere: https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere\n",
    "\n",
    "- <Reranker + Query Rewriting>(llamaindex) :\n",
    "\n",
    "https://ai.gopubby.com/transforming-data-orchestration-the-query-pipeline-and-flagembedding-rerank-with-llamaindex-dee5a2e9a797\n",
    "\n",
    "https://aksdesai1998.medium.com/advanced-rag-extracting-complex-pdfs-containing-tables-text-using-llamaparse-48b61693da58\n",
    "\n",
    "- cohere (+qdrant)\n",
    "https://medium.com/@parikshitsaikia1619/unlock-rags-potential-with-distance-metrics-and-rerankers-42df4f171f5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad603347-421a-4d91-8cf7-4d582e21f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "def reranker_cohere(state):\n",
    "    co = cohere.Client(coher_API_KEY)\n",
    "    \n",
    "    \"\"\"\n",
    "    cohere models\n",
    "    https://docs.cohere.com/docs/models\n",
    "    \"\"\"\n",
    "    print(\"--- RERANK ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents_or = state[\"documents\"]\n",
    "\n",
    "    documents=[r[\"text\"] for r in documents_or[\"data\"][\"Get\"][\"B_with_title\"]]\n",
    "    source=[{\"source_title\":r[\"source_title\"],\"source\":r[\"source\"]} for r in documents_or[\"data\"][\"Get\"][\"B_with_title\"]]\n",
    "    \n",
    "    rerank_res = co.rerank(\n",
    "        model=\"rerank-multilingual-v3.0\",\n",
    "        query=question, # search query used for docs retrieval\n",
    "        documents=documents, #list of retrieved docs\n",
    "        top_n=4, # selecting top 4 docs\n",
    "    )\n",
    "    \n",
    "    doc_txt = \"\"\n",
    "    \n",
    "    for idx,result in enumerate(rerank_res.results):\n",
    "        doc_txt += f\"doc {idx}. {documents[result.index]} \\n\"\n",
    "        \n",
    "    return {\"documents\": doc_txt, \"question\": question, \"source\": source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b8f1a-bcac-4182-8c03-5eb345b4e182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68efa1-82fa-4882-a777-51b304fc5161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85188546-2b1c-468c-96e4-dcd5920cd992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef2543c1-59f7-4d06-be5e-46157e880254",
   "metadata": {},
   "source": [
    "# Grader 1 : Relevant Grader\n",
    "\n",
    "- 검색된 doc이 relevant한지 아닌지 확인\n",
    "\n",
    "Corrective RAG (CRAG): https://finddme.github.io/llm%20/%20multimodal/2024/02/21/RAG/#5-corrective-rag-crag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e001c6-054b-41f5-ae6e-0dac882032b8",
   "metadata": {},
   "source": [
    "## Relevant Grader test / prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5f8b38-43f4-44a7-b277-f13f32897728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\"\"\"\n",
    "pydantic_v1\n",
    "- https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/\n",
    "- 특정 schema에 맞게 output을 구성하도록 LLM에 quary할 수 있게하는 parser.\n",
    "\"\"\"\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_docs = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_relevance = grade_prompt | structured_llm_grader_docs\n",
    "\n",
    "doc_txt=\"\"\n",
    "for idx,i in enumerate(response[\"data\"][\"Get\"][\"B_with_title\"]):\n",
    "    t=i[\"text\"]\n",
    "    doc_txt += f\"doc {idx}. {t} \\n\"\n",
    "\n",
    "retrieval_grader_relevance.invoke({\"question\": query, \"document\": doc_txt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc9c33-78d2-41c7-a84a-acfdadfe82dd",
   "metadata": {},
   "source": [
    "## Relevant Grader def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da14cc6f-a8ac-4cab-9fa6-402b2e8223cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents[\"data\"][\"Get\"][\"B_with_title\"]:\n",
    "        score = retrieval_grader_relevance.invoke({\"question\": question, \"document\": d[\"text\"]})\n",
    "        grade = score.binary_score\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf87b89-526f-4bd2-b5d3-a9f77b6fdcee",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3681d-abb1-42c4-ba78-4f5bf3bd9f4a",
   "metadata": {},
   "source": [
    "## Generate test / prepare (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff647e1-9be0-4738-a660-1f4db683ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMa 3의 구조에 대해 다음과 같이 설명할 수 있습니다:\n",
      "\n",
      "LLaMa 3는 표준 디코더 전용 트랜스포머 아키텍처를 기반으로 하며, 128K 토큰 크기의 어휘를 사용하여 언어를 더 효과적으로 인코딩합니다. 이 모델은 8B와 70B 두 가지 크기로 제공되며, 둘 다 GQA(Grouped Query Attention)를 사용하여 모델 추론 효율성을 높였습니다. LLaMa 3는 8,192 토큰의 컨텍스트 윈도우를 가지고 있으며, 15T 개의 토큰으로 학습되었는데, 이는 LLaMa 2보다 약 7배 더 큰 학습 데이터를 사용한 것입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser # 출력물을 기본 str 형태로 받는 라이브러리\n",
    "\n",
    "# Prompt\n",
    "# prompt = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"You are an Korean assistant for question-answering tasks. \n",
    "#     Use the following pieces of retrieved context to answer the question. \n",
    "#     If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "#     And you must answer in Korean.\n",
    "# Question: {question}\n",
    "# Context: {context}\n",
    "# Answer:\"\"\"\n",
    "# )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a Korean-speaking assistant specializing in question-answering tasks. \n",
    "    Use the provided context informations and relevant documents to answer the following question as accurately as possible. \n",
    "    If the answer is not clear from the context or if you do not know the answer, explicitly state \"모르겠습니다.\" (I don't know). \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    All responses must be given in Korean.\n",
    "    Based on the given information, return a very detailed response.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": doc_txt, \"question\": query})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f51dfd-e64a-40d1-beb5-04f919c63d52",
   "metadata": {},
   "source": [
    "## Generate def (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6170419c-617c-478d-8c96-92090f809cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE Answer---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    source = state[\"source\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question,\"source\": source, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55de9df-e031-4a3c-868f-fa9c50b9e87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a751fc74-968e-47de-943b-ba6f0ea96d8a",
   "metadata": {},
   "source": [
    "## Generate test / prepare (chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7708b84e-62df-4fa2-bf34-affce7c7950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_test=\"안녕\"\n",
    "prompt=f\"\"\"\n",
    "You are a friendly and helpful assistant designed to engage in everyday chats with users. Your goal is to make the conversation pleasant, informative, and engaging. \n",
    "Use a casual and approachable tone. Be polite, patient, and show empathy when needed. \n",
    "    \n",
    "Guidelines for casual conversation:\n",
    "- Use informal language and a friendly tone\n",
    "- Feel free to use common contractions (e.g., \"don't\", \"can't\", \"I'm\")\n",
    "- Incorporate occasional humor or light-hearted comments when appropriate\n",
    "- Be empathetic and show interest in the user's thoughts and experiences\n",
    "- Avoid overly formal or technical language unless the conversation calls for it\n",
    "\n",
    "When you receive a message from the user, follow these steps:\n",
    "\n",
    "1. Read and analyze the user's message.\n",
    "\n",
    "2. Identify the main topic or intent of the message.\n",
    "\n",
    "3. Consider an appropriate casual response that addresses the user's input and maintains the flow of conversation.\n",
    "\n",
    "4. If the user asks a question or seeks information, provide a helpful answer in a casual manner. If you're unsure about something, it's okay to admit that you don't know.\n",
    "\n",
    "5. If appropriate, ask a follow-up question to keep the conversation going or to learn more about the user's interests or thoughts.\n",
    "\n",
    "Formulate your response based on the above guidelines and analysis. Your response should be friendly, engaging, and natural-sounding.\n",
    "\n",
    "Write your response inside <response> tags. Make sure your response is appropriate for a casual conversation and maintains a friendly tone throughout.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "            prompt,\n",
    "    ),\n",
    "    (\"human\", question_test),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f730a973-d683-4324-8d13-ce09076457f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 반가워요. 오늘 기분은 어떠세요? 저는 당신과 대화하게 되어 정말 기쁩니다. 한국어로 대화하니 더 친근한 느낌이 드네요. 혹시 오늘 특별한 계획이 있으신가요?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.content.replace(\"<response>\",\"\").replace(\"</response>\",\"\").strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afb714-7880-44cf-af75-9a1670b05458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e8015-ec84-4de7-8b80-184ef59dd8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec755f9-da0f-4c55-910b-8c92b1342840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc0cb118-fe41-41a7-b198-f90e42b9e830",
   "metadata": {},
   "source": [
    "## Generate def (chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dd638e3-edb7-4151-81af-c8f027a7bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conv(state):\n",
    "    \n",
    "    print(\"---GENERATE Answer (conversation)---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    source = state[\"source\"]\n",
    "    \n",
    "    prompt=f\"\"\"\n",
    "    You are a friendly and helpful assistant designed to engage in everyday chats with users. Your goal is to make the conversation pleasant, informative, and engaging. \n",
    "    Use a casual and approachable tone. Be polite, patient, and show empathy when needed. \n",
    "        \n",
    "    Guidelines for casual conversation:\n",
    "    - Use informal language and a friendly tone\n",
    "    - Feel free to use common contractions (e.g., \"don't\", \"can't\", \"I'm\")\n",
    "    - Incorporate occasional humor or light-hearted comments when appropriate\n",
    "    - Be empathetic and show interest in the user's thoughts and experiences\n",
    "    - Avoid overly formal or technical language unless the conversation calls for it\n",
    "    \n",
    "    When you receive a message from the user, follow these steps:\n",
    "    \n",
    "    1. Read and analyze the user's message.\n",
    "    \n",
    "    2. Identify the main topic or intent of the message.\n",
    "    \n",
    "    3. Consider an appropriate casual response that addresses the user's input and maintains the flow of conversation.\n",
    "    \n",
    "    4. If the user asks a question or seeks information, provide a helpful answer in a casual manner. If you're unsure about something, it's okay to admit that you don't know.\n",
    "    \n",
    "    5. If appropriate, ask a follow-up question to keep the conversation going or to learn more about the user's interests or thoughts.\n",
    "    \n",
    "    Formulate your response based on the above guidelines and analysis. Your response should be friendly, engaging, and natural-sounding.\n",
    "    \n",
    "    Write your response inside <response> tags. Make sure your response is appropriate for a casual conversation and maintains a friendly tone throughout.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "                prompt,\n",
    "        ),\n",
    "        (\"human\", question),\n",
    "    ]\n",
    "    # ai_msg = llm.invoke(messages)\n",
    "    # response = ai_msg.content.replace(\"<response>\",\"\").replace(\"</response>\",\"\").strip(\"\\n\")\n",
    "    # source = [{\"source_title\":\" \",\"source\":\"casual conversation\"}]\n",
    "    # return {\"documents\": documents, \"question\": question,\"source\": source, \"generation\": ai_msg.content}\n",
    "    return llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dddc95-0631-492b-a15c-00369d3b7b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cc1af-a376-4e5c-94a6-8c3bdc3df371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc724ff-f9c4-40e5-9761-42115668e59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82848d3c-9c89-4c99-88a9-3f20926c4539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16082e4-e86a-4f31-8c1d-a28eaddf06bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Grader 2 : Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeffd4f9-ac9a-4e2c-9509-b4fbbba52d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "# BaseModel: pydantic_v1\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Don't consider calling external APIs for additional information. Answer is supported by the facts, 'yes' or 'no'.\")\n",
    " \n",
    "# LLM with function call \n",
    "structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations)\n",
    " \n",
    "# Prompt \n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is supported by a set of retrieved facts. \\n \n",
    "     Restrict yourself to give a binary score, either 'yes' or 'no'. If the answer is supported or partially supported by the set of facts, consider it a yes. \\n\n",
    "    Don't consider calling external APIs for additional information as consistent with the facts.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "]\n",
    ")\n",
    "  \n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader_hallucination\n",
    "hallucination_grader.invoke({\"documents\": doc_txt, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a1a9c-babe-4090-8a11-25ec0808b412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Grader 3 : Answer Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff47f6d-2848-4707-87a2-748a50bc241d",
   "metadata": {},
   "source": [
    "## Answer Grader test / prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "422392c2-3197-4ea4-a217-e5ae271fd7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='no')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader \n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader_answer\n",
    "answer_grader.invoke({\"question\": query,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16954716-6825-4eb4-9df8-32772a83ed87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hallucination def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49afe1a4-021d-43a5-ac62-d466815ea3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    source = state[\"source\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f7552-15b2-4ac6-a17d-b621a1147a1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Web search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45045879-841a-43f7-b5ff-6a91c872c06c",
   "metadata": {},
   "source": [
    "## Web search test / prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4f566-9e0c-4f98-bb74-a9ac8403b3d3",
   "metadata": {},
   "source": [
    "### Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9ab8875-576c-4781-b1f5-42ddc3c72b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58223cf3-2eaa-4735-a9ec-94abdadbe7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tavily.search(query=\"LLaMa3 구조에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cadc05e-4cd5-46e2-8d17-f5106c27e909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This release includes model weights and starting code for pre-trained and instruction-tuned Llama 3 language models — including sizes of 8B to 70B parameters. This repository is a minimal example of loading Llama 3 models and running inference. For more detailed examples, see llama-recipes.',\n",
       " 'Llama 3는 오픈 소스 대규모 언어 모델의 진화에서 중요한 이정표를 나타내며 성능, 기능 및 책임 있는 개발 방식의 한계를 뛰어 넘었습니다. 혁신적인 아키텍처, 대규모 훈련 데이터세트, 최첨단 미세 조정 기술을 갖춘 Llama 3는 8B 및 70B 매개변수 규모에서 LLM에 ...',\n",
       " '라마3의 강력한 기능을 실제로 활용하고자 할 때, 다양한 방식으로 Llama3을 사용하는 방법을 참조할 수 있습니다. 이 글에서는 메타가 제공하는 서비스 외에도, 허깅페이스와 같은 오픈소스 커뮤니티를 통해 어떻게 라마3를 사용할 수 있는지 소개하고 있습니다.',\n",
       " '메타에서 최근 공개한 오픈소스 대형 언어 모델인 라마3를 다양한 방식으로 사용해보는 방법을 알아봅니다. 허깅페이스, 메타 AI 서비스, 로컬 PC 등에서 Llama3를 활용하는 방안을 소개합니다. Assistant에서 Llama3를 사용 할수 있는 방법에 대해 알아봅니다.',\n",
       " '이렇게 생성한 모델을 ollama run llama3-ko 와 같이 즉시 실행할 수도 있고 RAG 등과 함께 사용할 때 다음 코드 형태로 사용할 수도 있다. from langchain_community. chat_models import ChatOllama # Ollama 를 이용해 로컬에서 LLM 실행 model = ChatOllama (model = \"llama3-ko\") Reference']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d[\"content\"] for d in response[\"results\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0bf5575-8402-4f1a-82ad-ef1562b6b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tavily.search(query=\"LLaMa3 구조에 대해 알려줘\",search_depth=\"advanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5366028-2dc4-48d9-ac92-5f8ef3501ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This release includes model weights and starting code for pre-trained and instruction-tuned Llama 3 language models — including sizes of 8B to 70B parameters. This repository is a minimal example of loading Llama 3 models and running inference. For more detailed examples, see llama-recipes.',\n",
       " 'Llama 3는 오픈 소스 대규모 언어 모델의 진화에서 중요한 이정표를 나타내며 성능, 기능 및 책임 있는 개발 방식의 한계를 뛰어 넘었습니다. 혁신적인 아키텍처, 대규모 훈련 데이터세트, 최첨단 미세 조정 기술을 갖춘 Llama 3는 8B 및 70B 매개변수 규모에서 LLM에 ...',\n",
       " '라마3의 강력한 기능을 실제로 활용하고자 할 때, 다양한 방식으로 Llama3을 사용하는 방법을 참조할 수 있습니다. 이 글에서는 메타가 제공하는 서비스 외에도, 허깅페이스와 같은 오픈소스 커뮤니티를 통해 어떻게 라마3를 사용할 수 있는지 소개하고 있습니다.',\n",
       " '메타에서 최근 공개한 오픈소스 대형 언어 모델인 라마3를 다양한 방식으로 사용해보는 방법을 알아봅니다. 허깅페이스, 메타 AI 서비스, 로컬 PC 등에서 Llama3를 활용하는 방안을 소개합니다. Assistant에서 Llama3를 사용 할수 있는 방법에 대해 알아봅니다.',\n",
       " '이렇게 생성한 모델을 ollama run llama3-ko 와 같이 즉시 실행할 수도 있고 RAG 등과 함께 사용할 때 다음 코드 형태로 사용할 수도 있다. from langchain_community. chat_models import ChatOllama # Ollama 를 이용해 로컬에서 LLM 실행 model = ChatOllama (model = \"llama3-ko\") Reference']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d[\"content\"] for d in response[\"results\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3b36279-a779-4e2c-a10a-a273c848285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLaMa3는 오픈 소스 대규모 언어 모델로, 혁신적인 아키텍처와 대규모 훈련 데이터세트를 통해 8B 및 70B 매개변수 규모에서 성능을 발휘합니다. 최첨단 미세 조정 기술을 사용하여 성능, 기능 및 책임 있는 개발 방식의 한계를 뛰어 넘었습니다. 다양한 방식으로 활용할 수 있으며, 허깅페이스와 같은 오픈소스 커뮤니티를 통해 사용 가능합니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.qna_search(query=\"LLaMa3 구조에 대해 알려줘\", search_depth=\"advanced\",max_results =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10b784-f49a-494f-8d97-3b5eb575b8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b30e3-ad47-4fe7-b5dc-515f64794871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7313bda1-ddf1-4988-95a2-f3e5f0a1042e",
   "metadata": {},
   "source": [
    "### langchain_community\n",
    "\n",
    "ref.\n",
    "\n",
    "DUCKDUCKGO: https://python.langchain.com/v0.2/docs/integrations/tools/ddg/\n",
    "\n",
    "Wikipedia: https://python.langchain.com/v0.2/docs/integrations/tools/wikipedia/\n",
    "\n",
    "arXiv: https://python.langchain.com/v0.2/docs/integrations/tools/arxiv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc9ad91f-30d4-46d6-91ee-64f1bc2ad49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a55b230a-193b-4027-bfd2-64fcf1511768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DUCKDUCKGO SEARCH WRAPPER\n",
    "ddg_wrapper = DuckDuckGoSearchAPIWrapper(max_results=1)\n",
    "search = DuckDuckGoSearchResults(api_wrapper=ddg_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d56043dc-091b-4881-9e31-fba22c985748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[snippet: Llama 3는 오픈 소스 대규모 언어 모델의 진화에서 중요한 이정표를 나타내며 성능, 기능 및 책임 있는 개발 방식의 한계를 뛰어 넘었습니다. 혁신적인 아키텍처, 대규모 훈련 데이터세트, 최첨단 미세 조정 기술을 갖춘 Llama 3는 8B 및 70B 매개변수 규모에서 LLM에 ..., title: Llama 3에 대해 알아야 할 모든 것 | 가장 강력한 오픈 소스 모델 | 사용 개념 - Unite.AI, link: https://unite.ai/ko/Lama-3에-대해-알아야-할-모든-것-가장-강력한-오픈-소스-모델이자-사용법-개념/], [snippet: 메타에서 최근 공개한 오픈소스 대형 언어 모델인 라마3를 다양한 방식으로 사용해보는 방법을 알아봅니다. 허깅페이스, 메타 AI 서비스, 로컬 PC 등에서 Llama3를 활용하는 방안을 소개합니다. Assistant에서 Llama3를 사용 할수 있는 방법에 대해 알아봅니다., title: 라마3 (Llama 3) 사용법: 다양한 방식으로 Llama3을 사용하는 방법 | 프롬프트해커 대니, link: https://www.magicaiprompts.com/blog/how-to-use-llama], [snippet: GPT-4에 비견할만한 성능을 가진다는 Meta의 오픈소스 LLM Llama3를 사용해보자! Llama 3 모델 Llama 3 모델 특징8B & 70B 파라미터 규모의 모델으로, MMLU, HumanEval 등 벤치마크 태스크에서 경쟁모델보다 좋은 성능을 보임Decoder-only 트랜스포머 아키텍처를 기본으로 하되, Llama2 대비 큰 향상을 보임토큰 인코딩 ..., title: Llama3 한국어 성능 테스트 | Colab에서 Meta-Llama-3 모델 사용해보기, link: https://littlefoxdiary.tistory.com/128], [snippet: 라마3의 강력한 기능을 실제로 활용하고자 할 때, 다양한 방식으로 Llama3을 사용하는 방법을 참조할 수 있습니다. 이 글에서는 메타가 제공하는 서비스 외에도, 허깅페이스와 같은 오픈소스 커뮤니티를 통해 어떻게 라마3를 사용할 수 있는지 소개하고 있습니다., title: 메타 라마3 (Meta Llama 3)란? Meta의 차세대 AI 언어 모델, link: https://www.magicaiprompts.com/blog/meta-llama-3-introduction]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.run(\"LLaMa3 구조에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14f94255-f0bf-44d0-97a5-71e8f6c96c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Wrapper \n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5648ec74-fd5f-4cdb-a481-3920b8f0b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No good Wikipedia Search Result was found'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.run(\"LLaMa3 구조에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "389c030e-465f-4da4-8c0c-a037d218db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arXiv Wrapper\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07c80c14-896a-411d-91e0-f0c2e6921fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/arxiv.py:156: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  ).results()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Published: 2024-07-19\\nTitle: An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs\\nAuthors: Wei Huang, Xingyu Zheng, Xudong Ma, Haotong Qin, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xiang'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.run(\"LLaMa3 구조에 대해 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0b7fc-1a47-4728-a80f-6e3c402ddb0d",
   "metadata": {},
   "source": [
    "## Web search def (Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f9aedf3-7b8a-4404-8a15-27b833e714a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def web_search_tavily(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH. Append to vector store db---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    tavily_response = tavily.search(query=question,search_depth=\"advanced\")\n",
    "    tavily_response2 = tavily.qna_search(query=question, search_depth=\"advanced\",max_results =3)\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in tavily_response[\"results\"]])\n",
    "    web_results+=f\"\\n{tavily_response2}\"\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    source = [{\"source_title\":\" \",\"source\":\"web search result\"}]\n",
    "    return {\"documents\": documents, \"question\": question, \"source\": source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61acef92-55c0-4b67-9e2d-26182189b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def web_search_ddg(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH. Append to vector store db---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    search_res=search.run(question)\n",
    "    search_res1=[]\n",
    "    for s in search_res.split(\", title:\"):\n",
    "        search_res1.append(s.replace(\"[snippet: \",\"\").replace(\"]\",\"\"))\n",
    "    web_results=\"\\n\".join(search_res1)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c10cd8d-f906-4525-a561-f6058d5bc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    source = state[\"source\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b262a62-1bad-4183-8655-af7ae834d60c",
   "metadata": {},
   "source": [
    "# set Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07012724-5b18-4a60-8c4a-2d4d7a8bd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search_tavily\", web_search_tavily) # web search # key: action to do\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"generate_conv\", generate_conv)\n",
    "# workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"reranker_cohere\", reranker_cohere)\n",
    "workflow.add_node(\"generate\", generate) # generatae\n",
    "\n",
    "workflow.add_edge(\"web_search_tavily\", \"generate\") #start -> end of node\n",
    "# workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"reranker_cohere\")\n",
    "workflow.add_edge(\"reranker_cohere\", \"generate\")\n",
    "workflow.add_edge(\"generate_conv\", END)\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"web_search_tavily\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"casualconv\":\"generate_conv\",\n",
    "    },\n",
    ")\n",
    " \n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\", # start: node\n",
    "    grade_generation_v_documents_and_question, # defined function\n",
    "    {\n",
    "        \"not supported\": \"generate\", #returns of the function\n",
    "        \"useful\": END,               #returns of the function\n",
    "        \"not useful\": \"web_search_tavily\",   #returns of the function\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c5f01-342a-4873-9ba4-db97122947bb",
   "metadata": {},
   "source": [
    "# Run Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d5233f1-b77f-4f8f-ad26-867b6b258805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO CASUAL CONV.---\n",
      "---GENERATE Answer (conversation)---\n",
      "'Finished running: generate_conv:'\n",
      "('<response>\\n'\n",
      " '안녕하세요! 반가워요. 오늘 기분은 어떠세요? 저는 당신과 대화하게 되어 정말 기쁘네요. 혹시 오늘 특별한 계획이 있으신가요? 아니면 '\n",
      " '그냥 평범한 하루를 보내고 계신가요?\\n'\n",
      " '</response>')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"안녕\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb6746-8eb0-439e-a534-5a3942599b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef0f1829-9412-4044-8fc9-6bbfd3939c19",
   "metadata": {},
   "source": [
    "# Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4bdaa75-5aa7-45e1-922f-e3f4ee60691b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import AsyncGenerator\n",
    "from typing import AsyncIterable\n",
    "import httpx\n",
    "async def graph_stream(inputs: dict) -> AsyncIterable[str]:\n",
    "        async for response in app.astream_events(inputs, version=\"v2\"):\n",
    "            with open(\"./stream_test.txt\",\"a\",encoding=\"utf-8\")as e:\n",
    "                e.write(f\"\\n\\n\\n====================\\n\\n{response}\\n\\n====================\")\n",
    "            if response[\"event\"] == \"on_chain_stream\":\n",
    "                if \"generate_conv\" in list(response[\"data\"][\"chunk\"].keys()):\n",
    "                    try:\n",
    "                        yield response[\"data\"][\"chunk\"][\"generate_conv\"][\"generation\"]\n",
    "                    except Exception as e: \n",
    "                        try:\n",
    "                            yield response[\"data\"][\"chunk\"][\"generation\"]\n",
    "                        except Exception as e: pass\n",
    "\n",
    "def response_streamer(user_input):\n",
    "    inputs = {\"question\": user_input}\n",
    "    for event in app.astream_events(inputs, version=\"v1\"):\n",
    "        # print(event)\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            print(event)\n",
    "            content = event[\"output\"][\"generate_conv\"][\"generation\"]\n",
    "            # if content:\n",
    "                # Empty content in the context of OpenAI or Anthropic usually means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "            print(content)\n",
    "            yield content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c343b4f4-4bf8-4ced-b9a4-ed2012b7f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<starlette.responses.StreamingResponse at 0x7fdaf6427dc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starlette.responses import StreamingResponse\n",
    "streaming=graph_stream({\"question\": \"안녕\"})\n",
    "StreamingResponse(streaming, media_type=\"text/event-stream\",headers={\"X-Accel-Buffering\": \"no\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5fd667e-39da-422c-ad37-df9936ce8c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object response_streamer at 0x7fdaf65ca030>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_streamer({\"question\": \"안녕\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e84a82a8-ab02-419c-a928-887d1a48f1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': {'question': '안녕'}}, 'name': 'LangGraph', 'tags': [], 'run_id': 'f65e8724-3fa8-4327-b594-0049f86d5804', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {}, 'name': '__start__', 'tags': ['graph:step:0', 'langsmith:hidden'], 'run_id': '61899468-c376-4ea3-875f-017698ea5382', 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804']}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'question': '안녕'}}, 'name': 'ChannelWrite<question,generation,web_search,documents,source>', 'tags': ['seq:step:1', 'langsmith:hidden'], 'run_id': '4f577c38-08cb-4aaa-aeb4-93683a9d77f6', 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '61899468-c376-4ea3-875f-017698ea5382']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'question': '안녕'}, 'input': {'question': '안녕'}}, 'run_id': '4f577c38-08cb-4aaa-aeb4-93683a9d77f6', 'name': 'ChannelWrite<question,generation,web_search,documents,source>', 'tags': ['seq:step:1', 'langsmith:hidden'], 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '61899468-c376-4ea3-875f-017698ea5382']}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'question': '안녕'}}, 'name': 'route_question', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': 'db8751c7-aa51-41f1-aeb6-3681897d7101', 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '61899468-c376-4ea3-875f-017698ea5382']}\n",
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO CASUAL CONV.---\n",
      "{'event': 'on_chain_end', 'data': {'output': 'casualconv', 'input': {'question': '안녕'}}, 'run_id': 'db8751c7-aa51-41f1-aeb6-3681897d7101', 'name': 'route_question', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '61899468-c376-4ea3-875f-017698ea5382']}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'question': '안녕'}}, 'name': 'ChannelWrite<branch:__start__:route_question:generate_conv>', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': '415bdbd1-e835-4b4c-8504-4d80286877a7', 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '61899468-c376-4ea3-875f-017698ea5382']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'question': '안녕'}, 'input': {'question': '안녕'}}, 'run_id': '415bdbd1-e835-4b4c-8504-4d80286877a7', 'name': 'ChannelWrite<branch:__start__:route_question:generate_conv>', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '61899468-c376-4ea3-875f-017698ea5382']}\n",
      "{'event': 'on_chain_stream', 'run_id': '61899468-c376-4ea3-875f-017698ea5382', 'name': '__start__', 'tags': ['graph:step:0', 'langsmith:hidden'], 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'data': {'chunk': {'question': '안녕'}}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'question': '안녕'}, 'input': {'question': '안녕'}}, 'run_id': '61899468-c376-4ea3-875f-017698ea5382', 'name': '__start__', 'tags': ['graph:step:0', 'langsmith:hidden'], 'metadata': {'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804']}\n",
      "{'event': 'on_chain_start', 'data': {}, 'name': 'generate_conv', 'tags': ['graph:step:1'], 'run_id': '0384ca29-a138-4d8e-b229-47c9c3768a9a', 'metadata': {'langgraph_step': 1, 'langgraph_node': 'generate_conv', 'langgraph_triggers': ['branch:__start__:route_question:generate_conv'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804']}\n",
      "---GENERATE Answer (conversation)---\n",
      "{'event': 'on_chain_start', 'data': {'input': {'documents': None, 'question': '안녕', 'source': [{'source_title': ' ', 'source': 'casual conversation'}], 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>'}}, 'name': 'ChannelWrite<generate_conv,question,generation,web_search,documents,source>', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': '2d95a425-76ca-4cea-934a-b113be04e7cb', 'metadata': {'langgraph_step': 1, 'langgraph_node': 'generate_conv', 'langgraph_triggers': ['branch:__start__:route_question:generate_conv'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '0384ca29-a138-4d8e-b229-47c9c3768a9a']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'documents': None, 'question': '안녕', 'source': [{'source_title': ' ', 'source': 'casual conversation'}], 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>'}, 'input': {'documents': None, 'question': '안녕', 'source': [{'source_title': ' ', 'source': 'casual conversation'}], 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>'}}, 'run_id': '2d95a425-76ca-4cea-934a-b113be04e7cb', 'name': 'ChannelWrite<generate_conv,question,generation,web_search,documents,source>', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'langgraph_step': 1, 'langgraph_node': 'generate_conv', 'langgraph_triggers': ['branch:__start__:route_question:generate_conv'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804', '0384ca29-a138-4d8e-b229-47c9c3768a9a']}\n",
      "{'event': 'on_chain_stream', 'run_id': '0384ca29-a138-4d8e-b229-47c9c3768a9a', 'name': 'generate_conv', 'tags': ['graph:step:1'], 'metadata': {'langgraph_step': 1, 'langgraph_node': 'generate_conv', 'langgraph_triggers': ['branch:__start__:route_question:generate_conv'], 'langgraph_task_idx': 0}, 'data': {'chunk': {'documents': None, 'question': '안녕', 'source': [{'source_title': ' ', 'source': 'casual conversation'}], 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>'}}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'documents': None, 'question': '안녕', 'source': [{'source_title': ' ', 'source': 'casual conversation'}], 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>'}, 'input': {'question': '안녕', 'generation': None, 'web_search': None, 'documents': None, 'source': None}}, 'run_id': '0384ca29-a138-4d8e-b229-47c9c3768a9a', 'name': 'generate_conv', 'tags': ['graph:step:1'], 'metadata': {'langgraph_step': 1, 'langgraph_node': 'generate_conv', 'langgraph_triggers': ['branch:__start__:route_question:generate_conv'], 'langgraph_task_idx': 0}, 'parent_ids': ['f65e8724-3fa8-4327-b594-0049f86d5804']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'f65e8724-3fa8-4327-b594-0049f86d5804', 'name': 'LangGraph', 'tags': [], 'metadata': {}, 'data': {'chunk': {'generate_conv': {'question': '안녕', 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>', 'documents': None, 'source': [{'source_title': ' ', 'source': 'casual conversation'}]}}}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'question': '안녕', 'generation': '<response>\\n안녕하세요! 반가워요. 오늘 기분은 어떠세요? 특별한 계획이라도 있나요? 아니면 그냥 평범한 하루를 보내고 계신가요? 저는 여러분과 이렇게 대화를 나눌 수 있어서 정말 기쁩니다. 무엇에 대해 이야기하고 싶으세요?\\n</response>', 'documents': None, 'source': [{'source_title': ' ', 'source': 'casual conversation'}]}}, 'run_id': 'f65e8724-3fa8-4327-b594-0049f86d5804', 'name': 'LangGraph', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "async for response in app.astream_events({\"question\": \"안녕\"}, version=\"v2\"):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd888a9-8901-4436-a71c-343854b00bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67858a3f-b091-4b9c-bda0-d3fc38c16b74",
   "metadata": {},
   "source": [
    "# Graph visualization\n",
    "\n",
    "ref. https://langchain-ai.github.io/langgraph/how-tos/visualization/#using-mermaid-pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3da36be5-2ccc-4d99-bd78-c9e8d30f6518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__[__start__]:::startclass;\n",
      "\t__end__[__end__]:::endclass;\n",
      "\tweb_search_tavily([web_search_tavily]):::otherclass;\n",
      "\tretrieve([retrieve]):::otherclass;\n",
      "\tgenerate_conv([generate_conv]):::otherclass;\n",
      "\treranker_cohere([reranker_cohere]):::otherclass;\n",
      "\tgenerate([generate]):::otherclass;\n",
      "\tgenerate_conv --> __end__;\n",
      "\treranker_cohere --> generate;\n",
      "\tretrieve --> reranker_cohere;\n",
      "\tweb_search_tavily --> generate;\n",
      "\t__start__ -. websearch .-> web_search_tavily;\n",
      "\t__start__ -. vectorstore .-> retrieve;\n",
      "\t__start__ -. casualconv .-> generate_conv;\n",
      "\tgenerate -. not supported .-> generate;\n",
      "\tgenerate -. useful .-> __end__;\n",
      "\tgenerate -. not useful .-> web_search_tavily;\n",
      "\tclassDef startclass fill:#ffdfba;\n",
      "\tclassDef endclass fill:#baffc9;\n",
      "\tclassDef otherclass fill:#fad7de;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63cdef55-0547-4399-9a3d-088dff350729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeColors\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2946e-e68c-40b2-80ca-ff2f4700520f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

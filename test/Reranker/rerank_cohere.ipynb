{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6c42e1-1165-454b-a474-d933a3607199",
   "metadata": {},
   "source": [
    "ref.\n",
    "\n",
    "https://medium.com/@parikshitsaikia1619/unlock-rags-potential-with-distance-metrics-and-rerankers-42df4f171f5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aeed4c2-6962-4b2f-8ff5-37734bf8b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.6.5.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate, os, openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "TAVILY_API_KEY = \"\"\n",
    "Openai_API_KEY = \"\"\n",
    "GROQ_API_KEY = \"\"\n",
    "coher_API_KEY = \"\"\n",
    "client = weaviate.Client(\n",
    "    url=\"http://192.168.2.186:8080\"\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] =  Openai_API_KEY\n",
    "openai.api_key =os.getenv(\"OPENAI_API_KEY\")\n",
    "embed_model = \"text-embedding-ada-002\"\n",
    "embeddings = OpenAIEmbeddings(model=embed_model)\n",
    "def get_embedding(text, engine=\"text-embedding-ada-002\") : \n",
    "    res = openai.Embedding.create(input=text,engine=engine)['data'][0]['embedding']\n",
    "    # from openai import OpenAI\n",
    "    # embedding_client = OpenAI()\n",
    "    # res= embedding_client.embeddings.create(input = text, model=engine).data[0].embedding\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2502cdbb-da04-4f99-89f7-3a43e43ae4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"LLaMa3의 구조에 대해 설명해줘\"\n",
    "query_vector = get_embedding(query)\n",
    "response = client.query.get(\"Test\", [\"text\",\"title\"]).with_hybrid(query, vector=query_vector).with_limit(6).do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9cefc7f-314e-41b6-aac3-cc69051538b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_res=[r[\"text\"] for r in response[\"data\"][\"Get\"][\"Test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d615dd52-cebe-4ba0-b0cf-12623d2f2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(coher_API_KEY)\n",
    "\n",
    "\"\"\"\n",
    "cohere models\n",
    "https://docs.cohere.com/docs/models\n",
    "\"\"\"\n",
    "rerank_res = co.rerank(\n",
    "    model=\"rerank-multilingual-v3.0\",\n",
    "    query=query, # search query used for docs retrieval\n",
    "    documents=retrieval_res, #list of retrieved docs\n",
    "    top_n=4, # selecting top 4 docs\n",
    ")\n",
    "\n",
    "reranked_docs = []\n",
    "\n",
    "\n",
    "for result in rerank_res.results:\n",
    "    reranked_docs.append(retrieval_res[result.index])\n",
    "\n",
    "reranked_context = \"\\n\\n------------------\\n\\n\".join(reranked_docs) #combined into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afbfda39-6e51-4937-b3f1-2e5a8881ab36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RerankResponseResultsItem(document=None, index=2, relevance_score=0.98365986),\n",
       " RerankResponseResultsItem(document=None, index=0, relevance_score=0.9802431),\n",
       " RerankResponseResultsItem(document=None, index=1, relevance_score=0.9129032),\n",
       " RerankResponseResultsItem(document=None, index=3, relevance_score=0.00069054146)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_res.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b78d900e-742e-4680-9b4f-d36d645e133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base architecture: LLaMa 3는 standard decoder-only transformer architecture를 기반으로 개발되었다. Tokenizer size: 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding하여 모델 성능을 향상시킴. -> LLaMa 2보다 4배 큰 tokenizer size Attention method: 8B, 70B 모두 GQA를 사용하여 모델 추론 효율성을 높였다. -> LLaMa 2는 70에만 GQA 적용 context window: 8,192 token -> LLaMa 2는 4,096였다. Tokenizer size가 커지면 encoding을 효과적/효율적으로 할 수 있는 이유: tokenzer size가 4배 커지면 동일한 데이터가 약 4배정도 압축되어 모델에 입력될 수 있다. 예를 들어 입력한 데이터가 LLaMa 2의 tokenizer로 200 token으로 encoding 될 때, LLaMa 3 tokenizer로는 100개가 되지 않는 token 수로 encoding될 수 있기 때문이다. 그리고 tokenizer size가 커진 만큼 모델이 많이 학습하지 않은 한국어 데이터에 대한 tuning을 수행할 때 vocab에 따로 한국어 token을 추가하지 않아도 된다. Training data size LLaMa 3는 15T개의 token으로 학습되었다. (LLaMa 2는 2T로, 2에 비해 3는 약 7배 더 큰 학습데이터를 사용하였다.) 수집된 데이터는 모두 publicly available source에서 수집되었다. code data: LLaMa 2에 사용된 code data 수보다 4배 더 많음 multilingual use case를 위해 전체 training data의 5% 이상을 high-quality non-English(30개 이상) data로 구성\n",
      "\n",
      "------------------\n",
      "\n",
      "LLaMa 3 LLaMA 3는 Meta에서 공개한 Open Source LLM model로, LLaMa2에 이어 공개된 모델이다. LLaMA 3는 Instruct Model과 Pre-trained Model에 대해 각각 8B, 70B 두 사이즈가 공개되었다. Pretraining과 Post-training 방법의 개선으로 공개된 8B, 70B의 Pretrained, Instruction-fine-tuned model이 2024 4월18일 기준 해당 parameter scale 모델 중 가장 좋은 성능을 보인다고 한다. Post-training과정에서는 false refusal rate를 줄이고, model의 alignment를 개선하고, model response의 다양성을 증가시켰다. 특히 LLaMa 3는 2보다 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있을 것으로 보인다. LLaMa 3 개발 시 Meta팀은 benchmark 성능 향상 뿐만 아니라 실제 추론 능력(real-world scenario) 최적화에도 집중하였다고 한다. 실제 추론 능력 검증을 위해 새로운 high-quality human evaluation set을 개발했다고 한다. 해당 데이터는 12가지 use case에 대한 총 1,800개 prompt로 구성되어 있다. (12가지 use case: asking for advice, brainstorming, classification, closed question answering, coding, creative writing, extraction, inhabiting a character/persona, open question answering, reasoning, rewriting, summarization). 아래 표는 해당 evaluation set에 대한 Claude Sonnet, Mistral Medium, GPT-3.5의 추론 결과를 비교한 것이다. https://ai.meta.com/blog/meta-llama-3/ LLaMA 3 개발팀은 모델 개발 시 Model architecture, Training data, Scaling up pretraining, Instruction fine-tuning을 중점적으로 개선했다고 한다. Model architecture\n",
      "\n",
      "------------------\n",
      "\n",
      "output: LLaMa 3은 Meta에서 개발한 Open Source LLM(Large Language Model) 모델로, LLaMa 2를 이어 발표된 모델입니다. 이 모델은 Instruct Model과 Pre-trained Model로 8B, 70B 두 사이즈가 공개되었으며, 이는 2024년 4월 18일 기준 해당 파라미터 스케일 모델 중 가장 좋은 성능을 보이고 있습니다. LLaMa 3는 코드 생성, instruction 수행 능력이 크게 향상되어 모델을 보다 다양하게 활용할 수 있습니다. 이 모델은 standard decoder-only transformer architecture를 기반으로 하고, 128K token 수를 가진 vocab을 사용하여 언어를 보다 효과적으로 encoding합니다. LLaMa 3는 15T 개의 token으로 학습되었으며, 이는 LLaMa 2보다 약 7배 더 큰 학습 데이터를 사용하였습니다. Graph visualization ref. https://langchain-ai.github.io/langgraph/how-tos/visualization/#using-mermaid-pyppeteer from IPython.display import Image, display from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeColors print(app.get_graph().draw_mermaid()) display( Image( app.get_graph().draw_mermaid_png( draw_method=MermaidDrawMethod.API, ) ) ) × Search\n",
      "\n",
      "------------------\n",
      "\n",
      "Stage 1. Pre-training for Feature Alignment: CC3M의 subset을 기반으로 projection layer matrix만 업데이트한다. Stage 2. Fine-tuning End-to-End: 투영 행렬과 LLM 모두 두 가지 다른 사용 시나리오에 맞게 업데이트됩니다: Multimodal Chatbot(Visual Chat): LLaVA는 일상적인 사용자 지향 application(downstream) 수행을 위해 multimodal instruction-following data로 fine-tuning을 진행한다. Science QA: LLaVA는 science domain을 위해 multimodal reasonsing dataset으로 로 fine-tuning을 진행한다.\n"
     ]
    }
   ],
   "source": [
    "print(reranked_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa31ea-8b1c-4f3d-a978-58451effa4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
